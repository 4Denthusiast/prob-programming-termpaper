%!TEX root = probabilisticProgrammingMartingales.tex
\subsection*{Reorganised proof}

Fix a probability space $(\Omega, \calF, \mathbb{P})$. 
We have in mind $\Omega = S$, and $\mathbb{P} = \mu$.

\begin{definition}\rm
A sequence of random variables $(Y_n)_{n \in \omega}$ adapted to a filtration $(\calF_n)_{n \in \omega}$ is a \emph{supermartingale} if for all $n \in \omega$, $\expect{|Y_n|} < \infty$, and $\expect{Y_{n+1} \mid \calF_n} \leq Y_n$.
\lo{The preceding means: $\forall A \in \calF_n$, $\int_A \dif \mu \, Y_{n+1} \leq \int_A \dif \mu \,Y_n$.}
For $\epsilon > 0$, it is a \emph{$\epsilon$-ranking supermartingale} if, in addition, for all $n$, $Y_n \geq 0$ and $\expect{Y_{n+1} \mid \calF_n} \leq Y_n - \epsilon \, \mathbf{1}_{\set{Y_n > 0}}$.
\citep{DBLP:conf/popl/FioritiH15,DBLP:conf/popl/ChatterjeeFNH16}
\end{definition}

Intuitively $Y_n$ is the rank of the program after $n$ steps of computation.
(Assume that the rank of a term is 0 iff it is a value.) In a $\epsilon$-ranking supermartingale, each computation step causes a strict decrease in rank, provided the term being reduced is not a value.

Let $S$ and $T$ be stopping times adapted to $(\calF_n)_{n \in \omega}$.
Recall the $\sigma$-algebra (consisting of measurable subsets ``priori to $T$'')
\[
\calF_T := \set{A \in \calF \mid \forall i \in \omega \, . \, A \cap \set{T \leq i} \in \calF_i}
\]
and if $S \leq T$, then $\calF_S \subseteq \calF_{T}$.

The following is an iterated version of Doob's well-known Optional Sampling Theorem (see, e.g., \cite[\S 6.7]{AshDD00}).
\begin{theorem}[Optional Sampling]
\label{thm:optional sampling}
Let $(X_n)_{n\in \omega}$ be a supermartingale, and $(T_n)_{n \in \omega}$ a sequence of increasing stopping times, then $(X_{T_n})_{n \in \omega}$ is a supermartingale adapted to $(\calF_{T_n})_{n \in \omega}$ if one of the following conditions holds:
\begin{enumerate}
\item each $T_n$ is bounded i.e.~$T_n < c_n$ where $c_n$ is a constant
\item $(X_n)_{n\in \omega}$ is uniformly integrable.
\end{enumerate}
\end{theorem}

\begin{theorem}[{\citep[Lemma 5.5]{DBLP:conf/popl/FioritiH15}, \citep[Prop 1]{DBLP:conf/popl/ChatterjeeFNH16}}]
\label{thm:rank-PAST}
Let $(Y_n)_{n \in \omega}$ be a $\epsilon$-ranking supermartingale, and set $T(s) := \min \set{n \mid Y_n(s) = 0}$. 

Then $T < \infty$ almost surely, and $\expect{T} \leq \dfrac{\expect{Y_0}}{\epsilon}$.
\end{theorem}

\paragraph{Proof of \Cref{thm:rankable gives supermartingale}.}

Fix a closed SPCF term $M \in \Lambda^0$, and a ranking function $f$ for it, satisfying $f(N) = 0$ iff $N$ is a value.
Let $n \in \omega$, define the following random variables on the probability space $(S, \calF, \mu)$:
\begin{align*}
M_n(s) &:= \pi_0 (\red^n(M, s))\\
\#\textrm{draws}_n(s) &:= l \textrm{ where $\pi_1 (\red^n(M, s)) = \underbrace{\pi_t( \cdots (\pi_t}_l(s))$}\\
T_0(s) & := 0 \\
T_{n+1}(s) & := \min \{ k \mid k>T_n(s), M_k(s) \textrm{ a value or of form } E[\tY (\lambda x. N)] \}\\
X_n(s) & := f(M_n(s)) \\
Y_n(s) & := X_{T_n}(s)
\end{align*}
and define a filtration $\mathcal{F}_n = \sigma(M_1, \cdots, M_n)$.


%Take $M \in \Lambda^0$ (closed SPCF terms). Define, for each $n \in \omega$, the random variable $M_n : S \to \Lambda^0$ by $M_n(s) := \pi_0 (\red^n(M, s))$.

We say that a given SPCF term is \emph{type-1} (respectively 2, 3 and 4) if it has the shape $E[\tY (\lambda x. N)]$ (respectively $E[\tsample]$, $E[R]$ where $R$ is any other redex, and of a value).
%By abuse of language, we refer to the three respective types of redexes as type-$1$, 2 and 3.

%Henceforth fix $M \in \Lambda^0$.
For $i \in \{1, 2, 3\}$ and $n \in \omega$, define function $f_{i, n+1}(M) : S \to \Real$ by
\[
f_{i, n+1}(M)(s) :=
\begin{cases}
f(M_{n+1}(s)) & \hbox{if }s \in \mathbf{T}_i := 
\set{s \mid M_n(s) \textrm{ is type-$i$}}\\
0 & \textrm{otherwise}
\end{cases}
\]
\iffalse
\begin{lemma}
\label{lem:inde}
For each $i$ and $n$, $f_{i, n+1}(M)$ is %not just $\mathcal{F}_{n+1}$-measurable but also 
$\mathcal{F}_{n}$-measurable.
\end{lemma}

\begin{proof} First fix notation
\begin{align*}
\sigma(M_{n+1}) &= \sigma\big(\set{M_{n+1}^{-1}(U_{\beta_k})
\mid \beta_k\in \mathsf{Sk}_k, U_{\beta_k} \in \mathcal{B}(\Real^k), k \geq 0}\big)\\
\sigma(M_{n}) &= \sigma\big(\set{M_{n}^{-1}(U_{\alpha_j})
\mid \alpha_j\in \mathsf{Sk}_j, U_{\alpha_j} \in \mathcal{B}(\Real^j), j \geq 0}\big);
\end{align*}
and for $\beta_k \in \mathsf{Sk}_i$, we write $\beta_k[\dagger]$ to mean the instantiation of $\beta_k$ by a $k$-vector of numerals $\dagger$; $\alpha_j[\dagger']$ has the same meaning.
Now for each $\beta_k[\dagger] = E[R']$ where $R'$ is the contractum of a type-$i$ redex $R$, there exist $\alpha_j \in \mathsf{Sk}_j$ and $j$-vector $\dagger'$ such that $\alpha_j[\dagger'] = E[R]$ of type $i$.
Moreover, for each $U_{\beta_k}$, there exists $U_{\alpha_j}$ such that $M_{n+1}^{-1}(U_{\beta_k}) = M_{n}^{-1}(U_{\alpha_j}) \in \sigma(M_n)$.
\end{proof}
\fi

Plainly
\(
f(M_{n+1}) = %\sum_{i=1}^3 f_{i, n+1}(M).
f_{1, n+1}(M) + f_{2, n+1}(M) + f_{3, n+1}(M),
\)
$\mu$-almost-surely.
%(Unless otherwise stated, all equations and inequations between random variables are assumed to hold only $\mu$-a.s.)
Therefore 
\[
\expect{f(M_{n+1}) \mid \mathcal{F}_n} \nonumber \\
= 
\expect{f_{1, n+1}(M) \mid \mathcal{F}_n} + \expect{f_{2, n+1}(M) \mid \mathcal{F}_n} + \expect{f_{3, n+1}(M) \mid \mathcal{F}_n} 
\]
\iffalse
\begin{align}
& \expect{f(M_{n+1}) \mid \mathcal{F}_n} \nonumber \\
& = \expect{f_{1, n+1}(M) + f_{2, n+1}(M) + f_{3, n+1}(M)  \mid \mathcal{F}_n} 
\nonumber \\
& = \expect{f_{1, n+1}(M) \mid \mathcal{F}_n} + \expect{f_{2, n+1}(M) \mid \mathcal{F}_n} + \expect{f_{3, n+1}(M) \mid \mathcal{F}_n} 
\label{eqn:linear cond ex} 
& = f_{1, n+1}(M) + f_{2, n+1}(M) + f_{3, n+1}(M)
\label{eqn:inde cond exp}
& = f_{1, n+1}(M) + \int_I f() + f_{3, n+1}(M) \label{eqn:inde cond exp} \\
\end{align}
\Cref{eqn:linear cond ex} follows from the linearity of conditional expectation. 
%\Cref{eqn:inde cond exp} is justified because $f_{i, n+1}(M)$ is $\mathcal{F}_n$-measurable (\Cref{lem:inde}).
\fi

It remains to show: for all $A \in \calF_n$
\begin{align}
& \int_A \dif \mu \, \expect{f(M_{n+1}) \mid \mathcal{F}_n} \nonumber \\
& \leq  \int_A \mu (\dif s) \, \big(f(M_{n})[s \in \mathbf{T}_1] + f(M_{n})[s \in \mathbf{T}_2] + f(M_{n})[s \in \mathbf{T}_3]\big)
\label{eqn:ts} \\
& = \int_A \dif \mu \, f(M_n). 
\label{eqn:partition} 
\end{align}
where $[s \in \mathbf{T}_i]$ is the Iverson bracket.

\iffalse
Here we use the notation: given function $g : S \to \Real$ and $U \subseteq S$, $g[U] : S \to \Real$ denotes the function
\[
g[U](x) :=
\begin{cases}
g(x) & \textrm{$x \in U$}\\
0 & \textrm{otherwise}
\end{cases}
\]
\fi

The inequality (\ref{eqn:ts}) follows immediately from the lemma below; 
and (\ref{eqn:partition}) holds because $\{\mathbf{T}_1, \mathbf{T}_2, \mathbf{T}_3, \mathbf{T}_4\}$ is a partition of $S$.

\begin{lemma}
For all $i \in \set{1, 2, 3}$ and $A \in \calF_n$
\[
\int_A \dif \mu \, f_{i, n+1}(M) \leq \int_A \mu(\dif s) \, f(M_n)[s \in \mathbf{T}_i].
\] 
\end{lemma}

\begin{proof}
We show the non-trivial case of $i = 2$.
First we express 
\begin{equation}
f_{2, n+1}(M) = \sum_{i \in \calI} 
\lambda s . f(E_i[\underline{\sigma_i(s)}][\underline{\rho(s)}])[s \in U_i]
\label{eqn:f 2 n+1}
\end{equation}
where 
\begin{itemize}
\item $\calI$ is a countable indexing set
\item $E_i \in \bigcup_{j \in \omega}\mathsf{Sk}_j$, and $\sigma_i : S \to \bigcup_{j \in \omega}\Real^j$ is a random variable
\item $\rho(s) = \pi_h(\pi_1(\red^n(M, s))) \in \Real$ 
\item $\set{U_i}_{i \in \calI}$ is a partition of $\mathbf{T}_2$ 
\item Each $U_i$ is determined by a skeletal term, namely, $E_i$, and the number of draws (i.e.~$\ndraw{n}{s}$) consumed in $n$ steps of computation of $M$, in the sense that for all $s, s' \in U_i$, 
%$\pi_0(\red^n(M, s))$ and $\pi_0(\red^n(M, s))$ 
$M_n(s)$ and $M_n(s')$ have the same underlying skeleton $E_i$ and $\ndraw{n}{s} = \ndraw{n}{s'}$.
Notice that $E_i[\underline{\sigma_i(s)}][{\tsample}] = M_{n}(s)$ for $s \in U_i$.
\end{itemize}

\emph{Notation}. Given an $\omega$-sequence (e.g.~$s \in S$) and $m \in \omega$, we write $s_{\leq m}$ to mean the $m$-long prefix of $s$.

Observe that if $s \in U_i$ and $\ndraw{n}{s} = l$ then $s_{\leq l} = s_{\leq l}'$ implies $M_n(s) = M_n(s')$.
It follows that $\set{s_{\leq l}} \cdot I^\omega \subseteq U_i$ (where $\cdot$ means concatenation of sequences).
Moreover, for any $A \in \calF_n$, if $s \in A$ then $\set{s_{\leq l}} \cdot I^\omega \subseteq A$.
This means that for any measurable $g$, if $g(s)$ only depends on the $(l+1)$-long prefix of $s$ (i.e.~$g(s) = g'(s_{\leq l+1})$), then 
\begin{equation}
\int_{A \cap U_i}  \mu(\dif s) \, g(s) = 
\int_{(A \cap U_i)_{\leq l+1}} \Leb_{l+1}(\dif t) \, g'(t)
\label{eqn:truncate}
\end{equation}
where $A_{m} := \set{s_{\leq m} \mid s \in A}$, and $\Leb_n$ is the Lebesgue measure on $I^n$.

Take $s \in U_i$ with $\ndraw{n}{s} = l$.
Now $\sigma_i(s)$ depends on $s_{\leq l}$, and $\rho(s)$ depends on $s_{\leq l+1}$.
Let $u$ range over $(U_i)_{\leq l}$.
Then, it follows from the definition of $f$ that
\[
\int_I \Leb(\dif r) \, f(E_i[\underline{\sigma_i(u)}][\underline{r}])) \leq f(E_i[\underline{\sigma_i(u)}][\tsample])).
\]
Integrating both sides, we get
\begin{align*}
& \int_{(A \cap U_i)_{\leq l}} \Leb_l(\dif u) \, \int_I \textrm{Leb}(\dif r) \, f(E_i[\underline{\sigma_i(u)}][\underline{r}])) \\
\leq & \int_{(A \cap U_i)_{\leq l}} \Leb_l(\dif u) \, f(E_i[\underline{\sigma_i(u)}][\tsample]))
\end{align*}
It follows from the preceding observation that
\begin{align*}
& \int_{(A \cap U_i)_{\leq l+1}} \Leb_{l+1}(\dif u') \, f(E_i[\underline{\sigma_i(u')}][\underline{\rho(u')}])) \\
& \leq 
\int_{(A \cap U_i)_{\leq l+1}} \Leb_{l+1}(\dif u') \, f(E_i[\underline{\sigma_i(u')}][\tsample]))
\end{align*}
and so we have
\begin{equation}
\int_{A \cap U_i} \mu(\dif s) \, f(E_i[\underline{\sigma_i(s)}][\underline{\rho(s)}])) \\
\leq 
\int_{A \cap U_i} \mu(\dif s) \, f(E_i[\underline{\sigma_i(s)}][\tsample])).
\label{eqn:a u ui}
\end{equation}

Finally, integrating both sides of (\ref{eqn:f 2 n+1}), we have
\begin{align*}
\int_A \mu(\dif s) \, f_{2, n+1}(M)(s) 
&= 
\int_A \mu(\dif s) \, \sum_{i \in \calI} f(E_i[\underline{\sigma_i(s)}][\underline{\rho(s)}])[s \in U_i] \\
&= 
\sum_{i \in \calI} \int_{A \cap U_i} \mu(\dif s) \, f(E_i[\underline{\sigma_i(s)}][\underline{\rho(s)}])\\
&\leq 
\sum_{i \in \calI} \int_{A \cap U_i} \mu(\dif s) \, f(E_i[\underline{\sigma_i(s)}][\tsample])
\quad \hbox{$\because$ (\ref{eqn:a u ui})}\\
&= 
\int_{A} \mu(\dif s) \, \sum_{i \in \calI} \, f(E_i[\underline{\sigma_i(s)}][\tsample])[s \in U_i]\\
&= \int_{A} \mu(\dif s) f_{n}(M)[s \in \mathbf{T}_2]
%\quad \hbox{$\because$ (\ref{eqn:f 2 n+1})}
\end{align*}

\end{proof}

\iffalse
[** To see $f_{2, n+1}(M) \leq f(M_n)[T_2]$, take $s \in T_2$. Then $f_{2, n+1}(M)(s) = f(E[\underline{a}])$ and $M_n = E[\tsample]$, for some $a \in [0, 1]$ and evaluation context $E$. Hence 
\[
f_{2, n+1}(M)(s) \leq \int_I f(E[\underline{r}]) \, \mu_{leb}(\textrm{d} r)
\leq f(E[\tsample]) = f(M_n)[T_2](s).
\]
**]
\fi
This concludes the proof of \Cref{thm:rankable gives supermartingale}. \hfill \qed

\medskip

Observe that $T_0, T_1, T_2, \cdots$ are an increasing sequence of stopping times adapted to $(\calF_n)_{n \in \omega}$, and each $T_i$ is bounded.
As $(X_n)_{n \in \omega}$ is a supermartingale (\Cref{thm:rankable gives supermartingale}), it follows from \Cref{thm:optional sampling} that $(Y_n)_{n \in \omega}$ is a supermartingale (in fact, a $1$-ranking one).
Therefore, by \Cref{thm:rank-PAST}, $M$ is PAST.

Thus, to summarise

\begin{corollary}
If a closed SPCF term is rankable, then it is positively almost-surely terminating.
\end{corollary}
