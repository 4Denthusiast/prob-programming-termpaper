%!TEX root = probabilisticProgrammingMartingales.tex
\paragraph{}

Fix a probability space $(\Omega, \calF, \mathbb{P})$. 
We have in mind $\Omega = S$, and $\mathbb{P} = \mu$.

\begin{definition}\rm
A sequence of random variables $(Y_n)_{n \in \omega}$ adapted to a filtration $(\calF_n)_{n \in \omega}$ is a \emph{supermartingale} if for all $n \in \omega$, $\expect{|Y_n|} < \infty$, and $\expect{Y_{n+1} \mid \calF_n} \leq Y_n$.
\lo{The preceding means: $\forall A \in \calF_n$, $\int_A \dif \mu \, Y_{n+1} \leq \int_A \dif \mu \,Y_n$.}
For a stopping time $T$ and $\epsilon > 0$, it is a \emph{$\epsilon$-ranking supermartingale} if, in addition, for all $n$, $Y_n \geq 0$ and $\expect{Y_{n+1} \mid \calF_n} \leq Y_n - \epsilon \cdot \mathbf{1}_{\set{T > n}}$.
\citep{DBLP:conf/popl/FioritiH15,DBLP:conf/popl/ChatterjeeFNH16}
\end{definition}

Intuitively $Y_n$ gives the rank of the program after $n$ steps of computation, and $T$ is the time at which it reaches a value (which may be infinite if it fails to terminate).
In a $\epsilon$-ranking supermartingale, each computation step causes a strict decrease in rank of at least $\epsilon$, provided the term in question is not a value.

Let $T$ and $T'$ be stopping times adapted to $(\calF_n)_{n \in \omega}$.
Recall the $\sigma$-algebra (consisting of measurable subsets ``priori to $T$'')
\[
\calF_T := \set{A \in \calF \mid \forall i \in \omega \, . \, A \cap \set{T \leq i} \in \calF_i}
\]
and if $T \leq T'$, then $\calF_{T} \subseteq \calF_{T'}$.

The following is an iterated version of Doob's well-known Optional Sampling Theorem (see, e.g., \cite[\S 6.7]{AshDD00}).
\begin{theorem}[Optional Sampling]
\label{thm:optional sampling}
Let $(X_n)_{n\in \omega}$ be a supermartingale, and $(T_n)_{n \in \omega}$ a sequence of increasing stopping times, then $(X_{T_n})_{n \in \omega}$ is a supermartingale adapted to $(\calF_{T_n})_{n \in \omega}$ if one of the following conditions holds:
\begin{enumerate}
\item each $T_n$ is bounded i.e.~$T_n < c_n$ where $c_n$ is a constant
\item $(X_n)_{n\in \omega}$ is uniformly integrable.
\end{enumerate}
\end{theorem}

\begin{theorem}[{\citep[Lemma 5.5]{DBLP:conf/popl/FioritiH15}, \citep[Prop 1]{DBLP:conf/popl/ChatterjeeFNH16}}]
\label{thm:rank-PAST}
Let $(Y_n)_{n \in \omega}$ be a $\epsilon$-ranking supermartingale with respect to the stopping time $T$, then $T < \infty$ almost surely, and $\expect{T} \leq \dfrac{\expect{Y_0}}{\epsilon}$.
\end{theorem}

\begin{proof}[Proof Sketch]
First establish by induction on $n$: 
\[
\forall n \in \omega \, . \, \expect{Y_n} \leq \expect{Y_0} - \epsilon \cdot \Big(\sum_{i=0}^{n-1} \mathbb{P}[T > i]\Big).
\]
It follows that $\sum_{i=0}^{\infty} \mathbb{P}[T > i]$ converges; hence $\lim_{n \to \infty} \mathbb{P}[T > i] = 0$ and so $\mathbb{P}[T < \infty] = 1$.
Then it suffices to prove: if $T < \infty$ a.s., then $\expect{T} = \sum^\infty_{i=0} \mathbb{P}[T > i]$.
\end{proof}

\medskip

A main result is the following theorem.
\begin{theorem} 
\label{thm:rankable and strict rankable}
If a closed SPCF term is rankable (respectively, strictly rankable) by $f$, then $(f(M_n))_{n \in \omega}$ is a 
supermartingale (respectively, ranking supermartingale) adapted to the filtration $(\mathcal{F}_n)_{n \in \omega}$ where $\mathcal{F}_n = \sigma(M_1, \cdots, M_n)$.
\end{theorem}


\paragraph{Proof of \Cref{thm:rankable and strict rankable}.}
\emph{Notation}. Given an $\omega$-sequence (e.g.~$s \in S$) and $m \in \omega$, we write $s_{\leq m} \in I^m$ to mean the $m$-long prefix of $s$.

Fix a closed SPCF term $M \in \Lambda^0$, and a ranking function $f$ for it, satisfying $f(N) = 0$ iff $N$ is a value.
Let $n \in \omega$, define the following random variables on the probability space $(S, \calF, \mu)$:
\begin{align*}
M_n(s) &:= \pi_0 (\red^n(M, s))\\
\ndraw{n}{s} &:= |\{k < n \mid \exists E: M_k(s) = E[\tsample]\}|,
\end{align*}
 so that $\pi_1 (\red^n(M, s)) = \underbrace{\pi_t( \cdots (\pi_t}_{\ndraw n s}(s))$, and the filtration $\mathcal{F}_n = \sigma(M_1, \cdots, M_n)$.
The $\calF_n$-measurability of $M_n$ (and hence of $\#\mathrm{draw}_n$) follows from \citep{DBLP:conf/icfp/BorgstromLGS16};
$\#\mathrm{draw}_n$ is a stopping time (bounded by $n$). \akr{$\#\mathrm{draw}_n$ is not a stopping time. It doesn't look like you actually use this claim anyway, so it should just be fine to remove, but did you mean something different?}
Take $s \in A \in \calF_n$ with $\ndraw{n}{s} = l$.
For any $s'\in S$, if $s_{\leq l} = s_{\leq l}'$ then $s' \in A$.
It follows that ${\set{s_{\leq l}} \cdot I^\omega} \subseteq A$.
\lo{NOTE. To fix notation:
\(
\sigma(M_{n}) = \sigma\big(\set{M_{n}^{-1}(\alpha_j, U_{\alpha_j})
\mid \alpha_j\in \mathsf{Sk}_j, U_{\alpha_j} \in \mathcal{B}(\Real^j), j \geq 0}\big)
\)
}
%Take $M \in \Lambda^0$ (closed SPCF terms). Define, for each $n \in \omega$, the random variable $M_n : S \to \Lambda^0$ by $M_n(s) := \pi_0 (\red^n(M, s))$.

\iffalse
\lo{We need to show that each $f(M_n)$ is integrable. LO claimed earlier that $0 \leq f(N) \leq f(M)$ for all $N \in \mathit{Rch}(M)$.
This is of false: take $f$ with $v \, \tsample \mapsto 1$ and $v \, \underline{r} \mapsto 2\, r$ for some value $v$. 
However, it is true that
\[
\int_{S} \mu(\dif s) \, |f(M_n)(s)| 
=
\int_{S} \mu(\dif s) \, f(M_n)(s)
\leq
\int_{S} \mu(\dif s) \, f(M_0)(s)
=
f(M)
\]
the inequality above follows from \Cref{lem:key rankable}.
}
\fi

We say that a given SPCF term is \emph{type-1} (respectively 2, 3 and 4) if it has the shape $E[\tY \lambda x. N]$ (respectively $E[\tsample]$, $E[R]$ where $R$ is any other redex, and of a value).
Define $\mathbf{T}_i := 
\set{s \mid M_n(s) \hbox{ is type-$i$}}$.
It is straightforward to see that each $\mathbf{T}_i \in \calF_n$, and $\set{\mathbf{T}_1, \mathbf{T}_2, \mathbf{T}_3, \mathbf{T}_4}$ is a partition of $S$.
%By abuse of language, we refer to the three respective types of redexes as type-$1$, 2 and 3.
Hence it suffices to prove the following lemma.

\iffalse
For $i \in \{1, 2, 3\}$ and $n \in \omega$, define function $f_{i, n+1}(M) : S \to \Real$ by
\[
f_{i, n+1}(M)(s) :=
\begin{cases}
f(M_{n+1}(s)) & \hbox{if }s \in \mathbf{T}_i := 
\set{s \mid M_n(s) \textrm{ is type-$i$}}\\
0 & \textrm{otherwise}
\end{cases}
\]
\begin{lemma}
\label{lem:inde}
For each $i$ and $n$, $f_{i, n+1}(M)$ is %not just $\mathcal{F}_{n+1}$-measurable but also 
$\mathcal{F}_{n}$-measurable.
\end{lemma}

\begin{proof} First fix notation
\begin{align*}
\sigma(M_{n+1}) &= \sigma\big(\set{M_{n+1}^{-1}(U_{\beta_k})
\mid \beta_k\in \mathsf{Sk}_k, U_{\beta_k} \in \mathcal{B}(\Real^k), k \geq 0}\big)\\
\sigma(M_{n}) &= \sigma\big(\set{M_{n}^{-1}(U_{\alpha_j})
\mid \alpha_j\in \mathsf{Sk}_j, U_{\alpha_j} \in \mathcal{B}(\Real^j), j \geq 0}\big);
\end{align*}
and for $\beta_k \in \mathsf{Sk}_i$, we write $\beta_k[\dagger]$ to mean the instantiation of $\beta_k$ by a $k$-vector of numerals $\dagger$; $\alpha_j[\dagger']$ has the same meaning.
Now for each $\beta_k[\dagger] = E[R']$ where $R'$ is the contractum of a type-$i$ redex $R$, there exist $\alpha_j \in \mathsf{Sk}_j$ and $j$-vector $\dagger'$ such that $\alpha_j[\dagger'] = E[R]$ of type $i$.
Moreover, for each $U_{\beta_k}$, there exists $U_{\alpha_j}$ such that $M_{n+1}^{-1}(U_{\beta_k}) = M_{n}^{-1}(U_{\alpha_j}) \in \sigma(M_n)$.
\end{proof}

Plainly
\(
f(M_{n+1}) = %\sum_{i=1}^3 f_{i, n+1}(M).
f_{1, n+1}(M) + f_{2, n+1}(M) + f_{3, n+1}(M),
\)
$\mu$-almost-surely.
%(Unless otherwise stated, all equations and inequations between random variables are assumed to hold only $\mu$-a.s.)
Therefore 
\[
\expect{f(M_{n+1}) \mid \mathcal{F}_n} \nonumber \\
= 
\expect{f_{1, n+1}(M) \mid \mathcal{F}_n} + \expect{f_{2, n+1}(M) \mid \mathcal{F}_n} + \expect{f_{3, n+1}(M) \mid \mathcal{F}_n} 
\]
\begin{align}
& \expect{f(M_{n+1}) \mid \mathcal{F}_n} \nonumber \\
& = \expect{f_{1, n+1}(M) + f_{2, n+1}(M) + f_{3, n+1}(M)  \mid \mathcal{F}_n} 
\nonumber \\
& = \expect{f_{1, n+1}(M) \mid \mathcal{F}_n} + \expect{f_{2, n+1}(M) \mid \mathcal{F}_n} + \expect{f_{3, n+1}(M) \mid \mathcal{F}_n} 
\label{eqn:linear cond ex} 
& = f_{1, n+1}(M) + f_{2, n+1}(M) + f_{3, n+1}(M)
\label{eqn:inde cond exp}
& = f_{1, n+1}(M) + \int_I f() + f_{3, n+1}(M) \label{eqn:inde cond exp} \\
\end{align}
\Cref{eqn:linear cond ex} follows from the linearity of conditional expectation. 
%\Cref{eqn:inde cond exp} is justified because $f_{i, n+1}(M)$ is $\mathcal{F}_n$-measurable (\Cref{lem:inde}).

It remains to show: for all $A \in \calF_n$
\begin{align}
& \int_A \dif \mu \, \expect{f(M_{n+1}) \mid \mathcal{F}_n} \nonumber \\
& \leq  \int_A \mu (\dif s) \, \big(f(M_{n})[s \in \mathbf{T}_1] + f(M_{n})[s \in \mathbf{T}_2] + f(M_{n})[s \in \mathbf{T}_3]\big)
\label{eqn:ts} \\
& = \int_A \dif \mu \, f(M_n). 
\label{eqn:partition} 
\end{align}
where $[s \in \mathbf{T}_i]$ is the Iverson bracket.

Here we use the notation: given function $g : S \to \Real$ and $U \subseteq S$, $g[U] : S \to \Real$ denotes the function
\[
g[U](x) :=
\begin{cases}
g(x) & \textrm{$x \in U$}\\
0 & \textrm{otherwise}
\end{cases}
\]

The inequality (\ref{eqn:ts}) follows immediately from the lemma below; 
and (\ref{eqn:partition}) holds because $\{\mathbf{T}_1, \mathbf{T}_2, \mathbf{T}_3, \mathbf{T}_4\}$ is a partition of $S$.
\fi

\begin{lemma}
\label{lem:key rankable}
For all $i \in \set{1, 2, 3, 4}$ and $A \in \calF_n$
\[
\int_A \mu(\dif s) \, f_{n+1}(M)[s \in \mathbf{T}_i] \leq \int_A \mu(\dif s) \, f(M_n)[s \in \mathbf{T}_i]
\] 
where $[s \in \mathbf{T}_i]$ is the Iverson bracket.
\end{lemma}

\begin{proof}
We show the non-trivial case of $i = 2$.
First we express 
\begin{equation}
f_{n+1}(M)[s \in \mathbf{T}_2] = \sum_{i \in \calI} 
f(E_i[\underline{\sigma_i(s)}][\underline{\rho(s)}])[s \in U_i]
\label{eqn:f 2 n+1}
\end{equation}
where 
\begin{itemize}
\item $\calI$ is a countable indexing set
\item $E_i[\cdot][\tsample] \in \mathsf{Sk}_{j_i}$, and $\sigma_i : S \to \Real^{j_i}$, and $\rho(s) = \pi_h(\pi_1(\red^n(M, s))) \in \Real$ 
\item $\set{U_i}_{i \in \calI}$ is a partition of $\mathbf{T}_2$, 
where each $U_i$ is determined by a skeletal term $E_i[\cdot][\tsample]$ and a number (of draws) $l_i \leq n$;
precisely 
\[
U_i := \#\mathrm{draw}_n^{-1}(l_i) \cap M_n^{-1}(E_i[\cdot][\tsample], \Real^{j_i}) \in \calF_n.
\]
\lo{I.e.~$(E_i[\underline{\sigma_i(s)}][{\tsample}], \pi_t^{l_i}(s)) = \red^n(M, s)$ iff $s \in U_i$.}
\end{itemize}

Observe that if $s \in U_i$ then $\set{s_{\leq l_i}} \cdot I^\omega \subseteq U_i$;
in fact $(U_i)_{\leq l_i} \cdot I^\omega = U_i$.
%Moreover, for any $A \in \calF_n$, if $s \in A$ then $\set{s_{\leq l}} \cdot I^\omega \subseteq A$.
This means that for any measurable $g : S \to \Real_{\geq 0}$, if $g(s)$ only depends on the $(l_i+1)$-long prefix of $s$, then, writing $g' : I^{l_i+1} \to \Real_{\geq 0}$ where $g(s) = g'(s_{\leq l_i+1})$ 
\begin{equation}
\int_{A \cap U_i}  \mu(\dif s) \, g(s) = 
\int_{(A \cap U_i)_{\leq l_i+1}} \Leb_{l_i+1}(\dif t) \, g'(t)
\label{eqn:truncate}
\end{equation}
(For simplicity, we will write $g' = g$ in the following.)
%where $A_{\leq m} := \set{s_{\leq m} \mid s \in A}$. 

Take $s \in U_i$, and set $l = l_i$.
Now $\sigma_i(s)$ depends on $s_{\leq l}$, and $\rho(s)$ depends on $s_{\leq l +1}$.
Let $u$ range over $(U_i)_{\leq l}$.
Then, it follows from the definition of $f$ that
\[
\int_I \Leb(\dif r) \, f(E_i[\underline{\sigma_i(u)}][\underline{r}])) \leq f(E_i[\underline{\sigma_i(u)}][\tsample])).
\]
Take $A \in \calF_n$, and integrating both sides, we get
\begin{align*}
& \int_{(A \cap U_i)_{\leq l}} \Leb_l(\dif u) \, \int_I \textrm{Leb}(\dif r) \, f(E_i[\underline{\sigma_i(u)}][\underline{r}])) \\
\leq & \int_{(A \cap U_i)_{\leq l}} \Leb_l(\dif u) \, f(E_i[\underline{\sigma_i(u)}][\tsample]))
\end{align*}
Since $\Leb_{l+1}$ is the (unique) product measure satisfying $\Leb_{l+1}(V \times B) = \Leb_l(V) \cdot \Leb(B)$, and $(U_i)_{\leq l_i} \cdot I^\omega = U_i$, we have
%it follows from the preceding observation that
\begin{align*}
& \int_{(A \cap U_i)_{\leq l+1}} \Leb_{l+1}(\dif u') \, f(E_i[\underline{\sigma_i(u')}][\underline{\rho(u')}])) \\
& \leq 
\int_{(A \cap U_i)_{\leq l+1}} \Leb_{l+1}(\dif u') \, f(E_i[\underline{\sigma_i(u')}][\tsample]))
\end{align*}
and so 
\begin{equation}
\int_{A \cap U_i} \mu(\dif s) \, f(E_i[\underline{\sigma_i(s)}][\underline{\rho(s)}])) \\
\leq 
\int_{A \cap U_i} \mu(\dif s) \, f(E_i[\underline{\sigma_i(s)}][\tsample])).
\label{eqn:a u ui}
\end{equation}

Finally, integrating both sides of (\ref{eqn:f 2 n+1}), we have
\begin{align*}
\int_A \mu(\dif s) \, f_{n+1}(M)[s \in \mathbf{T}_i] 
&= 
\int_A \mu(\dif s) \, \sum_{i \in \calI} f(E_i[\underline{\sigma_i(s)}][\underline{\rho(s)}])[s \in U_i] \\
&= 
\sum_{i \in \calI} \int_{A \cap U_i} \mu(\dif s) \, f(E_i[\underline{\sigma_i(s)}][\underline{\rho(s)}])\\
&\leq 
\sum_{i \in \calI} \int_{A \cap U_i} \mu(\dif s) \, f(E_i[\underline{\sigma_i(s)}][\tsample])
\quad \hbox{$\because$ (\ref{eqn:a u ui})}\\
&= 
\int_{A} \mu(\dif s) \, \sum_{i \in \calI} \, f(E_i[\underline{\sigma_i(s)}][\tsample])[s \in U_i]\\
&= \int_{A} \mu(\dif s) f_{n}(M)[s \in \mathbf{T}_2]
%\quad \hbox{$\because$ (\ref{eqn:f 2 n+1})}
\end{align*}

\end{proof}

As an immediate corollary of \Cref{lem:key rankable}, each $f(M_n)$ is integrable:
\[
\int_{S} \mu(\dif s) \, |f(M_n)(s)| 
=
\int_{S} \mu(\dif s) \, f(M_n)(s)
\leq
\int_{S} \mu(\dif s) \, f(M_0)(s)
=
f(M).
\]

\iffalse
[** To see $f_{2, n+1}(M) \leq f(M_n)[T_2]$, take $s \in T_2$. Then $f_{2, n+1}(M)(s) = f(E[\underline{a}])$ and $M_n = E[\tsample]$, for some $a \in [0, 1]$ and evaluation context $E$. Hence 
\[
f_{2, n+1}(M)(s) \leq \int_I f(E[\underline{r}]) \, \mu_{leb}(\textrm{d} r)
\leq f(E[\tsample]) = f(M_n)[T_2](s).
\]
**]
\fi
This concludes the proof of \Cref{thm:rankable gives supermartingale}. \hfill \qed

\medskip
As before, fix a $M \in \Lambda^0$.
Define random variables on $(S, \calF, \mu)$:
\begin{align}
T_{-1}(s) & := -1 \nonumber \\
T_{n+1}(s) & := \min \{ k \mid k>T_n(s), M_k(s) \textrm{ a value or of form } E[\tY \lambda x. N] \} \nonumber \\
X_n(s) & := f(M_n(s)) \nonumber \\
Y_n(s) & := X_{T_n}(s) \nonumber \\
T_M^\tY(s) &:= \min \set{n \mid M_{T_n(s)}(s) \textrm{ is a value}} \label{eq:y stopping time}
\end{align}
The random variable $T_M^\tY$ is known as the \emph{$\tY$-runtime of $M$}, and it can equivalently be defined as the number of $\tY$-reduction steps in the reduction sequence of $M$. Note that, as the type system ensures that the reduction relation excluding $\tY$-reduction is strongly normalising, only finitely many reductions can occur in a row without one of them being a $\tY$-reduction, therefore $T_M^\tY < \infty$ iff $M$ is AST.

\begin{theorem}[Soundness]
\begin{enumerate}
\item If a closed SPCF term $M$ is rankable, then $T^{\tY}_M < \infty$ a.s.~(equivalently $M$ is AST), and $\expect{T^{\tY}_M} < \infty$. 

\item If a closed SPCF term $M$ is strictly rankable, then $\expect{T_M} < \infty$ i.e.~$M$ is PAST.
\end{enumerate}
\end{theorem}

\begin{proof}
For 1, observe that $T_0, T_1, T_2, \cdots$ are an increasing sequence of stopping times adapted to $(\calF_n)_{n \in \omega}$, and each $T_i$ is bounded.
Thanks to \Cref{thm:rankable and strict rankable}, 
$(X_n)_{n \in \omega}$ is a supermartingale;
it then follows from \Cref{thm:optional sampling} that $(Y_n)_{n \in \omega}$ is a ($1$-ranking) supermartingale with respect to the filtration $(\calF_{T_n})_{n \in \omega}$ and the stopping time $T^{\tY}_M$.
Therefore, by \Cref{thm:rank-PAST}, $T^{\tY}_M < \infty$ a.s.~and $\expect{T^{\tY}_M} < \infty$.

Statement 2 follows immediately from \Cref{thm:rank-PAST}.
\end{proof}

Thus the method of (strict) ranking function is sound for proving (positive) a.s.~termination of SPCF programs.
It is in fact also complete in a sense: if $\expect{T^\tY_N} < \infty$ for all $N \in \mathit{Rch}(M)$ then $M$ is rankable (\Cref{thm:minimal}).
