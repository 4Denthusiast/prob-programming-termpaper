% !TEX root = main.tex
\section{Positions}

A \emph{position} is a finite sequence of steps into a term, defined inductively as
\begin{align*}
\alpha ::= & \; {\cdot} \mid \lambda ; \alpha \mid @_1 ; \alpha \mid @_2 ; \alpha \mid \underline f_i ; \alpha \\
& {} \mid \tY ; \alpha \mid \textsf{if}_1 ; \alpha \mid \textsf{if}_2 ; \alpha \mid \textsf{if}_3 ; \alpha.
\end{align*}
The \emph{subterm of $M$ at $\alpha$}, denoted $M \mid \alpha$, is defined as
\begin{align*}
M \mid \cdot & = M \\
\lambda x. M \mid \lambda ; \alpha & = M \mid \alpha \\
M_1 \, M_2 \mid @_i ; \alpha & = M_i \mid \alpha \quad \text{for } i = 1,2 \\
\underline f(M_1,\dots,M_n) \mid \underline f_i ; \alpha & = M_i \mid \alpha \quad \text{for }i \leq n \\
\tY M \mid \tY ; \alpha & = M \mid \alpha \\
\tif{M_1 < 0}{M_2}{M_3} \mid \textsf{if}_i ; \alpha & = M_i \mid \alpha \quad \text{for } i = 1,2,3
\end{align*}
so that every subterm is located at a unique position, but not every position corresponds to a subterm (e.g. $x \, y \mid \lambda$ is undefined). 
A position such that $M\mid \alpha$ does exist is said to \emph{occur} in $M$. 
%\changed[lo]{
\emph{Substitution} of $N$ at position $\alpha$ in $M$, written $M[N/\alpha]$, is defined similarly.
For example, let 
\(
M = \lambda x \, y. y \, (\tif{x < 0}{y \, (\underline f (x))}{\underline 3})
\)
and
\(
\alpha =\lambda ; \lambda ; @_2 ; \textsf{if}_2 ; @_2
\)
then 
\(
M[\tsample / \alpha] = \lambda x \, y. y \, (\tif{x < 0}{y \, \tsample}{\underline 3}).
\)
%}

Two subterms $N_1$ and $N_2$ of a term $M$, corresponding to positions $\alpha_1$ and $\alpha_2$, can overlap in a few different ways. 
If $\alpha_1$ is a prefix of $\alpha_2$ (written as $\alpha_1 \leq \alpha_2$), then $N_2$ is also a subterm of $N_1$. If neither $\alpha_1 \leq \alpha_2$ nor $\alpha_1 \geq \alpha_2$, the positions are said to be \emph{disjoint}. 
The notion of disjointness is mostly relevant in that if $\alpha_1$ and $\alpha_2$ are disjoint, performing a substitution at $\alpha_1$ will leave the subterm at $\alpha_2$ unaffected.

Thus we can define a nondeterministic reduction relation $\to$.
\begin{definition}
\label{def:more general red}
The binary relation $\to$ is defined by the following rules, each is conditional on a redex occurring at position $\alpha$ in the term $M$:
\begin{align*}
  \text{if } M \mid \alpha = (\lambda x.N) V,\ & M \to M[N[V/x]/\alpha] \\
  \text{if } M \mid \alpha = \underline f (\underline r_1, \dots , \underline r_n),\ & M \to M[\underline{f(r_1,\dots,r_n)}/\alpha] \\
  \text{if } M \mid \alpha = \tY \lambda x. N,\ & M \to M[\lambda z. N[(\tY \lambda x. N)/x] z/\alpha]\\ \text{where }z&\text{ is not free in $N$}\\
  \text{if } M \mid \alpha = \tif{\underline r < 0}{N_1}{N_2},\ & M \to M[N_1/\alpha] \text{ where }r < 0 \\
  \text{if } M \mid \alpha = \tif{\underline r < 0}{N_1}{N_2},\ & M \to M[N_2/\alpha] \text{ where }r \geq 0 \\
  \text{if } M \mid \alpha = \tsample \text{ and $\lambda$ does}&\text{ not occur after $@_2$ or $\tY$ in $\alpha$},\\ & M \to M[\underline r/\alpha] \text{ where } r \in [0,1].
\end{align*}
In each of these cases, $M \mid \alpha$ is the \emph{redex}, and the reduction \emph{takes place at $\alpha$}. Each subterm can be a redex in at most one way, but there can be multiple redexes at different positions.
\end{definition}

The argument of a $\beta$ redex and the body of a $\tY$ redex may be duplicated by those reductions. It is therefore these cases that need to be handled carefully to avoid duplicating samples at the wrong time. In both cases, the potentially duplicated part must already be a value, which excludes terms like $\tsample$ or $\tsample + \underline 1$, which should be evaluated before being duplicated. In the other direction, if a $\tsample$ occurs inside of a $\lambda$, it may need to be duplicated before being evaluated, which is why a $\tsample$ reduction isn't allowed inside a $\lambda$ inside a $\tY$ or the right side of an application. These restrictions are in some cases unnecessarily strict, for example, in $(\lambda x. x) ((\lambda y. \tsample) \underline 0)$, it would be fine to evaluate the sample first, but they are at least sufficient to ensure confluence in terms of the distribution of results. Getting individual traces to behave correctly will take more work though.

\section{Skeletal Reduction Sequences}
Labelling the pre-chosen samples by the positions in the term by using $I^{\{\alpha \mid \, (M \mid \alpha) = \tsample\}}$ as the trace space would not be sufficient to solve the issue of different samples being used in corresponding locations in different reduction sequences because in some cases, a $\tsample$ will be duplicated before being reduced, for example, in $(\lambda x. x \, {\underline 0} \, \mathbin{\underline{+}} \, {x \, \underline 0}) (\lambda y. \tsample)$, both of the $\tsample$ redexes that eventually occur originate at $@_2 ; \lambda$. 
It is therefore necessary to consider possible positions that may occur in other terms reachable from the original term. 
Even this is itself inadequate because some of the positions in different reachable terms need to be considered the same, and the number of reachable terms is in general uncountable, which leads to measure-theoretic issues.

We are thus led to consider the reduction relation on skeletons. Define a \emph{skeleton} to be a term but, instead of having real constants $\underline r$, it has a placeholder $\skeletonPlaceholder$, so that each term $M$ has a skeleton $\mathit{Sk}(M)$, and each skeleton $S$ can be converted to a term $S[r]$ given a vector $r$ of $n$ real numbers to substitute in, where $n$ is the number of occurrences of $\skeletonPlaceholder$ in $S$. Positions in a skeleton and the reduction relation $\to$ on skeletons can be extended from the definitions on terms in the obvious way, with $\tif{\skeletonPlaceholder < 0}{A}{B}$ reducing nondeterministically to both $A$ and $B$, $\tsample$ reducing to $\skeletonPlaceholder$, and $\skeletonPlaceholder$ considered as (the skeletal equivalent of) a value, so that $(\lambda x.A) \, \skeletonPlaceholder$ reduces to $A[\skeletonPlaceholder / x]$.
For example, we have 
\(
(\lambda x . \tif{x < 0}{x}{\skeletonPlaceholder}) \, \tsample
\to
(\lambda x . \tif{x < 0}{x}{\skeletonPlaceholder}) \, \skeletonPlaceholder
\to
\tif{\skeletonPlaceholder < 0}{\skeletonPlaceholder}{\skeletonPlaceholder}
\to
\skeletonPlaceholder
\).

Given a closed term $M$, let $L_0(M)$ be the set of pairs, the first element of which is a $\to$-reduction sequence of skeletons starting at $\mathit{Sk}(M)$, and the second of which is a position in the final skeleton of the reduction sequence. 
As with the traces from $I^{\mathbb N}$ used to pre-select samples to use in the standard trace semantics, modified traces, which are elements of $I^{L_0(M)}$ (with one more caveat introduced after Def. \ref{defn:sim}), will be used to pre-select a sample from $I$ for each element of $L_0(M)$, which will then be used if a $\tsample$ reduction is ever performed at that position.


A (skeletal) reduction sequence is assumed to contain the information on the locations of all of the redexes as well as the actual sequence of skeletons that occurs. For example, $(\lambda x. x) ((\lambda x. x) \, \skeletonPlaceholder)$ could reduce to $(\lambda x. x) \, \skeletonPlaceholder$ with the redex at either $\cdot$ or $@_2$, and these give different reduction sequences.

\begin{example}%[Labelling samples by terms]
Consider the terms
\begin{align*}
A[M] &= \tif{\tif{M>0}{I}{I} (\lambda y.\tsample)\, \underline 0 - \underline{0.5} > 0}{\underline 0}{\Omega}\\
B &= \tif{\tsample - \underline{0.5} > 0}{\underline 0}{\Omega}
\end{align*}
If terms rather than skeletons were used to label samples, the set of modified traces where $A[\tsample]$ terminates would be
\[
\begin{array}{l}
\bigcup_{r \in [0,1]} \big\{s \mid\ s(A[\tsample], \mathsf{if}_1;\underline{-}_1;@_1;@_1;\mathsf{if}_1) = r, \\
\qquad\qquad\qquad s(A[\tsample] \to A[\underline r] \to^* B, \mathsf{if}_1 ; \underline{-}_1) > 0.5 \big\}.
\end{array}
\]
\iffalse
\lo{The occurrence of $L_s(M)$ in the set comprehension below should be $L_0(M)$?}

\lo{I think we should write ``$[A[\tsample], A[\underline r], \ldots, B$'' in the line above as ``$A[\tsample] \to A[\underline r] \to^\ast B$''; similarly $\mathit{Sk}(A[\tsample]) \to \mathit{Sk}(A[\underline 0]) \to^\ast \mathit{Sk}(B)$ below.}
\fi
This is a rather unwieldy expression, but the crucial part is that $r$ occurs twice in the conditions on $s$: once as the value a sample must take, and once in the location of a sample. 
As this set is unmeasurable, the termination probability would not even be well-defined. 
Labelling samples by skeletons instead, this problem does not occur because there are only countably many skeleton, and at each step in a reduction sequence, only finitely many could have occurred yet. Although skeletal reduction sequences omit the information on what the results of sampling were, they still contain all the necessary information on how many, and which, reductions took place.

For this particular term, $\mathit{Sk}(A[\underline r])$ does not depend on the value of $r$, therefore the set where it terminates becomes simply the following, which is measurable.
\begin{align*}
\big\{s \mid s(\mathit{Sk}(A[\tsample]) \to \mathit{Sk}(A[\underline 0]) \to^* \mathit{Sk}(B), \mathsf{if}_1 ; \underline{-}_1) > 0.5 \big\}
\end{align*}
\end{example}

Reduction sequences are used rather than reachable skeletons because if the same skeleton is reached twice, different samples may be needed:

\begin{example} Consider the term $M = \tY (\lambda f x.\, \tif{\tsample - \underline{0.5} < 0}{f \, x}{x}) \, \underline 0$, which reduces after a few steps to $N = \tif{\tsample - \underline{0.5}  < 0}{M}{\underline 0}$. If we label samples by just skeletons and positions, and the pre-selected sample for $(\mathit{Sk}(N),\textsf{if}_1;\underline{-}_1)$ is less than 0.5, $N$ reduces back to $M$, then $N$ again, then the same sample is used the next time, therefore it's an infinite loop, whereas if samples are labelled by reduction sequences, the samples for $M \to^\ast N$ are independent from the samples for $M \to^\ast N \to M \to^\ast N$, and so on.
\end{example}

The reduction sequences of skeletons will often be discussed as though they were just skeletons, identifying them with their final skeletons. With this abuse of notation, a reduction sequence $N$ (actually $N_1 \to^\ast N_n = N$) may be said to reduce to a reduction sequence $O$, where the reduction sequence implicitly associated with the final skeleton $O$ is $N_1 \to^\ast N_n \to O$.

\section{Potential Positions}
This is still not quite sufficient to attain confluence because sometimes the same samples must be used at corresponding positions in different reduction sequences. 

\begin{example} \label{ex:cousin sim} 
The term $M = \tsample + \tsample$ has the reachable skeletons $N_1 = \skeletonPlaceholder + \tsample, N_2 = \tsample + \skeletonPlaceholder, O = \skeletonPlaceholder + \skeletonPlaceholder$ and $\skeletonPlaceholder$, with reductions $M \to N_1 \to O \to \skeletonPlaceholder$ and $M \to N_2 \to O \to \skeletonPlaceholder$. 
In the reduction $M \to N_1$, the sample labelled $(M, \underline{+}_1)$ is used, and in the reduction $N_2 \to O$, the sample labelled $(M \to N_2, \underline{+}_1)$ is used. 
Each of these samples becomes the value of the first numeral in $O$ in their respective reduction sequences, therefore in order for confluence to be attained, they must be the same. Which elements of $L_0(M)$ must match can be described by the relation $\sim^*$:
\end{example}

\begin{definition} \label{defn:sim}
The relation $\sim$ is defined as the union of the minimal symmetric relations $\sim_p$ (``$p$'' for parent-child) and $\sim_c$ (``$c$'' for cousin) satisfying
\begin{asparaenum}[(i)]
    \item If $N$ reduces to $O$ with the redex at position $\alpha$, and $\beta$ is a position in $N$ disjoint from $\alpha$, then $(N,\beta) \sim_p (O,\beta)$.
    
    \item If $N$ $\beta$-reduces to $O$ at position $\alpha$, $\beta$ is a position in $N \mid \alpha;@_1;\lambda$ and $N \mid \alpha;@_1;\lambda;\beta$ is not the variable involved in the reduction, $(N,\alpha;@_1;\lambda;\beta) \sim_p (O, \alpha;\beta)$
    
    \item If $N$ $\textsf{if}$-reduces to $O$ at position $\alpha$, with the first resp. second branch being taken, and $\alpha;\textsf{if}_i;\beta$ occurs in $N$ (where $i = 2$ resp. $3$), $(N,\alpha;\textsf{if}_i;\beta) \sim_p (O,\alpha;\beta)$
    
    \item If $N$, $O_1$ and $O_2$ match any of the following cases:
    \begin{compactenum}
        \item $N$ contains redexes at disjoint positions $\alpha_1$ and $\alpha_2$, $O_1$ is $N$ reduced first at $\alpha_1$ then $\alpha_2$ and $O_2$ is $N$ reduced first at $\alpha_2$ then at $\alpha_1$.
        
        \item $N \mid \alpha = \tif{\underline r < 0}{N_1}{N_2}$, where $r < 0$ (or, respectively, $r \geq 0$), $(N_2 \text{ resp. } N_1) \mid \beta$ is a redex, and $O_1$ is $N$ reduced at $\alpha$ and $O_2$ is $N$ reduced first at $\alpha;(\textsf{if}_3 \text{ resp. } \textsf{if}_2);\beta$ then at $\alpha$
        
        \item $N \mid \alpha = \tif{\underline r < 0}{N_1}{N_2}$, where $r < 0$ (or, respectively, $r \geq 0$), $(N_1 \text{ resp. } N_2) \mid \beta$ is a redex, and $O_1$ is $N$ reduced first at $\alpha$ then at $\alpha;\beta$ and $O_2$ is $N$ reduced first at $\alpha;(\textsf{if}_2 \text{ resp. } \textsf{if}_3);\beta$ then at $\alpha$
        
        \item $N \mid \alpha = (\lambda x. A) B$, there is a redex in $A$ at position $\beta$, $O_1$ is $N$ reduced first at $\alpha$ then at $\alpha;\beta$, and $O_2$ is $N$ reduced first at $\alpha;@_1;\lambda;\beta$ then at $\alpha$
        
        \item $N \mid \alpha = (\lambda x. A) B$, $B \mid \beta$ is a redex, $(\gamma_i)_i$ is a list of all the positions in $A$ where $A \mid \gamma = x$, ordered from left to right, $O_1$ is $N$ reduced first at $\alpha ; @_2 ; \beta$ then at $\alpha$, and $O_2$ is $N$ reduced first at $\alpha$ then at $\alpha;\gamma_i;\beta$ for each $i$ in order.
        
        \item $N \mid \alpha = \tY (\lambda x. A)$, $A$ reduced at $\beta$ is $A'$, $(\gamma_i)_i$ is a list of all the positions where $A' \mid \gamma  = x$, ordered from left to right, $O_1$ is $N$ reduced first at $\alpha;\tY;\lambda;\beta$ then at $\alpha$, and $O_2$ is $N$ reduced first at $\alpha$ then at $\alpha;\lambda;@_1;\gamma_i;Y;\lambda;\beta$ for each $i$ in order where $\gamma_i$ is left of $\beta$ then at $\alpha;\lambda;@_1;\beta$ then at $\alpha;\lambda;@_1;\gamma_i;Y;\lambda;\beta$ for the remaining values of $i$.
    \end{compactenum}
    (in which case $O_1$ and $O_2$ are equal as skeletons, but with different reduction sequences), $O_1'$ and $O_2'$ are the results of applying some reduction sequence to each of $O_1$ and $O_2$ (the same reductions in each case, which is always possible because they're equal skeletons), and $\delta$ is a position in $O_1'$ (or equivalently $O_2'$), then $(O_1',\delta) \sim_c (O_2',\delta)$.
\end{asparaenum}
\end{definition}

\begin{example}
In \Cref{ex:cousin sim}, $(M,\underline{+}_1) \sim_p (M \to N_2,\underline{+}_1)$ by case i of $\sim_p$ (because the reduction $M \to N_2$ occurs at $\underline{+}_2$, which is disjoint from $\underline{+}_1$), and similarly, $(M,\underline{+}_2) \sim_p (M \to N_1,\underline{+}_2)$.

If we extend it to have three $\tsample$s, $\sim_c$ becomes necessary as well: Let $M_{sss} = \tsample + \tsample + \tsample$ (taking the three-way addition to be a single primitive function), $M_{Xss} = \skeletonPlaceholder + \tsample + \tsample$, and so on. There are then reduction sequences $M_{sss} \to M_{Xss} \to M_{XXs} \to M_{XXX} \to \skeletonPlaceholder$ and $M_{sss} \to M_{sXs} \to M_{XXs} \to M_{XXX} \to \skeletonPlaceholder$. For the first two reductions, these reduction sequences take the same samples by $\sim_p$, case i, as in \Cref{ex:cousin sim}~. The next reduction uses the samples labelled by $(M_{sss} \to M_{Xss} \to M_{XXs}, \underline{+}_3)$ and $(M_{sss} \to M_{sXs} \to M_{XXs}, \underline{+}_3)$, which are related by $\sim_c$, case a, therefore when these reduction sequences reach $M_{XXX}$, they still contain all the same numbers, as desired.
\end{example}

The reflexive transitive closure $\sim^*$ of this relation is used to define the set of \emph{potential positions} $L(M) = L_0(M) / \sim^*$, and each equivalence class can be considered as the same position as it may occur across multiple reachable skeletons. 
If $(N,\alpha) \sim^* (O,\beta)$, then $N \mid \alpha$ and $O \mid \beta$ both have the same shape (i.e.~they're either both the placeholder $\skeletonPlaceholder$, both variables, both applications, both $\tsample$s etc.), therefore it's well-defined to talk of the set of \emph{potential positions where there is a $\tsample$}, $L_s(M)$.
The new sample space is then defined as $I^{L_s(M)}$, with the Borel $\sigma$-algebra and product measure.
Since $I^{L_s(M)}$ is a countable product, the measure space is well-defined \cite[Cor.~2.7.3]{AshDD00}.

\section{The Confluent Trace Semantics}
Before defining the new version of the reduction relation, the following lemma is necessary for it to be well-defined.

\begin{restatable}{lemma}{lemmaSimMN}
\label{lem:sim-M-N}
The relation $\sim$ is defined on $L_0(M)$ with reference to a particular starting term $M$, so different versions, $\sim_M$ and $\sim_N$, can be defined starting at different terms. If $M \to N$, then $\sim^*_N$ is equal to the restriction of $\sim^*_M$ to $L_0(N)$.
\end{restatable} 


At each reduction step $M \to N$, the sample space must be restricted from $I^{L_s(M)}$ to $I^{L_s(N)}$. 
The injection $L_0(N) \to L_0(M)$ is trivial to define by appending $\mathit{Sk}(M) \to \mathit{Sk}(N)$ to each path, and using \Cref{lem:sim-M-N}, this induces a corresponding injection on the quotient, $L(N) \to L(M)$. 
The corresponding map $L_s(N) \to L_s(M)$ is then denoted $i(M \to N)$.

\begin{definition}[$\Rightarrow$ reduction]
This version of the reduction relation now specifies the results of $\tsample$ reductions, but is still nondeterministic with respect to the order of reduction. It relates $\biguplus_{M \in \Lambda_0} I^{L_s(M)}$ to itself.
We write an element of $\biguplus_{M \in \Lambda_0} I^{L_s(M)}$ as $(M', s)$ where the term $M' \in \Lambda^0$ and $s \in  I^{L_s(M')}$.
\begin{align*}
& (M,s) \Rightarrow (N,s \circ i(M \to N)) \text{ if $M \to N$ at $\alpha$ and either} \\
& \qquad \text{the redex is not $\tsample$, or} \\
& \qquad \text{$M \mid \alpha = \tsample$ and $N = M[\underline{s(\mathit{Sk}(M),\alpha)}/\alpha]$}
\end{align*}
\end{definition}

\medskip
This reduction relation now has all of the properties required of it. In particular, it can be considered an extension of the standard trace semantics (as will be seen later in \Cref{thm:AstEquivalence}), and also:
\begin{restatable}{lemma}{churchRosser} \label{churchRosser}
The relation $\Rightarrow$ is confluent.
\end{restatable}

In order to show that $\Rightarrow$ behaves as expected, the following lemma is also necessary, in order to show that a sample is never used multiple times in the same reduction sequence:
\begin{lemma} \label{lem:redexDestroyed}
If $M \to N$, with the redex at position $\alpha$, then no position in any term reachable from $N$ is related by $\sim^*$ to $(M,\alpha)$.
\end{lemma}

The reduction relation $\Rightarrow$ is nondeterministic, so it admits multiple possible reduction strategies. 
%\begin{definition}
A \emph{reduction strategy} starting from a closed term $M$ is a measurable partial function $f$ from $\mathit{Rch}(M)$ to positions, such that for any reachable term $N$ where $f$ is defined, $f(N)$ is a position of a redex in $N$, and if $f(N)$ is not defined, $N$ is a value.
Using a reduction strategy $f$, a subset of $\Rightarrow$ that isn't nondeterministic, $\Rightarrow_f$, can be defined by $(N,s) \Rightarrow_f (N',s')$ just if $(N,s) \Rightarrow (N',s')$ and $N$ reduces to $N'$ with the redex at $f(N)$.
%\end{definition}

The usual call-by-value semantics can be implemented as one of these reduction strategies, given by (with $V$ a value and $T$ a term that isn't a value and $M$ a general term)
\begin{align*}
\cbv(T M) & = @_1 ; \cbv(T) \\
\cbv(V T) & = @_2 ; \cbv(T) \\
\cbv(\underline f(V_1, \dots, V_{k-1}, T, M_{k+1}, \dots, M_n)) & = \underline f_k ; \cbv(T) \\
\cbv(\tY T) & = \tY ; \cbv(T) \\
\cbv(\tif{T < 0}{M_1}{M_2}) & = \textsf{if}_1 ; \cbv(T) \\
\cbv(V) & \text{ is undefined} \\
\cbv(T) & = \cdot \text{ otherwise}
\end{align*}
(this last case covers redexes at the root position).

A closed term $M$ terminates with a given reduction strategy $f$ and samples $s$ if there is some natural number $n$ such that $(M,s) \Rightarrow_f^n (N,s')$ where $f$ gives no reduction at $N$. 
The term \emph{terminates almost surely w.r.t.~$f$} if it terminates with $f$ for almost all $s$.

This reduction strategy allows the confluent trace semantics to be related to the standard version of the trace semantics with a fixed reduction order and linear traces. In \cite{arxiv}, which gives the full definition of the standard trace semantics, this is used to prove the following theorems that allow termination results to be transferred from the confluent trace semantics to the standard trace semantics.

\begin{restatable}{theorem}{AstEquivalence} \label{thm:AstEquivalence}
A closed term $M$ is AST with respect to $\cbv$ iff it is AST.
\end{restatable}

\begin{restatable}{theorem}{CbvIsTerminatingest} \label{thm:CbvIsTerminatingest}
If $M$ terminates with some reduction strategy $f$ and trace $s$, it terminates with $\cbv$ and $s$.
\end{restatable}

\begin{corollary}[Reduction strategy independence]
\label{cor:Reduction strategy independence}
If $M$ is AST with respect to any reduction strategy, it is AST.
\end{corollary}
\begin{proof}
Suppose $M$ is AST w.r.t.~$f$. Let the set of samples with which it terminates with this reduction strategy be $X$. By \Cref{thm:CbvIsTerminatingest}, $M$ also terminates with $\cbv$ and every element of $X$, and $X$ has measure 1, by assumption, therefore $M$ is AST with respect to $\cbv$ therefore by \Cref{thm:AstEquivalence} it is AST.
\end{proof}
