% !TEX root = main.tex
\section{Confluent trace semantics}
\label{sec:confluent}

When proving almost sure termination in this way, it is necessary to consider all of the terms a given term may reduce to, and organise them into a manageable number of cases to assign ranking function values. 
Sometimes however, the reduction that the programmer has in mind may not be strictly the call-by-value order defined so far; 
or the number of cases necessary for defining a sparse ranking function is excessive because the reduction does not proceed in a neat and orderly manner. 
In cases such as these (e.g.~\cref{ex:sum of powers}), it may be much more convenient to instead consider ranking functions with respect to alternative reduction strategies constructed to match the individual term in question, 
thereby reducing the number of cases needed and simplifying the reachable terms. 
It will be shown in \Cref{thm:confluent ranking} that ranking functions with respect to alternative reduction strategies are sound for proving AST in the usual sense (i.e.~with respect to the standard CBV reduction strategy). 
In order to obtain this result, we prove a relation between the results of using different reduction strategies, 
by first constructing a confluent trace semantics that is equivalent to the standard semantics as a special case.

Non-probabilistic lambda calculi generally have the Church-Rosser property, that if a term $A$ reduces to both $B_1$ and $B_2$, there is some $C$ 
%with reduction sequences $B_1 \to^* C$ and $B_2 \to^* C$, 
to which both $B_1$ and $B_2$ reduce,
so the reduction order mostly doesn't matter. 
In the probabilistic case, this may not be true, because $\beta$-reduction can duplicate $\tsample$s, so the outputs of the copies of the sample may be identical or independent, depending on whether the sample is taken before or after $\beta$-reduction. 
There are, however, some restricted variations on the reduction order that do not have this problem.

\medskip
Even with this restriction, a trace semantics in the style of the one already defined would not be entirely Church-Rosser, as, for example, $\red^3(\tsample - \tsample, (1,0,\dots))$ would be either $1$ or $-1$ depending on the order of evaluation of the $\tsample$s, as that determines which sample from the pre-selected sequence is used for each one. To fix this, rather than pre-selecting samples according to the order they'll be drawn in, select them according to the position in the term where they'll be used instead.

A \emph{position} is a finite sequence of steps into a term, defined inductively as
\begin{align*}
\alpha ::= & \; {\cdot} \mid \lambda ; \alpha \mid @_1 ; \alpha \mid @_2 ; \alpha \mid \underline f_i ; \alpha \\
& {} \mid \tY ; \alpha \mid \textsf{if}_1 ; \alpha \mid \textsf{if}_2 ; \alpha \mid \textsf{if}_3 ; \alpha.
\end{align*}
The \emph{subterm of $M$ at $\alpha$}, denoted $M \mid \alpha$, is defined as
\begin{align*}
M \mid \cdot & = M \\
\lambda x. M \mid \lambda ; \alpha & = M \mid \alpha \\
M_1 M_2 \mid @_i ; \alpha & = M_i \mid \alpha \quad \text{for } i = 1,2 \\
\underline f(M_1,\dots,M_n) \mid \underline f_i ; \alpha & = M_i \mid \alpha \quad \text{for }i \leq n \\
\tY M \mid \tY ; \alpha & = M \mid \alpha \\
\tif{M_1 < 0}{M_2}{M_3} \mid \textsf{if}_i ; \alpha & = M_i \mid \alpha \quad \text{for } i = 1,2,3
\end{align*}
so that every subterm is located at a unique position, but not every position corresponds to a subterm (e.g. $x \, y \mid \lambda$ is undefined). 
A position such that $M\mid \alpha$ does exist is said to \emph{occur} in $M$. 
%\changed[lo]{
\emph{Substitution} of $N$ at position $\alpha$ in $M$, written $M[N/\alpha]$, is defined similarly.
For example, let 
\(
M = \lambda x \, y. y \, (\tif{x < 0}{y \, (\underline f (x))}{\underline 3})
\)
and
\(
\alpha =\lambda ; \lambda ; @_2 ; \textsf{if}_2 ; @_2
\)
then 
\(
M[\tsample / \alpha] = \lambda x \, y. y \, (\tif{x < 0}{y \, \tsample}{\underline 3}).
\)
%}

Two subterms $N_1$ and $N_2$ of a term $M$, corresponding to positions $\alpha_1$ and $\alpha_2$, can overlap in a few different ways. 
If $\alpha_1$ is a prefix of $\alpha_2$ (written as $\alpha_1 \leq \alpha_2$), then $N_2$ is also a subterm of $N_1$. If neither $\alpha_1 \leq \alpha_2$ nor $\alpha_1 \geq \alpha_2$, the positions are said to be \emph{disjoint}. 
The notion of disjointness is mostly relevant in that if $\alpha_1$ and $\alpha_2$ are disjoint, performing a substitution at $\alpha_1$ will leave the subterm at $\alpha_2$ unaffected.

With this notation, a more general reduction relation $\to$ can be defined.
\begin{definition}
\label{def:more general red}
The binary relation $\to$ is defined by the following rules, each is conditional on a redex occurring at position $\alpha$ in the term $M$:
\begin{align*}
  \text{if } M \mid \alpha = (\lambda x.N) V,\ & M \to M[N[V/x]/\alpha] \\
  \text{if } M \mid \alpha = \underline f (\underline r_1, \dots , \underline r_n),\ & M \to M[\underline{f(r_1,\dots,r_n)}/\alpha] \\
  \text{if } M \mid \alpha = \tY \lambda x. N,\ & M \to M[\lambda z. N[(\tY \lambda x. N)/x] z/\alpha]\\ \text{where }z&\text{ is not free in $N$}\\
  \text{if } M \mid \alpha = \tif{\underline r < 0}{N_1}{N_2},\ & M \to M[N_1/\alpha] \text{ where }r < 0 \\
  \text{if } M \mid \alpha = \tif{\underline r < 0}{N_1}{N_2},\ & M \to M[N_2/\alpha] \text{ where }r \geq 0 \\
  \text{if } M \mid \alpha = \tsample \text{ and $\lambda$ does}&\text{ not occur after $@_2$ or $\tY$ in $\alpha$},\\ & M \to M[\underline r/\alpha] \text{ where } r \in [0,1].
\end{align*}
In each of these cases, $M \mid \alpha$ is the \emph{redex}, and the reduction takes place at $\alpha$.
\end{definition}

\medskip
Labelling the pre-chosen samples by the positions in the term would also not work because in some cases, a $\tsample$ will be duplicated before being reduced, for example, in $(\lambda x. x \, {\underline 0} \, \mathbin{\underline{+}} \, {x \, \underline 0}) (\lambda y. \tsample)$, both of the $\tsample$ redexes that eventually occur originate at $@_2 ; \lambda$. 
It is therefore necessary to consider possible positions that may occur in other terms reachable from the original term. 
Even this is itself inadequate because some of the positions in different reachable terms need to be considered the same, and the number of reachable terms is in general uncountable, which leads to measure-theoretic issues.

We are thus led to consider the reduction relation on skeletons (and positions in a skeleton), which can be extended from the definitions on terms in the obvious way, with $\tif{\skeletonPlaceholder < 0}{A}{B}$ reducing nondeterministically to both $A$ and $B$, $\tsample$ reducing to $\skeletonPlaceholder$, and $\skeletonPlaceholder$ considered as (the skeletal equivalent of) a value, so that $(\lambda x.A) \, \skeletonPlaceholder$ reduces to $A[\skeletonPlaceholder / x]$.
For example, we have 
\(
(\lambda x . \tif{x < 0}{x}{\skeletonPlaceholder}) \, \tsample
\to
(\lambda x . \tif{x < 0}{x}{\skeletonPlaceholder}) \, \skeletonPlaceholder
\to
\tif{\skeletonPlaceholder < 0}{\skeletonPlaceholder}{\skeletonPlaceholder}
\to
\skeletonPlaceholder
\).

Given a closed term $M$, let $L_0(M)$ be the set of pairs, the first element of which is a $\to$-reduction sequence of skeletons starting at $\mathit{Sk}(M)$, and the second of which is a position in the final skeleton of the reduction sequence. 
As with the traces from $I^{\mathbb N}$ used to pre-select samples to use in the standard trace semantics, modified traces, which are elements of $I^{L_0(M)}$ (with one more caveat introduced later), will be used to pre-select a sample from $I$ for each element of $L_0(M)$, which will then be used if a $\tsample$ reduction is ever performed at that position.


A (skeletal) reduction sequence is assumed to contain the information on the locations of all of the redexes as well as the actual sequence of skeletons that occurs. For example, $(\lambda x. x) ((\lambda x. x) \, \skeletonPlaceholder)$ could reduce to $(\lambda x. x) \, \skeletonPlaceholder$ with the redex at either $\cdot$ or $@_2$, and these give different reduction sequences.

\begin{example}[Labelling samples by terms]
Consider the terms
\begin{align*}
A[x] &= \tif{\tif{x>0}{I}{I} \, \tsample - 0.5 > 0}{0}{\Omega}\\
B &= \tif{\tsample - 0.5 > 0}{0}{\Omega}
\end{align*}
If terms rather than skeletons were used to label samples, the set of modified traces where $A[\tsample]$ terminates would be
\begin{align*}
\bigcup_{r \in [0,1]} \Big\{s \mid\ & s(A[\tsample], \mathsf{if}_1;\underline{-}_1;@_1;\mathsf{if}_1) = r, \\
& s(A[\tsample] \to A[\underline r] \to^* B, \mathsf{if}_1 ; \underline{-}_1) > 0.5 \Big\}.
\end{align*}

\iffalse
\lo{The occurrence of $L_s(M)$ in the set comprehension below should be $L_0(M)$?}

\lo{I think we should write ``$[A[\tsample], A[\underline r], \ldots, B$'' in the line above as ``$A[\tsample] \to A[\underline r] \to^\ast B$''; similarly $\mathit{Sk}(A[\tsample]) \to \mathit{Sk}(A[\underline 0]) \to^\ast \mathit{Sk}(B)$ below.}
\fi
This is a rather unwieldy expression, but the crucial part is that $r$ occurs twice in the conditions on $s$: once as the value a sample must take, and once in the location of a sample. 
This set is unmeasurable, therefore the termination probability would not even be well-defined. 
Labelling samples by skeletons instead, this problem does not occur because there are only countably many skeleton, and at each step in a reduction sequence, only finitely many could have occurred yet. Although skeletal reduction sequences omit the information on what the results of sampling were, they still contain all the necessary information on how many, and which, reductions took place.

For this particular term, $\mathit{Sk}(A[\underline r])$ does not depend on the value of $r$, therefore the set where it terminates becomes simply the following, which is measurable.
\begin{align*}
\big\{s \mid s(\mathit{Sk}(A[\tsample]) \to \mathit{Sk}(A[\underline 0]) \to^* \mathit{Sk}(B), \mathsf{if}_1 ; \underline{-}_1) > 0.5 \big\}
\end{align*}
\end{example}

Reduction sequences are used rather than reachable skeletons because if the same skeleton is reached twice, different samples may be needed. 

\begin{example} Consider the term $M = \tY (\lambda f x.\ \tif{\tsample - \underline{0.5} < 0}{f \, x}{x}) \, \underline 0$, which reduces after a few steps to $N = \tif{\tsample - \underline{0.5}  < 0}{M}{\underline 0}$. If we label samples by just skeletons and positions, and the pre-selected sample for $(\mathit{Sk}(N),\textsf{if}_1;\underline{-}_1)$ is less than 0.5, $N$ reduces back to $M$, then $N$ again, then the same sample is used the next time, therefore it's an infinite loop, whereas if samples are labelled by reduction sequences, the samples for $M \to^\ast N$ are independent from the samples for $M \to^\ast N \to M \to^\ast N$, and so on.
\end{example}

The reduction sequences of skeletons will often be discussed as though they were just skeletons, identifying them with their final skeletons. With this abuse of notation, a reduction sequence $N$ (actually $N_1 \to^\ast N_n = N$) may be said to reduce to a reduction sequence $O$, where the reduction sequence implicitly associated with the final skeleton $O$ is $N_1 \to^\ast N_n \to O$.

\medskip
This is still not quite sufficient because sometimes the same samples must be used at corresponding positions in different reduction sequences. 

\begin{example} \label{ex:cousin sim} 
The term $M = \tsample + \tsample$ has the reachable skeletons $N_1 = \skeletonPlaceholder + \tsample, N_2 = \tsample + \skeletonPlaceholder, O = \skeletonPlaceholder + \skeletonPlaceholder$ and $\skeletonPlaceholder$, with reductions $M \to N_1 \to O \to \skeletonPlaceholder$ and $M \to N_2 \to O \to \skeletonPlaceholder$. 
In the reduction $M \to N_1$, the sample labelled $(M, \underline{+}_1)$ is used, and in the reduction $N_2 \to O$, the sample labelled $(M \to N_2, \underline{+}_1)$ is used. 
Each of these samples becomes the value of the first numeral in $O$ in their respective reduction sequences, therefore in order for Church-Rosserness to be attained, they must be the same. Which elements of $L_0(M)$ must match can be described by the relation $\sim^*$:
\end{example}

\begin{definition}
The relation $\sim$ is defined as the union of the minimal symmetric relations $\sim_p$ (``$p$'' for parent-child) and $\sim_c$ (``$c$'' for cousin) satisfying
\begin{enumerate}
    \item If $N$ reduces to $O$ with the redex at position $\alpha$, and $\beta$ is a position in $N$ disjoint from $\alpha$, then $(N,\beta) \sim_p (O,\beta)$.
    
    \item If $N$ $\beta$-reduces to $O$ at position $\alpha$, $\beta$ is a position in $N \mid \alpha;@_1;\lambda$ and $N \mid \alpha;@_1;\lambda;\beta$ is not the variable involved in the reduction, $(N,\alpha;@_1;\lambda;\beta) \sim_p (O, \alpha;\beta)$
    
    \item If $N$ $\textsf{if}$-reduces to $O$ at position $\alpha$, with the first resp. second branch being taken, and $\alpha;\textsf{if}_i;\beta$ occurs in $N$ (where $i = 2$ resp. $3$), $(N,\alpha;\textsf{if}_i;\beta) \sim_p (O,\alpha;\beta)$
    
    \item If $N$, $O_1$ and $O_2$ match any of the following cases:
    \begin{enumerate}
        \item $N$ contains redexes at disjoint positions $\alpha_1$ and $\alpha_2$, $O_1$ is $N$ reduced first at $\alpha_1$ then $\alpha_2$ and $O_2$ is $N$ reduced first at $\alpha_2$ then at $\alpha_1$.
        
        \item $N \mid \alpha = \tif{\underline r < 0}{N_1}{N_2}$, where $r < 0$ (or, respectively, $r \geq 0$), $(N_2 \text{ resp. } N_1) \mid \beta$ is a redex, and $O_1$ is $N$ reduced at $\alpha$ and $O_2$ is $N$ reduced first at $\alpha;(\textsf{if}_3 \text{ resp. } \textsf{if}_2);\beta$ then at $\alpha$
        
        \item $N \mid \alpha = \tif{\underline r < 0}{N_1}{N_2}$, where $r < 0$ (or, respectively, $r \geq 0$), $(N_1 \text{ resp. } N_2) \mid \beta$ is a redex, and $O_1$ is $N$ reduced first at $\alpha$ then at $\alpha;\beta$ and $O_2$ is $N$ reduced first at $\alpha;(\textsf{if}_2 \text{ resp. } \textsf{if}_3);\beta$ then at $\alpha$
        
        \item $N \mid \alpha = (\lambda x. A) B$, there is a redex in $A$ at position $\beta$, $O_1$ is $N$ reduced first at $\alpha$ then at $\alpha;\beta$, and $O_2$ is $N$ reduced first at $\alpha;@_1;\lambda;\beta$ then at $\alpha$
        
        \item $N \mid \alpha = (\lambda x. A) B$, $B \mid \beta$ is a redex, $(\gamma_i)_i$ is a list of all the positions in $A$ where $A \mid \gamma = x$, ordered from left to right, $O_1$ is $N$ reduced first at $\alpha ; @_2 ; \beta$ then at $\alpha$, and $O_2$ is $N$ reduced first at $\alpha$ then at $\alpha;\gamma_i;\beta$ for each $i$ in order.
        
        \item $N \mid \alpha = \tY (\lambda x. A)$, $A$ reduced at $\beta$ is $A'$, $(\gamma_i)_i$ is a list of all the positions where $A' \mid \gamma  = x$, ordered from left to right, $O_1$ is $N$ reduced first at $\alpha;\tY;\lambda;\beta$ then at $\alpha$, and $O_2$ is $N$ reduced first at $\alpha$ then at $\alpha;\lambda;@_1;\gamma_i;Y;\lambda;\beta$ for each $i$ in order where $\gamma_i$ is left of $\beta$ then at $\alpha;\lambda;@_1;\beta$ then at $\alpha;\lambda;@_1;\gamma_i;Y;\lambda;\beta$ for the remaining values of $i$.
    \end{enumerate}
    (in which case $O_1$ and $O_2$ are equal as skeletons, but with different reduction sequences), $O_1'$ and $O_2'$ are the results of applying some reduction sequence to each of $O_1$ and $O_2$ (the same reductions in each case, which is always possible because they're equal terms), and $\delta$ is a position in $O_1'$ (or equivalently $O_2'$), then $(O_1',\delta) \sim_c (O_2',\delta)$.
\end{enumerate}
The $\sim_c$-rules are illustrated in \Cref{sim-diagram}.
\end{definition}

\begin{example}
In \Cref{ex:cousin sim}, $(M,\underline{+}_1) \sim_p (M \to N_2,\underline{+}_1)$ by case i of $\sim_p$ (because the reduction $M \to N_2$ occurs at $\underline{+}_2$, which is disjoint from $\underline{+}_1$), and similarly, $(M,\underline{+}_2) \sim_p (M \to N_1,\underline{+}_2)$.

If we extend it to have three $\tsample$s, $\sim_c$ becomes necessary as well: Let $M_{sss} = \tsample + \tsample + \tsample$ (taking the three-way addition to be a single primitive function), $M_{Xss} = \skeletonPlaceholder + \tsample + \tsample$, and so on. There are then reduction sequences $M_{sss} \to M_{Xss} \to M_{XXs} \to M_{XXX} \to \skeletonPlaceholder$ and $M_{sss} \to M_{sXs} \to M_{XXs} \to M_{XXX} \to \skeletonPlaceholder$. For the first two reductions, these reduction sequences take the same samples by $\sim_p$, case i, as in \Cref{ex:cousin sim}~. The next reduction uses the samples labelled by $(M_{sss} \to M_{Xss} \to M_{XXs}, \underline{+}_3)$ and $(M_{sss} \to M_{sXs} \to M_{XXs}, \underline{+}_3)$, which are related by $\sim_c$, case a, therefore when these reduction sequences reach $M_{XXX}$, they still contain all the same numbers, as desired.
\end{example}

The reflexive transitive closure $\sim^*$ of this relation is used to define the set of \emph{potential positions} $L(M) = L_0(M) / \sim^*$, and each equivalence class can be considered as the same position as it may occur across multiple reachable skeletons. 
If $(N,\alpha) \sim^* (O,\beta)$, then $N \mid \alpha$ and $O \mid \beta$ both have the same shape (i.e.~they're either both the placeholder $\skeletonPlaceholder$, both variables, both applications, both $\tsample$s etc.), therefore it's well-defined to talk of the set of potential positions where there is a $\tsample$, $L_s(M)$. 
Formally $L_s(M) := \{[(X, \alpha)]_{\sim^\ast_M} : X \mid \alpha = \tsample\}$.
The new sample space is then defined as $I^{L_s(M)}$, with the Borel $\sigma$-algebra and product measure.
Since $I^{L_s(M)}$ is a countable product, the measure space is well-defined \cite[Cor.~2.7.3]{AshDD00}.

\medskip
Before defining the new version of the reduction relation $\red$, the following lemma is necessary for it to be well-defined.

\begin{restatable}{lemma}{lemmaSimMN}
\label{lem:sim-M-N}
The relation $\sim$ is defined on $L_0(M)$ with reference to a particular starting term $M$, so different versions, $\sim_M$ and $\sim_N$, can be defined starting at different terms. If $M \to N$, then $\sim^*_N$ is equal to the restriction of $\sim^*_M$ to $L_0(N)$.
\end{restatable} 


At each reduction step $M \to N$, the sample space must be restricted from $I^{L_s(M)}$ to $I^{L_s(N)}$. 
The injection $L_0(N) \to L_0(M)$ is trivial to define by appending $\mathit{Sk}(M) \to \mathit{Sk}(N)$ to each path, and using \Cref{lem:sim-M-N}, this induces a corresponding injection on the quotient, $L(N) \to L(M)$. 
The corresponding map $L_s(N) \to L_s(M)$ is then denoted $i(M \to N)$.

\begin{definition}
Unlike in the purely call-by-value case, the version of the reduction relation that takes into account samples is still a general relation rather than a function, so it is denoted ``$\Rightarrow$'' instead of ``$\red$'', and it relates $\biguplus_{M \in \Lambda_0} I^{L_s(M)}$ to itself.
We write an element of $\biguplus_{M \in \Lambda_0} I^{L_s(M)}$ as $(M', s)$ where the \emph{term} $M' \in \Lambda^0$ and $s \in  I^{L_s(M')}$.
\begin{align*}
& (M,s) \Rightarrow (N,s \circ i(M \to N)) \text{ if $M \to N$ at $\alpha$ and either} \\
& \qquad \text{the redex is not $\tsample$, or} \\
& \qquad \text{$M \mid \alpha = \tsample$ and $N = M[\underline{s(\mathit{Sk}(M),\alpha)}/\alpha]$}
\end{align*}
\end{definition}

\medskip
This reduction relation now has all of the properties required of it. In particular, it can be considered an extension of the standard trace semantics (as will be seen later in \Cref{thm:AstEquivalence}), and also:
\begin{restatable}{lemma}{churchRosser} \label{churchRosser}
The relation $\Rightarrow$ is Church-Rosser.
\end{restatable}

\medskip
The reduction relation $\Rightarrow$ is nondeterministic, so it admits multiple possible reduction strategies. 
A \emph{reduction strategy} starting from a closed term $M$ is a partial function $f$ from $\mathit{Rch}(M)$ to positions, such that for any reachable term $N$ where $f$ is defined, $f(N)$ is a position of a redex in $N$, and if $f(N)$ is not defined, $N$ is a value.
Using a reduction strategy $f$, a subset of $\Rightarrow$ that isn't nondeterministic, $\Rightarrow_f$, can be defined by $(N,s) \Rightarrow_f (N',s')$ just if $(N,s) \Rightarrow (N',s')$ and $N$ reduces to $N'$ with the redex at $f(N)$.

The usual call-by-value semantics can be implemented as one of these reduction strategies, given by (with $V$ a value and $T$ a term that isn't a value and $M$ a general term)
\begin{align*}
\cbv(T M) & = @_1 ; \cbv(T) \\
\cbv(V T) & = @_2 ; \cbv(T) \\
\cbv(\underline f(V_1, \dots, V_{k-1}, T, M_{k+1}, \dots, M_n)) & = \underline f_k ; \cbv(T) \\
\cbv(\tY T) & = \tY ; \cbv(T) \\
\cbv(\tif{T < 0}{M_1}{M_2}) & = \textsf{if}_1 ; \cbv(T) \\
\cbv(V) & \text{ is undefined} \\
\cbv(T) & = \cdot \text{ otherwise}
\end{align*}
(this last case covers redexes at the root position).

A closed term $M$ terminates with a given reduction strategy $f$ and samples $s$ if there is some natural number $n$ such that $(M,s) \Rightarrow_f^n (N,s')$ where $f$ gives no reduction at $N$. The term terminates almost surely with respect to $f$ if it terminates with $f$ for almost all $s$.

With these definitions, it is now possible to relate the confluent trace semantics back to the standard trace semantics.

\begin{restatable}{theorem}{AstEquivalence} \label{thm:AstEquivalence}
A closed term $M$ is AST with respect to $\cbv$ iff it is AST.
\end{restatable}

It is also true that the distributions of terms produced by the standard semantics and by the reduction strategy $\cbv$ are the same, for much the same reason, although that is not relevant to our main results.

\begin{restatable}{theorem}{CbvIsTerminatingest} \label{thm:CbvIsTerminatingest}
If $M$ terminates with some reduction strategy $f$ and trace $s$, it terminates with $\cbv$ and $s$.
\end{restatable}

It is not quite true, however, that all reduction strategies produce the same values (barring some reduction strategies terminating where others don't), because it is possible for a reduction strategy to overshoot the value that the standard semantics produces by, for example, performing reductions inside of a lambda. Even for the same value of the samples, the final values may be different. A more restricted result is true, however, that if a term terminates with some reduction strategy $f$ and samples $s$, the value that $\Rightarrow_{\cbv}$ terminates with can reduce via $\Rightarrow$ to the value that $\Rightarrow_f$ terminates with (as a corollary to the proof of \Cref{thm:CbvIsTerminatingest}).

\begin{corollary}
If $M$ is AST with respect to any reduction strategy, it is AST.
\end{corollary}
\begin{proof}
Suppose $M$ is AST w.r.t.~$f$. Let the set of samples with which it terminates with this reduction strategy be $X$. By \Cref{thm:CbvIsTerminatingest}, $M$ also terminates with $\cbv$ and every element of $X$, and $X$ has measure 1, by assumption, therefore $M$ is AST with respect to $\cbv$ therefore by \Cref{thm:AstEquivalence} it is AST.
\end{proof}

All of the theorems on the termination of rankable terms therefore extend to other reduction strategies too. The proofs of \Cref{thm:rankable implies termination,thm:partial implies rankable,thm:antitone rankable implies termination,thm:antitone partial implies rankable} are all sufficiently generic with respect to what the reduction relation actually is that they can be directly applied to other reduction strategies. They only require that the number of reductions that can occur without any of them being a $\tY$-reduction is bounded for any starting term (which is true, because \Cref{thm:de groote} applies equally to any reduction strategy). For a reduction strategy $r$ and term $M$, just substitute $N[r(N)]$ being a $\tY$-redex for $N$ being of the form $E[\tY \lambda x. O]$, and $r(N)$ being undefined for $N$ being a value.

The domain of definition of the ranking functions also needs to be changed from the reachable terms $\mathit{Rch}(M)$ to the \emph{reachable terms with respect to the reduction strategy $r$},
\[
\begin{array}{l}
\mathit{Rch}_r(M) := \{N \mid \exists n, (N_i)_{0 \leq i \leq n} : {}\\
\qquad\qquad\qquad N_0 = M, N_n = N, N_i \to N_{i+1} \text{ at } r(N_i)\}.
\end{array}
\]

More explicitly, the modified forms of the theorems are:
\begin{definition}
An \emph{antitone ranking function on $M$ with respect to a reduction strategy $r$} is a measurable function $f:\mathit{Rch}_r(M) \to \mathbb{R}$ such that $f(N) \geq 0$ for all $N$, and there exists an antitone function $\epsilon : \nnReal \to \pReal$ such that
\begin{itemize}
    \item $f(N) \geq \epsilon(f(N)) + f(N')$ if $N[r(N)]$ is a $\tY$-redex and $N$ reduces to $N'$ at $r(N)$
    \item $f(N) \geq \int_I f(N[\underline{x}/r(N)]) \, \Leb(\mathrm{d}x)$ if $N[r(N)] = \tsample$

    \item $f(N) \geq f(N')$ if $r(N)$ is any other redex, where $N \to N'$ at $r(N)$.
\end{itemize}
Any closed term for which an antitone ranking function with respect to a reduction strategy $r$ exists is called \emph{antitone rankable} with respect to $r$. 
\end{definition}

\begin{definition}
An \emph{antitone sparse ranking function on $M$ with respect to a reduction strategy $r$} is a partial function $f : \mathit{Rch}_r(M) \rightharpoonup \mathbb R$ such that there exists some antitone function $\epsilon : \nnReal \to \pReal$ such that
\begin{inparaenum}[(i)]
    \item $f(N) \geq 0$ for all $N \in \dom(f)$,
    \item $M \in \dom(f)$,
    \item for any $N \in \dom(f)$, evaluation of $N$ at the positions specified by $r$ will eventually reach some $O$ such that either $r(O)$ isn't defined or $f(O)$ is, and $f(N) \geq \mathbb E[f(O) + \epsilon(f(O)) \times \text{the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of $\dom(f)$).
\end{inparaenum}
\iffalse
\begin{itemize}
    \item $f(N) \geq 0$ for all $N$ where $f$ is defined.
    \item $f$ is defined at $M$.
    \item For any $N$ in the domain of definition of $f$, evaluation of $N$ at the positions specified by $r$ will eventually reach some $O$ such that either $r(O)$ isn't defined or $f(O)$ is, and $f(N) \geq \mathbb E[f(O) + \epsilon(f(O)) \times \text{the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}
\fi
\end{definition}

If the antitone function is just the constant function $1$ instead, then it is called a \emph{(sparse) ranking function on $M$ with respect to $r$} instead, and the term is called \emph{rankable} with respect to $r$.

\begin{theorem} \label{thm:confluent ranking}
If a closed PPCF term $M$ has a ranking function, sparse ranking function, antitone ranking function or antitone sparse ranking function with respect to a reduction strategy $r$, then $M$ is AST with respect to $r$, and AST.
\end{theorem}

When constructing an (antitone) sparse ranking function with respect to an alternative reduction strategy, it is intuitively simplest to define the sparse ranking function and the reduction strategy together. For each key reachable term where the sparse ranking function is defined, simply decide which sub-term should be reduced next, and how far before the next key reachable term. The only restrictions to bear in mind on evaluating sub-terms are that a $\tsample$ may not be evaluated inside a $\lambda$ or $\tY$ on the right of an application, and a $\beta$ redex may not be reduced until its argument is a value. This can be seen as applied to \Cref{ex:sum of powers} at the end of \Cref{apx:confluent}.

\medskip
If, as seems likely, \Cref{thm:antitone rankable implies termination} is complete for terms from which every reachable term is AST, then by the following theorem, \Cref{thm:confluent ranking} can't actually prove the termination of any terms not already provable by \Cref{thm:antitone rankable implies termination} and \Cref{thm:antitone partial implies rankable}.

\begin{restatable}{theorem}{confluentNoStronger}
For any closed term $M$ and reduction strategy $r$ on $M$, if every term in $\mathit{Rch}_r(M)$ is AST, then every term in $\mathit{Rch}(M)$ is AST.
\end{restatable}

However, there are some terms which terminate more simply and directly via some alternative reduction strategy. For these examples, a custom reduction strategy is specified for each term under consideration.

\begin{example} \label{ex:sum of powers}
The following term picks a random natural number $n$, then computes $\sum_{k=0}^n k^n$.
\begin{align*}
M =&\ (\lambda n. P[n]) \lfloor \tsample^{-1/2} \rfloor \\
P[n] =&\ (\lambda p. \Xi[p] \, n) (\lambda x. \Theta[x] \, n)) \\
\Xi[p] =&\ \tY \lambda f n. \tif{n = 0}{\underline 0}{p \, n + f (n-1)} \\
\Theta[x] =&\ \tY \lambda f n. \tif{n = 0}{\underline 1}{x \times f (n-1)}.
\end{align*}

With the CBV reduction strategy, the recursion in $\Theta[x]$ (that defines the function $k \mapsto k^x$) is executed separately for every term in the sum in $\Xi$. Not only is this much more complicated than simplifying $\Theta[x] \, \underline n$ first, it requires considerably more $\tY$-reduction steps, so many so that $M$ is $\tY$-PAST with respect to a reduction strategy that simplifies $\Theta[x] \, \underline n$ first (call it ``$r$''), but isn't $\tY$-PAST with respect to the standard reduction strategy, so that $M$ is rankable with respect to $r$, but isn't rankable. It is still antitone rankable, but this is much more complicated. Details of the ranking function construction with and without using the confluent semantics are given in \Cref{apx:confluent}.
\end{example}

Even some of the examples given earlier implicitly use slightly non-standard reduction strategies to make the definitions of their ranking functions slightly simpler. For example, the antitone sparse ranking function in \Cref{ex:unbiased random walk} was defined at $\Theta_2\ \underline n$ (where $\Theta_2 = \tY \, \lambda f n . \, \tif{n = 0}{0}{f \, (n - 1) \oplus_{1/2} f \, (n + 1)}$), but $\Theta_2\ \underline{10}$ actually reduces to $\Theta_2\ (\underline{10} - 1)$ or $\Theta_2\ (\underline{10} + 1)$, and (taking the second of these), it then reduces to $(\lambda z. (\lambda n. \, \tif{n = 0}{0}{\Theta_2 \, (n - 1) \oplus_{1/2} \Theta_2 \, (n + 1)}) z) \, (\underline{10} + 1)$, then $(\lambda z. (\lambda n. \, \tif{n = 0}{0}{\Theta_2 \, (n - 1) \oplus_{1/2} \Theta_2 \, (n + 1)}) z) \, \underline{11}$, bypassing $\Theta_2\ \underline{11}$ entirely. The antitone sparse ranking function in the form given can be justified anyway, by considering it to be using a reduction strategy which is the same as $\cbv$ except that in $\Theta_2\ (\underline f \underline n)$, it evaluates the redex at $@_2$ first.

\Cref{ex:higher-order recursion} also makes use of an alternative reduction strategy. In this case, it is to allow the antitone sparse ranking function to skip over all of those terms where the unknown function $F$ is part way through evaluation.
