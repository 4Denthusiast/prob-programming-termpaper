% !TEX root = main.tex
\section{Confluent trace semantics}
\label{sec:confluent}

When proving almost sure termination in this way, it is necessary to consider which terms a given term may reduce to. Sometimes however, the reduction that the programmer has in mind may not be strictly the call-by-value order defined so far; or considering an alternative reduction order may be simpler or more intuitive.

Non-probabilistic lambda calculi generally have the Church-Rosser property, that if a term $A$ reduces to both $B_1$ and $B_2$, there is some $C$ 
%with reduction sequences $B_1 \to^* C$ and $B_2 \to^* C$, 
to which both $B_1$ and $B_2$ reduce,
so the reduction order mostly doesn't matter. 
In the probabilistic case, this may not be true, because $\beta$-reduction can duplicate $\tsample$s, so the outputs of the copies of the sample may be identical or independent, depending on whether the sample is taken before or after $\beta$-reduction. 
There are, however, some restricted variations on the reduction order that do not have this problem.

\paragraph{}
Even with this restriction, a sampling semantics in the style of the one already defined would not be entirely Church-Rosser, as, for example, $\red^3(\tsample - \tsample, (1,0,\dots))$ would be either $1$ or $-1$ depending on the order of evaluation of the $\tsample$s, as that determines which sample from the pre-selected sequence is used for each one. To fix this, rather than pre-selecting samples according to the order they'll be drawn in, select them according to the position in the term where they'll be used instead.

A \emph{position} is a finite sequence of steps into a term, defined inductively as
\begin{align*}
\alpha ::= & \; {\cdot} \mid \lambda ; \alpha \mid @_1 ; \alpha \mid @_2 ; \alpha \mid \underline f_i ; \alpha \\
& {} \mid \tY ; \alpha \mid \textsf{if}_1 ; \alpha \mid \textsf{if}_2 ; \alpha \mid \textsf{if}_3 ; \alpha \mid \tscore ; \alpha.
\end{align*}
The \emph{subterm of $M$ at a position $\alpha$}, denoted $M \mid \alpha$, is defined as
\begin{align*}
M \mid \cdot & = M \\
\lambda x. M \mid \lambda ; \alpha & = M \mid \alpha \\
M_1 M_2 \mid @_i ; \alpha & = M_i \mid \alpha \quad \text{for } i = 1,2 \\
\underline f(M_1,\dots,M_n) \mid \underline f_i ; \alpha & = M_i \mid \alpha \quad \text{for }i \leq n \\
\tY M \mid \tY ; \alpha & = M \mid \alpha \\
\tif{M_1 < 0}{M_2}{M_3} \mid \textsf{if}_i ; \alpha & = M_i \mid \alpha \quad \text{for } i = 1,2,3 \\
\tscore(M) \mid \tscore ; \alpha & = M \mid \alpha
\end{align*}
so that every subterm is located at a unique position, but not every position corresponds to a subterm (e.g. $x \, y \mid \lambda$ is undefined). 
A position such that $M\mid \alpha$ does exist is said to \emph{occur} in $M$. 
%\changed[lo]{
\emph{Substitution} of $N$ at position $\alpha$ in $M$, written $M[N/\alpha]$, is defined similarly.
For example, let 
\[
M = \lambda x \, y. y \, (\tif{x < 0}{y \, (\underline f (x))}{\underline 3})
\qquad
\alpha =\lambda ; \lambda ; @_2 ; \textsf{if}_2 ; @_2
\]
then 
\(
M[\tsample / \alpha] = \lambda x \, y. y \, (\tif{x < 0}{y \, \tsample}{\underline 3}).
\)
%}

Two subterms $N_1$ and $N_2$ of a term $M$, corresponding to positions $\alpha_1$ and $\alpha_2$, can overlap in a few different ways. 
If $\alpha_1$ is an initial segment (i.e.~prefix) of $\alpha_2$ (written as $\alpha_1 \leq \alpha_2$), then $N_2$ is also a subterm of $N_1$. If neither $\alpha_1 \leq \alpha_2$ nor $\alpha_1 \geq \alpha_2$, the positions are said to be \emph{disjoint}. 
The notion of disjointness is mostly relevant in that if $\alpha_1$ and $\alpha_2$ are disjoint, performing a substitution at $\alpha_1$ will leave the subterm at $\alpha_2$ unaffected.
\lo{The preceding sentence is unclear. Typo?} \akr{It wasn't a typo, but I've made it a little more explicit now. Is this clearer?} \lo{Yes, it's clear now.}

With this notation, a more general reduction relation $\to$ can be defined. \akr{It would almost work to have one of the neater versions of $\tY$-reduction here.}
%\lo{On first reading, I struggled to parse the following clauses.}
\begin{definition}
\label{def:more general red}
The binary relation $\to$ is defined by the following rules, each is conditional on a redex occurring at position $\alpha$ in the term $M$:
\begin{align*}
  \text{if } M \mid \alpha = (\lambda x.N) V,\ & M \to M[N[V/x]/\alpha] \\
  \text{if } M \mid \alpha = \underline f (\underline r_1, \dots , \underline r_n),\ & M \to M[\underline{f(r_1,\dots,r_n)}/\alpha] \\
  \text{if } M \mid \alpha = \tY \lambda x. N,\ & M \to M[\lambda z. N[(\tY \lambda x. N)/x] z/\alpha]\\ \text{where }z&\text{ is not free in $N$}\\
  \text{if } M \mid \alpha = \tif{\underline r < 0}{N_1}{N_2},\ & M \to M[N_1/\alpha] \text{ where }r < 0 \\
  \text{if } M \mid \alpha = \tif{\underline r < 0}{N_1}{N_2},\ & M \to M[N_2/\alpha] \text{ where }r \geq 0 \\
  \text{if } M \mid \alpha = \tsample \text{ and $\lambda$ does}&\text{ not occur after $@_2$ or $\tY$ in $\alpha$},\\ & M \to M[\underline r/\alpha] \text{ where } r \in [0,1] \\
  \text{if } M \mid \alpha = \tscore(\underline r),\ & M \to M[\underline r/\alpha].
\end{align*}
In each of these cases, $M \mid \alpha$ is the \emph{redex}, and the reduction takes place at $\alpha$.
\end{definition}

\paragraph{}
Labelling the pre-chosen samples by the positions in the term would also not work because in some cases, a $\tsample$ will be duplicated before being reduced, for example, in $(\lambda x. x \, {\underline 0} \, \mathbin{\underline{+}} \, {x \, \underline 0}) (\lambda y. \tsample)$, both of the $\tsample$ redexes that eventually occur originate at $@_2 ; \lambda$. 
It is therefore necessary to consider possible positions that may occur in other terms reachable from the original term. 
Even this is itself inadequate because some of the positions in different reachable terms need to be considered the same, and the number of reachable terms is in general uncountable, which leads to measure-theoretic issues.

\changed[lo]{We are thus lead to consider the reduction relation on skeletons (and positions in a skeleton), which can be extended from the definitions on terms in the obvious way,} 
%The positions in a skeleton and the reduction relation on skeletons can be extended from the definitions on terms in the obvious way, 
with $\tif{\skeletonPlaceholder < 0}{A}{B}$ reducing nondeterministically to both $A$ and $B$, and $\tsample$ reducing to $\skeletonPlaceholder$.
For example, we have 
\(
(\lambda x . \tif{x < 0}{x}{\skeletonPlaceholder}) \, \tsample
\to
(\lambda x . \tif{x < 0}{x}{\skeletonPlaceholder}) \, \skeletonPlaceholder
\to
\tif{\skeletonPlaceholder < 0}{\skeletonPlaceholder}{\skeletonPlaceholder}
\to
\skeletonPlaceholder
\).

Given a closed term $M$, let $L_0(M)$ be the set of pairs, the first element of which is a $\to$-reduction sequence of skeletons starting at $\mathit{Sk}(M)$, and the second of which is a position in the final skeleton of the reduction sequence. As with the traces from $I^{\mathbb N}$ used to pre-select samples to use in the usual linear sampling semantics,\lo{I assume by linear sampling semantics, you mean the standard sampling semantics. If so, I would drop linear.} there will be (with one more caveat introduced later) a sample from $I$ pre-selected for each element of $L_0(M)$, which will then be used if a $\tsample$ reduction is ever performed at that position.

\begin{example}[Labelling samples by terms]
Consider the terms
\begin{align*}
A[x] &= \tif{\tif{x>0}{I}{I} \, \tsample - 0.5 > 0}{0}{\Omega}\\
B &= \tif{\tsample - 0.5 > 0}{0}{\Omega}
\end{align*}
If terms rather than skeletons were used to label samples, the set of traces where $A[\tsample]$ terminates would be
\[\begin{array}{l}
\bigcup_{r \in [0,1]} \{s \in I^{L_s(A[\tsample])} \mid \\
\qquad\qquad\qquad\quad s([A[\tsample]], \mathsf{if}_1;\underline{-}_1;@_1;\mathsf{if}_1) = r, \\
\qquad\qquad\qquad\quad s([A[\tsample], A[\underline r], \ldots, B, \mathsf{if}_1) > 0.5 \}.
\end{array}\] 
\lo{We have just defined $L_0(M)$, but what is $L_s(\cdot)$ (subscript $s$)? I assume it is the version of $L_0(M)$ that labels samples by term.} 

\lo{The occurrence of $L_s(M)$ in the set comprehension below should be $L_0(M)$?}

\lo{The $\mathsf{if}_1$ in the line above (and in the set comprehension below) should be $\mathsf{if}_1 ; \underline{-}_1$.}

\lo{I think we should write ``$[A[\tsample], A[\underline r], \ldots, B$'' in the line above as ``$A[\tsample] \to A[\underline r] \to^\ast B$''; similarly $\mathit{Sk}(A[\tsample]) \to \mathit{Sk}(A[\underline 0]) \to^\ast \mathit{Sk}(B)$ below.}
This is a rather unwieldy expression, but the crucial part is that $r$ occurs twice in the conditions on $s$: once as the value a sample must take, and once in the location of a sample. 
This set is unmeasurable, therefore the termination probability would not even be well-defined. 
Labelling samples by skeletons instead, this problem does not occur because there are only countably many skeleton, and at each step in a reduction sequence, only finitely many could have occurred yet. 
For this particular term, $\mathit{Sk}(A[\underline r])$ does not depend on the value of $r$, therefore the set where it terminates becomes simply
\begin{align*}
&\{s \in I^{L_s(A[\tsample])} \mid \\
&\qquad s(\mathit{Sk}(A[\tsample]), \mathit{Sk}(A[\underline 0]), \ldots, \mathit{Sk}(B), \mathsf{if}_1) > 0.5 \},
\end{align*}
which is measurable. Skeletal reduction sequences still contain just as much information as reduction sequences of terms, however, on which reductions took place.
\qed
\end{example}

Reduction sequences are used rather than reachable skeletons because if the same skeleton is reached twice, different samples may be needed. 

\begin{example} Consider the term $M = \tY (\lambda f x.\ \tif{\tsample - \underline{0.5} < 0}{f \, x}{x}) \, \underline 0$, which reduces after a few steps to $N = \tif{\tsample - \underline{0.5}  < 0}{M}{\underline 0}$. If the pre-selected sample for $(N,\textsf{if}_1;\underline{-}_1)$ is less than 0.5, $N$ reduces back to $M$, then $N$ again, then the same sample is used the next time, therefore it's an infinite loop, whereas if samples are labelled by reduction sequences, the samples for $M \to \dots \to N$ are independent from the samples for $M \to \dots \to N \to M \to \dots \to N$, and so on.
\end{example}

The reduction sequences of skeletons will often be discussed as though they were just skeletons, identifying them with their final skeletons. With this abuse of notation, a reduction sequence $N$ (actually $N_1 \to \dots \to N_n = N$) may be said to reduce to a reduction sequence $O$, where the reduction sequence implicitly associated with the final skeleton $O$ is $N_1 \to \dots \to N_n \to O$.

\paragraph{}
This is still not quite sufficient because sometimes the same samples must be used at corresponding positions in different reduction sequences. For example, the term $M = \tsample + \tsample$ has the reachable skeletons $N_1 = \skeletonPlaceholder + \tsample, N_2 = \tsample + \skeletonPlaceholder, O = \skeletonPlaceholder + \skeletonPlaceholder$ and $\skeletonPlaceholder$, with reductions $M \to N_1 \to O \to \skeletonPlaceholder$ and $M \to N_2 \to O \to \skeletonPlaceholder$. In the reduction $M \to N_1$, the sample labelled $(M, \underline{+}_2)$ is used, and in the reduction $N_2 \to O$, the sample labelled $(M \to N_2, \underline{+}_2)$ is used. Each of these samples becomes the value of the second numeral in $O$ in their respective reduction sequences, therefore in order for Church-Rosserness to be attained, they must be the same. Which elements of $L_0(M)$ must match can be described by the relation $\sim^*$:

\begin{definition}
The relation $\sim$ is defined as the union of the minimal symmetric relations $\sim_p$ (``$p$'' for parent-child) and $\sim_c$ (``$c$'' for cousin) satisfying
\begin{enumerate}
    \item If $N$ reduces to $O$ with the redex at position $\alpha$, and $\beta$ is a position in $N$ disjoint from $\alpha$, then $(N,\beta) \sim_p (O,\beta)$.
    
    \item If $N$ $\beta$-reduces to $O$ at position $\alpha$, $\beta$ is a position in $N \mid \alpha;@_1;\lambda$ and $N \mid \alpha;@_1;\lambda;\beta$ is not the variable involved in the reduction, $(N,\alpha;@_1;\lambda;\beta) \sim_p (O, \alpha;\beta)$
    
    \item If $N$ $\textsf{if}$-reduces to $O$ at position $\alpha$, with the first resp. second branch being taken, and $\alpha;\textsf{if}_i;\beta$ occurs in $N$ (where $i = 2$ resp. $3$), $(N,\alpha;\textsf{if}_i;\beta) \sim_p (O,\alpha;\beta)$
    
    \item If $N$, $O_1$ and $O_2$ match any of the following cases:
    \begin{enumerate}
        \item $N$ contains redexes at disjoint positions $\alpha_1$ and $\alpha_2$, $O_1$ is $N$ reduced first at $\alpha_1$ then $\alpha_2$ and $O_2$ is $N$ reduced first at $\alpha_2$ then at $\alpha_1$.
        
        \item $N \mid \alpha = \tif{\underline r < 0}{N_1}{N_2}$, where $r < 0$ (or, respectively, $r \geq 0$), $(N_2 \text{ resp. } N_1) \mid \beta$ is a redex, and $O_1$ is $N$ reduced at $\alpha$ and $O_2$ is $N$ reduced first at $\alpha;(\textsf{if}_3 \text{ resp. } \textsf{if}_2);\beta$ then at $\alpha$
        
        \item $N \mid \alpha = \tif{\underline r < 0}{N_1}{N_2}$, where $r < 0$ (or, respectively, $r \geq 0$), $(N_1 \text{ resp. } N_2) \mid \beta$ is a redex, and $O_1$ is $N$ reduced first at $\alpha$ then at $\alpha;\beta$ and $O_2$ is $N$ reduced first at $\alpha;(\textsf{if}_2 \text{ resp. } \textsf{if}_3);\beta$ then at $\alpha$
        
        \item $N \mid \alpha = (\lambda x. A) B$, there is a redex in $A$ at position $\beta$, $O_1$ is $N$ reduced first at $\alpha$ then at $\alpha;\beta$, and $O_2$ is $N$ reduced first at $\alpha;@_1;\lambda;\beta$ then at $\alpha$
        
        \item $N \mid \alpha = (\lambda x. A) B$, $B \mid \beta$ is a redex, $(\gamma_i)_i$ is a list of all the positions in $A$ where $A \mid \gamma = x$, ordered from left to right, $O_1$ is $N$ reduced first at $\alpha ; @_2 ; \beta$ then at $\alpha$, and $O_2$ is $N$ reduced first at $\alpha$ then at $\alpha;\gamma_i;\beta$ for each $i$ in order.
        
        \item $N \mid \alpha = \tY (\lambda x. A)$, $A$ reduced at $\beta$ is $A'$, $(\gamma_i)_i$ is a list of all the positions where $A' \mid \gamma  = x$, ordered from left to right, $O_1$ is $N$ reduced first at $\alpha;\tY;\lambda;\beta$ then at $\alpha$, and $O_2$ is $N$ reduced first at $\alpha$ then at $\alpha;\lambda;@_1;\gamma_i;Y;\lambda;\beta$ for each $i$ in order where $\gamma_i$ is left of $\beta$ then at $\alpha;\lambda;@_1;\beta$ then at $\alpha;\lambda;@_1;\gamma_i;Y;\lambda;\beta$ for the remaining values of $i$.
    \end{enumerate}
    (in which case $O_1$ and $O_2$ are equal as skeletons, but with different reduction sequences), $O_1'$ and $O_2'$ are the results of applying some reduction sequence to each of $O_1$ and $O_2$ (the same reductions in each case, which is always possible because they're equal terms), and $\delta$ is a position in $O_1'$ (or equivalently $O_2'$), then $(O_1',\delta) \sim_c (O_2',\delta)$.
\end{enumerate}
The $\sim_c$-rules are illustrated in Figure \ref{sim-diagram}.
\end{definition}

The reflexive transitive closure $\sim^*$ of this relation is used to define the set of \emph{potential positions} $L(M) = L_0(M) / \sim^*$, and each equivalence class can be considered as the same position as it may occur across multiple reachable skeletons. 
If $(N,\alpha) \sim^* (O,\beta)$, then $N \mid \alpha$ and $O \mid \beta$ both have the same shape (i.e.~they're either both the placeholder $\skeletonPlaceholder$, both variables, both applications, both $\tsample$s etc.), therefore it's well-defined to talk of the set of potential positions where there is a $\tsample$, $L_s(M)$. 
Formally $L_s(M) := \{[(X, \alpha)]_{\sim^\ast_M} : X \mid \alpha = \tsample\}$.
The new sample space is then defined as $I^{L_s(M)}$, with the Borel $\sigma$-algebra and product measure.
\changed[lo]{Since $I^{L_s(M)}$ is a countable product, the measure space is well-defined \citep[Cor.~2.7.3]{AshDD00}.}

\paragraph{}
Before defining the new version of the reduction relation $\red$, the following lemma is necessary for it to be well-defined.

\begin{restatable}{lemma}{lemmaSimMN}
\label{lem:sim-M-N}
The relation $\sim$ is defined on $L_0(M)$ with reference to a particular starting term $M$, so different versions, $\sim_M$ and $\sim_N$, can be defined starting at different terms. If $M \to N$, then $\sim^*_N$ is equal to the restriction of $\sim^*_M$ to $L_0(N)$.
\end{restatable} 


At each reduction step $M \to N$, the sample space must be restricted from $I^{L_s(M)}$ to $I^{L_s(N)}$. 
The injection $L_0(N) \to L_0(M)$ is trivial to define by appending $\mathit{Sk}(M) \to \mathit{Sk}(N)$ to each path, and using Lemma~\ref{lem:sim-M-N}, this induces a corresponding injection on the quotient, $L(N) \to L(M)$. 
The corresponding map $L_s(N) \to L_s(M)$ is then denoted $i(M \to N)$.

Unlike in the purely call-by-value case, the version of the reduction relation that takes into account samples is still a general relation rather than a function, so it is denoted ``$\Rightarrow$'' instead of ``$\red$'', and it relates $\biguplus_{M \in \Lambda_0} I^{L_s(M)}$ to itself.
\changed[lo]{We write an element of $\biguplus_{M \in \Lambda_0} I^{L_s(M)}$ as $(M', s)$ where the \emph{term} $M' \in \Lambda^0$ and $s \in  I^{L_s(M')}$.}
\begin{align*}
& (M,s) \Rightarrow (N,s \circ i(M \to N)) \text{ if $M \to N$ at $\alpha$ and either} \\
& \qquad \text{the redex is not $\tsample$, or} \\
& \qquad \text{$M \mid \alpha = \tsample$ and $N = M[\underline{s(\mathit{Sk}(M),\alpha)}/\alpha]$}
\end{align*}

\paragraph{}
This reduction relation now has all of the properties required of it. In particular, it can be considered an extension of the usual linear trace semantics (as will be seen later in Theorem \ref{thm:AstEquivalence}), and also:
\begin{restatable}{lemma}{churchRosser} \label{churchRosser}
The relation $\Rightarrow$ is Church-Rosser.
\end{restatable}

\paragraph{}
The reduction relation $\Rightarrow$ is nondeterministic, so it admits multiple possible reduction strategies. 
A \emph{reduction strategy} starting from a closed term $M$ is a partial function $f$ from $Rch(M)$ to positions, such that for any reachable term $N$ where $f$ is defined, $f(N)$ is a position of a redex in $N$, and if $f(N)$ is not defined, $N$ is a value.
Using a reduction strategy $f$, a subset of $\Rightarrow$ that isn't nondeterministic, $\Rightarrow_f$, can be defined by $(N,s) \Rightarrow_f (N',s')$ just if $(N,s) \Rightarrow (N',s')$ and $N$ reduces to $N'$ with the redex at $f(N)$.

The usual call-by-value semantics can be implemented as one of these reduction strategies, given by (with $V$ a value and $T$ a term that isn't a value and $M$ a general term)
\begin{align*}
\cbv(T M) & = @_1 ; \cbv(T) \\
\cbv(V T) & = @_2 ; \cbv(T) \\
\cbv(\underline f(V_1, \dots, V_{k-1}, T, M_{k+1}, \dots, M_n)) & = \underline f_k ; \cbv(T) \\
\cbv(\tY T) & = \tY ; \cbv(T) \\
\cbv(\tif{T < 0}{M_1}{M_2}) & = \textsf{if}_1 ; \cbv(T) \\
\cbv(\tscore(T)) & = \tscore ; \cbv(T) \\
\cbv(V) & \text{ is undefined} \\
\cbv(T) & = \cdot \text{ otherwise}
\end{align*}
(this last case covers redexes at the root position).

A closed term $M$ terminates with a given reduction strategy $f$ and samples $s$ if there is some natural number $n$ such that $(M,s) \Rightarrow_f^n (N,s')$ where $f$ gives no reduction at $N$. The term terminates almost surely with respect to $f$ if it terminates with $f$ for almost all $s$.

With these definitions, it is now possible to relate the confluent sampling semantics back to the linear sampling semantics.

\begin{restatable}{theorem}{AstEquivalence} \label{thm:AstEquivalence}
A closed term $M$ is AST with respect to $\cbv$ iff it is AST.
\end{restatable}

It is also true that the distributions of terms produced by the linear semantics and by the reduction strategy $\cbv$ are the same, for much the same reason, although that is not relevant to our main results.

\begin{restatable}{theorem}{CbvIsTerminatingest} \label{thm:CbvIsTerminatingest}
If $M$ terminates with some reduction strategy $f$ and trace $s$, it terminates with $\cbv$ and $s$.
\end{restatable}

It is not quite true, however, that all reduction strategies produce the same values (barring some reduction strategies terminating where others don't), because it is possible for a reduction strategy to overshoot the value that the linear semantics produces by, for example, performing reductions inside of a lambda. Even for the same value of the samples, the final values may be different. A more restricted result is true, however, that if a term terminates with some reduction strategy $f$ and samples $s$, the value that $\Rightarrow_{\cbv}$ terminates with can reduce via $\Rightarrow$ to the value that $\Rightarrow_f$ terminates with (as a corollary to the proof of Theorem \ref{thm:CbvIsTerminatingest}).

\begin{corollary}
If $M$ is AST with respect to any reduction strategy, it is AST.
\end{corollary}
\begin{proof}
Suppose $M$ is AST with respect to $f$. Let the set of samples with which it terminates with this reduction strategy be $X$. By Theorem~\ref{thm:CbvIsTerminatingest}, $M$ also terminates with $\cbv$ and every element of $X$, and $X$ has measure 1, by assumption, therefore $M$ is AST with respect to $\cbv$ therefore by Theorem~\ref{thm:AstEquivalence} it is AST.
\end{proof}

All of the theorems on the termination of rankable terms therefore extend to other reduction strategies too. The proofs of Theorems \ref{thm:rankable implies termination}, \ref{thm:partial implies rankable}, \ref{thm:antitone rankable implies termination} and \ref{thm:antitone partial implies rankable} are all sufficiently generic with respect to what the reduction relation actually is that they can be directly applied to other reduction strategies. They only require that the number of reductions that can occur without any of them being a $\tY$-reduction is bounded for any starting term (which is true, because Theorem \ref{thm:de groote} applies equally to any reduction strategy). For a reduction strategy $r$ and term $M$, just substitute $N[r(N)]$ being a $\tY$-redex for $N$ being of the form $E[\tY \lambda x. O]$, and $r(N)$ being undefined for $N$ being a value.

The domain of definition of the ranking functions also needs to be changed from the reachable terms $Rch(M)$ to the \emph{reachable terms with respect to the reduction strategy $r$},
\begin{align*}
& Rch_r(M) = \\ & \{N \mid \exists n, (N_i)_{0 \leq i \leq n}: N_0 = M, N_n = N, N_i \to N_{i+1} \text{ at } r(N_i)\}.
\end{align*}

To be more explicit, the modified forms of the theorems are:
\begin{definition}\rm
A \emph{ranking function on $M$ with respect to a reduction strategy $r$} is a measurable function $f:\mathit{Rch}_r(M) \to \mathbb{R}$ such that $f(N) \geq 0$ for all $N$, and
\begin{itemize}
    \item $f(N) \geq 1+ f(N')$ if $N[r(N)]$ is a $\tY$-redex and $N$ reduces to $N'$ at $r(N)$
    \item $f(N) \geq \int_I f(N[\underline{x}/r(N)]) \, \Leb(\mathrm{d}x)$ if $N[r(N)] = \tsample$

    \item $f(N) \geq f(N')$ if $r(N)$ is any other redex, where $N \to N'$ at $r(N)$.
\end{itemize}
Any closed term for which a ranking function with respect to a reduction strategy $r$ exists is called \emph{rankable} with respect to $r$. 
\end{definition}

\begin{definition}
A \emph{sparse ranking function on $M$ with respect to a reduction strategy $r$} is a partial function $f : Rch_r(M) \rightharpoonup \mathbb R$ such that
\begin{itemize}
    \item $f(N) \geq 0$ for all $N$ where $f$ is defined.
    \item $f$ is defined at $M$.
    \item For any $N$ in the domain of definition of $f$, evaluation of $N$ at the positions specified by $r$ will eventually reach some $O$ such that either $r(O)$ isn't defined or $f(O)$ is, and $f(N) \geq \mathbb E[f(O) + \text{ the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}
\end{definition}

\begin{definition}\rm
An \emph{antitone ranking function on $M$ with respect to a reduction strategy $r$} is a measurable function $f:\mathit{Rch}_r(M) \to \mathbb{R}$ such that $f(N) \geq 0$ for all $N$, and there exists an antitone function $\epsilon : \nnReal \to \pReal$ such that
\begin{itemize}
    \item $f(N) \geq \epsilon(f(N)) + f(N')$ if $N[r(N)]$ is a $\tY$-redex and $N$ reduces to $N'$ at $r(N)$
    \item $f(N) \geq \int_I f(N[\underline{x}/r(N)]) \, \Leb(\mathrm{d}x)$ if $N[r(N)] = \tsample$

    \item $f(N) \geq f(N')$ if $r(N)$ is any other redex, where $N \to N'$ at $r(N)$.
\end{itemize}
Any closed term for which an antitone ranking function with respect to a reduction strategy $r$ exists is called \emph{antitone rankable} with respect to $r$. 
\end{definition}

\begin{definition}
An \emph{antitone sparse ranking function on $M$ with respect to a reduction strategy $r$} is a partial function $f : Rch_r(M) \rightharpoonup \mathbb R$ such that there exists some antitone function $\epsilon : \nnReal \to \pReal$ such that
\begin{itemize}
    \item $f(N) \geq 0$ for all $N$ where $f$ is defined.
    \item $f$ is defined at $M$.
    \item For any $N$ in the domain of definition of $f$, evaluation of $N$ at the positions specified by $r$ will eventually reach some $O$ such that either $r(O)$ isn't defined or $f(O)$ is, and $f(N) \geq \mathbb E[f(O) + \epsilon(f(O)) \times \text{the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}
\end{definition}

\begin{theorem}
If a closed SPCF term $M$ has a ranking function, sparse ranking function, antitone ranking function or antitone sparse ranking function with respect to a reduction strategy $r$, then $M$ is AST with respect to $r$, and AST.
\end{theorem}
\lo{Give an example or two of a reduction strategy (for $\Rightarrow$) that is not $\cbv$.}

