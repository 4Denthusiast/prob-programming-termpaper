% !TEX root = main.tex
\label{apx:confluent}
The rules defining $\sim_c$ can be more intuitively understood as diagrams. In the following diagrams, a circle represents a term (or skeleton), and the leaves on the trees are subterms at the positions indicated by the labels in the other nodes. The labels on the arrows between trees are the positions of the reductions, and dashed arrows represent some number of reductions in a row. If a reduction sequence includes one branch of one of these cases at some point in it, then the positions in the reduction sequence obtained by substituting in the other branch are all related by $\sim_c$ to the corresponding positions in the original reduction sequence.
\include{sim-diagram}
%The diagram is not actually in the figure environment because that doesn't work properly with it extending over several pages. The empty figure is just needed to get the label and caption.
\begin{figure}[htb]
\caption{Illustration of the $\sim_c$ rules. \label{sim-diagram}
}
\end{figure}

\paragraph{}
Although the six cases in the definition of $\sim_c$ are defined separately, there are some commonalities worth noting. Each case (except a, which is symmetrical) has a right branch and a left branch (the branches of case b are unfortunately displayed the wrong way around in the diagram). The right branch has reduction steps at two positions, $\beta$ then $\alpha$, where $\beta > \alpha$ (i.e.~$\beta$ is inside $\alpha$), and in the left branch, the order of these reductions are swapped, so the reductions are at $\alpha$ then $\beta_1, \dots, \beta_n$. The position $\alpha$ of one of the reductions is unchanged, and the other reduction may have multiple (in cases e and f) or 0 (in cases b and e) images in the left branch. The images $(\beta_i)$ are all still inside (greater than or equal to) $\alpha$, and although they are not generally equal to $\beta$, the subskeletons at these positions have a similar shape to the subskeleton (initially) at position $\beta$, so that the reductions at these positions are the same type of reduction (e.g.~both $\beta$-reduction or both $\textsf{if}$-reduction). Case a is also similar, except that the relevant positions are all disjoint rather than contained in one another, and either branch could be considered the right branch.

\begin{lemma} \label{downAcrossUp}
If $(N_1,\alpha_1) \sim^* (N_2,\alpha_2)$, there is some descendants $N'_1, N'_2$ of $N_1$ resp.~$N_2$, and a position $\alpha'$ in both of them such that $(N_1,\alpha_1) \sim_p^* (N'_1, \alpha') \sim_c^* (N'_2, \alpha') \sim_p^* (N_2, \alpha_2)$.
\end{lemma}
\begin{proof}
  The $\sim_p$ relation can be split into $\sim_\downarrow \cup \sim_\uparrow$, where $(A,\alpha) \sim_\downarrow (B,\beta)$ if $A \to B$ and $(A,\alpha) \sim_p (B,\beta)$, and similarly $(A,\alpha) \sim_\uparrow (B,\beta)$ if $B \to A$ and $(A,\alpha) \sim_p (B,\beta)$. At each stage of this proof, it will be assumed that the $\sim_c$ steps and the $\sim_p$ steps in the sequence from $(N_1,\alpha_1)$ to $(N_2,\alpha_2)$ are rearranged such that there is never a $\sim_c$ immediately before a $\sim_\downarrow$, and there is never a $\sim_c$ immediately after a $\sim_\uparrow$. This rearrangement is always possible, because if there is some subsequence $(A,\alpha) \sim_c (B,\alpha) \sim_\downarrow (C,\beta)$, then there is an alternate path $(A,\alpha) \sim_\downarrow (B',\beta) \sim_c (C,\beta)$, where the reduction and the $\sim_\downarrow$ from $A$ to $B'$ are the same as those from $B$ to $C$, and the $\sim_c$ step is the same, but with the reduction sequences $O_1 \to^* O_1'$ and $O_2 \to^* O_2'$ in the definition of $\sim_c$ extended by the reduction $B \to C$. With these rearrangements assumed, it is sufficient to prove that the $\sim_p$ steps can be rearranged so that all of the $\sim_\downarrow$ steps come before all of the $\sim_\uparrow$ steps (possibly introducing some $\sim_c$ steps in the process).

  The rearrangement to put all of the $\sim_\downarrow$s before all of the $\sim_\uparrow$s proceeds by induction on the number of $\sim_\downarrow$s that occur after the first occurrence of $\sim_\uparrow$. If it's 0, all of the $\sim_p$s are already in the correct order, and we're done. Otherwise, take the subsequence from the first $\sim_\uparrow$ to the first $\sim_\downarrow$ after that. This must be rearranged to some $\sim_\downarrow$s followed by some $\sim_\uparrow$s, then once that's done, the number of $\sim_\downarrow$s after the first $\sim_\uparrow$ in the overall sequence will have decreased by 1.

  Let this subsequence be $A \sim_\uparrow^n B \sim_\downarrow C$. Suppose for induction that $A \sim_\uparrow^k (D,\delta) \sim_\downarrow^* (E,\epsilon) \sim_\uparrow^* C$ for some $D, \delta, E, \epsilon$, and $0 \leq k \leq n$, where in the reduction sequence $D \to^* E$, for any pair of reductions at positions which aren't disjoint, the reduction at the innermost (greater) position occurs first. Reduction sequences with this property will be called ``parallel'', as it is analagous to the parallel reduction introduced by Tait and Martin-L\"of in their proof of the Church-Rosser theorem \cite{Bar84,Takahashi1995}. This induction is in reverse, with $k$ decreasing from $n$ to $0$. If $k = n$, simply set $(D,\delta) = B$ and $(E,\epsilon) = C$. If $k = 0$, then $A$ is related to $B$ by a sequence of $\sim_\downarrow$s then $\sim_\uparrow$s, as desired. In the intermediate steps, $k$ must be decreased by one, so a subsequence of one $\sim_\uparrow$ followed by a parallel sequence of $\sim_\downarrow$s must be replaced by a parallel sequence of $\sim_\downarrow$s followed by some $\sim_\uparrow$s.

  For any individual $\sim_\uparrow$ then $\sim_\downarrow$ pair, let $(A,\alpha) \sim_\uparrow (B,\beta) \sim_\downarrow (C,\gamma)$. The skeleton $B$ reduces to both $A$ and $C$. If these are the same reduction, then $(A,\alpha) = (C,\gamma)$ and this subsequence may be removed. If they are different, then the way they overlap corresponds to one of the cases in the definition of $\sim_c$, either case a if the reduction positions are disjoint, or one of the cases b--f if one of the reduction positions is inside the other. In any case, there is a reduction sequence from each of $A$ and $C$ to some common skeleton $D$ ($O_1$ and $O_2$ in the definition of $\sim_c$). For the same reason that $(B,\beta) \sim_\downarrow (C,\gamma)$, there is some $\delta$ such that $(A,\alpha) \sim_\downarrow^* (D,\delta)$, and similarly for the same reason that $(B,\beta) \sim_\downarrow (A,\alpha)$, for the same $\delta$, $(C,\gamma) \sim_\downarrow^* (D,\delta)$. There are a lot of cases to check for this statement, but all of them are rather simple, and similar to each other. There is therefore an alternative $\sim^*$ sequence from $(A,\alpha)$ to $(C,\gamma)$, of the form $(A,\alpha) \sim_\downarrow^* (D_1,\delta) \sim_c (D_2,\delta) \sim_\uparrow^* (C,\gamma)$, where $D_1$ and $D_2$ are the alternative reduction sequences leading to the same skeleton $D$. There are only multiple $\sim_\downarrow$ steps in the result if the position of the reduction $B \to C$ is inside the position of the reduction $B \to A$, and similarly, there are only multiple $\sim_\uparrow$ steps in the result if the position of the reduction $B \to A$ is inside the position of the reduction $B \to C$. If there are multiple $\sim_\downarrow$ steps, they are not necessarily parallel, but in the only case where they aren't (case f of $sim_c$), the subsequence of $\sim_\downarrow$ steps with the reductions at $\alpha;\lambda;@_1;\beta$ then $\alpha;\lambda;@_1;\gamma_i;\tY;\lambda;\beta$ for each $i$ where $\gamma_i \geq \beta$ (in the notation of the definition of $\sim_c$) can be replaced by $\sim_\downarrow$s with the reductions at $\alpha;\lambda;@_1;\gamma'_i;\tY;\lambda;\beta$ then $\alpha;\lambda;@_1;\beta$, then another $\sim_c$, where $(\gamma'_i)$ is the list of positions in $A$ (\emph{before} the reduction at $\beta$ to $A'$, unlike $(\gamma_i)$) where $A | \gamma'_i = x$ and $\gamma'_i > \beta$.

  The effect of this rearrangement is (ignoring the $\sim_c$s) to swap the order of the $\sim_\uparrow$ and the $\sim_\downarrow$, except that if the positions of one of these reductions is inside the other, that inner $\sim_p$ may be duplicated. If the positions are disjoint, both of them are unchanged (corresponding to case a of $\sim_c$), and if one is inside the other, then the outer position is unchanged and the other resultant positions, although they aren't equal to the original inner position, are still inside the outer position. Taking the sub-sequence of one $\sim_\uparrow$ followed by a parallel sequence of $\sim_\downarrow$s, this rearrangement can be repeatedly applied to move the $\sim_\uparrow$ further along the sequence. If at some point a $\sim_\uparrow$ and $\sim_\downarrow$ match (by relating the same (skeleton, position) pairs in opposite directions), they may be removed and the process may stop early. The $\sim_\uparrow$ may be duplicated if it passes a $\sim_\downarrow$ whose position is outside of the $\sim_\uparrow$'s position, but by the assumption that the sequence of $\sim_\downarrow$s is initially parallel, no position inside of this occurs later in the sequence, therefore all of the resultant $\sim_\uparrow$s pass the remaining $\sim_\downarrow$s without changing or duplicating them. The $\sim_\uparrow$s may be further duplicated, but the number of steps left in this process is bounded by the product, for all of the remaining $\sim_\downarrow$s, of $2 +$ the number of occurrences of the variable relevant to the reduction, because for each $\sim_\downarrow$, that is more than the number of duplicates it could produce for any $\sim_\uparrow$ that passes it. It is also possible (earlier) for some of the $\sim_\downarrow$s to be duplicated, but the only case where this process would not terminate, that both the $\sim_\downarrow$s and the $\sim_\uparrow$s continue being duplicated forever so that the number of switches left to do never decreases, is prevented by the parallelness condition, as all of the duplications of $\sim_\downarrow$s occur before all of the duplications of $\sim_\uparrow$s.

  It still remains to be shown that the sequence of $\sim_\downarrow$s left at the end of this can be made parallel. The changes that may have occurred since the previous version of this sequence (which was known to be parrallel already), are that some of the $\sim_\downarrow$s may have been duplicated by passing the $\sim_\uparrow$, and if that $\sim_\uparrow$ matches one of the $\sim_\downarrow$s, that $\sim_\downarrow$ is removed. The position of the $\sim_\uparrow$'s reduction is the same for all of these duplications, because it is not changed until later in the sequence, when the $\sim_\downarrow$s with positions outside of that may occur. The resultant positions after a duplication are in the same position relative to each other, and therefore the order of the remaining $\sim_\downarrow$s is automatically parallel, except that (letting the position of the $\sim_\uparrow$'s reduction be $\alpha$) if there are $\sim_\downarrow$s with reductions at positions $\alpha;@_1;\lambda;\beta$ and $\alpha;@_2;\gamma$, or $\alpha;\tY;\lambda;\beta$ and $\alpha;\tY;\lambda;\gamma$, some of the resultant $\sim_\downarrow$s' positions may be inside each other where they were originally not. Similarly to the case where a $\tY$ reduction and a reduction at a position inside it are swapped though, the order can be fixed by swapping some of the $\sim_\downarrow$ steps with each other. In the first case, the reduction at position $\alpha;@_1;\lambda;\beta$ ends up at $\alpha;\beta$, and the reduction at position $\alpha;@_2;\gamma$ ends up at the positions $\alpha;\delta_i;\gamma$, where $(\delta_i)$ is the positions of the variable relevant to the $\sim_\uparrow$'s reduction inside its lambda. The positions of the variable in the lambda may be different before the reduction at $\beta$, but the positions of the redexes corresponding to the original redex at $\alpha;@_2;\gamma$ are changed similarly. Rather than reducing at $\alpha;\beta$ then at $\alpha;\delta_i;\gamma$, it is therefore possible to reach the same point by reducing at $\alpha;\delta'_i;\gamma$ then at $\alpha;\beta$, where $(\delta'_i)$ is the positions of the relevant variable within the lambda before the reduction at $\alpha;@_1;\lambda;\beta$ (and again, a $\sim_c$ of some sort must also be introduced). The other case, that the $\sim_\uparrow$'s reduction is a $\tY$-reduction, is similar. Each reduction which is duplicated has one image at $\alpha;\lambda;@_1;$its original relative position, and one for each occurrence of the variable. Those corresponding to variable occurrences may need to be moved to before those at $\lambda;@_1$.

  In summary, to reach the desired order of $\sim_p$ steps, each $\sim_\downarrow$ may have to be moved past some $\sim_\uparrow$ steps, and it may get duplicated in the process, but the resultant sequence of $\sim_\downarrow$s can all be put in parallel order, and it has been shown that moving a $\sim_\uparrow$ past a sequence of $\sim_\downarrow$s is always possible if they're in parallel order, therefore the overall process terminates and reaches a point where all of the $\sim_\downarrow$s precede all of the $\sim_\uparrow$s, and all of the $\sim_c$s are in between them. All of these rearrangements leave the end points of the sequence unchanged, therefore this is an alternative sequence of $\sim$ steps between $(N_1,\alpha_1)$ and $(N_2,\alpha_2)$ of the form $(N_1,\alpha_1) \sim_\downarrow^* (N_1', \alpha') \sim_c^* (N_2', \alpha') \sim_\uparrow^* (N_2,\alpha_2)$.
\end{proof}

If $(X,\alpha) \sim_c (Y,\alpha)$ then the final skeletons in $X$ and $Y$ are equal, and $(X,\beta) \sim_c (Y,\beta)$ for any $\beta$ that occurs in this skeleton, therefore $\sim_c$ can be considered to apply to reduction sequences even without positions, $X \sim_c Y$ iff $(X,\cdot) \sim_c (Y,\cdot)$. The structure of $\sim_c$ can be seen more clearly by choosing a canonical example from each equivalence class. For this, the member of the class that's in call-by-value order is used. A reduction sequence is in call-by-value order if for every pair of reductions in the sequence at positions $\alpha$ and $\beta$, where the reduction at $\alpha$ happens first, either $\alpha \leq \beta$ or $\alpha$ and $\beta$ are disjoint, with $\alpha$ to the left of $\beta$ (i.e.~the first elements of the sequences $\alpha, \beta$ where they differ are $@_1$ and $@_2$, $\underline f_i$ and $\underline f_j$ for $i < j$, or $\textsf{if}_i$ and $\textsf{if}_j$ for $i < j$, in that order). Note that although the reductions that occur in the sequence are in call-by-value order, it may not be the actual call-by-value reduction sequence starting from that point because some redexes may remain un-reduced even when other reductions that should happen afterwards occur. It is only required that the reductions that do occur occur in the correct order.

\begin{lemma} \label{canonicalCousins}
For any reduction sequence $X$, there is a unique reduction sequence $X_{cbv}$ that is related to $X$ by $\sim_c$ and is in call-by-value order, and if $X \sim_c^* Y$, then $X_{cbv} = Y_{cbv}$.
\end{lemma}

\begin{proof}

Define $X_n$ recursively so that $X_0 = X$ and if $X_n$ is not in CBV order, take the last pair of reductions in $X_n$ which aren't in CBV order. They are adjacent in the sequence, as the CBV order is a total order. Let the positions of these reductions be $\alpha$ and $\beta$ respecively. If they are disjoint, they can be considered as one of the branches of $\sim_c$ case a. Otherwise, $\beta < \alpha$, and the sub-skeleton at position $\beta$ at the appropriate point in the reduction sequence is a redex with non-trivial subterms, therefore it's of one of the forms $\tif{\skeletonPlaceholder>0}{A}{B}$, $(\lambda x.A) B$ or $\tY \lambda x. A$, therefore the reductions $\alpha$ and $\beta$ form the $O_2$ branch of one of the cases b, c, d, e or f of $\sim_c$. In either case, define $X_{n+1}$ as equal to $X_n$ except taking the other (left) branch, so that the order of the reductions at $\alpha$ and $\beta$ are switched (although the equivalent(s) of the reduction at $\alpha$ may not actually be at that position).

This sequence of swaps rearranges the reductions in $X$ rather like insertion-sort (but in some cases duplicating or deleting the new element), and eventually reaches some $X_n$ that is in CBV order. This is defined to be $X_{cbv}$.

\paragraph{}
If $Y \sim_c X \sim_c^* X_{cbv}$, and the reductions involved in $Y \sim_c X$ are not the last pair of reductions in $Y$ that aren't in CBV order, the order of the $\sim_c$s can be rearranged so that the sequence $Y \sim_c^* X_{cbv}$ is the canonical sequence given above, therefore $Y_{cbv} = X_{cbv}$. To be more precise about this rearrangement, consider the sequence of $\sim_c$ steps from $Y$ to $X_{cbv}$. The first step takes some sub-sequence of the reduction sequence $Y$ and swaps the order of those reductions. If $X = X_{cbv}$, then $X = Y_1 = Y_{cbv}$ already. Otherwise, consider the cases according to whether the image of this subsequence in $X$ occurs earlier than the next pair of reductions to be swapped (i.e.~the last pair of reductions in $X$ that occur not in CBV order). If not, then either the step $Y \sim_c X$ changes this subsequence into CBV order, or out of it. If it is into, then it was the last out-of-order pair in $Y$ therefore the whole sequence $Y \sim_c^* X_{cbv}$ is already in the canonical order and $Y_{cbv} = X_{cbv}$. If it switches the pair to the wrong order, then the result of that switch is the last out-of-order pair in $X$ therefore $Y = X_1$, and the sequence $X_1 \sim_c^* X_{cbv}$ is just a suffix of the sequence $X \sim_c^* X_{cbv}$ therefore $(X_1)_{cbv} = X_{cbv}$ and again $Y_{cbv} = X_{cbv}$.

The remaining cases are when the subsequence of reduction steps involved in $Y \sim_c X$ occur before the last out-of-order pair in $X$, but possibly overlapping. If there is no overlap, then the $\sim_c$ steps do not interfere with each other at all, and can simply be performed in the other order. This gives a different sequence of $\sim_c$ steps $Y \sim_c Y_1 \sim_c X_1 \sim_c^* X_{cbv}$. As $Y_{cbv} = (Y_1)_{cbv}$, we can proceed by induction in this case and use the fact that $(Y_1)_{cbv} = (X_1)_{cbv} = X_{cbv}$ to acheive the result.

In the other case, there is some overlap between the regions involved in $Y \sim_c X$ and $X \sim_c X_1$, but the last reduction in $X \sim_c X_1$ is later than the last reduction in $Y \sim_c X$. There are only 2 reductions in $X$ involved in $X \sim_c X_1$, therefore the region involved in $Y \sim_c X$ ends on the first of the reductions involved in $X \sim_c X_1$. Now consider the specific sequence of positions of the reductions in $X$ involved in either of these steps. Let the last of these positions be $\alpha$, the first $\gamma$, and the second-last, third-last and so on $\beta_1, \beta_2, \dots$ respectively, and let the positions of the redexes in $Y$ involved in $Y \sim_c X$ be $\beta$ then $(\gamma_i)$ in order.. Then $X \sim_c X_1$ swaps $\beta_1$ and $\alpha$, and $\alpha$ comes before $\beta_1$ in CBV order (i.e.~$\alpha < \beta_1$ or $\alpha$ is left of $\beta_1$), and the step $Y \sim_c X$ either proceeds forwards towards CBV order, swapping $\beta$ below $\gamma_1 = \gamma$ to form $\gamma$ then $(\beta_i)$, or proceeds backwards, taking $Y$ even farther from CBV order, swapping the $(\gamma_i)$ to above $\beta$, forming $\gamma$ then $\beta_1 = \beta$. In each of the cases below, it is required to show that the canonical sequence $(X_i)_i$ eventually reaches some term which is the same as one in $(Y_j)_j$, and from that point on, they match, therefore they reach the same end result: $X_{cbv} = Y_{cbv}$.

\begin{itemize}
  \item $Y \sim_c X$ in the backwards direction (so that $Y$ is closer to CBV order than $X$ is): By swapping the roles of $X$ and $Y$, and $\beta$ and $\gamma$, this is equivalent to the $Y \sim_c X$ forwards case. The assumption that $\alpha$ is before $\beta_1$ in CBV order (so that $\beta_1, \alpha$ is the pair to be swapped in $X \sim_c X_1$) maps to the assumption that $\alpha$ is before $\gamma_1$ (so that the result doesn't just follow trivially by $Y_1$ being equal to $X$) and vice-versa. Then one of the cases below establishes that $X_i = Y_j$ for some $i$ and $j$, therefore after swapping back, $Y_i = X_j$.
\item $Y \sim_c X$ forwards and $\alpha$ comes after $\gamma$ in CBV order: In this case, the $\gamma$ and $\alpha$ reduction steps are already in the correct order in $Y$, therefore $\beta$ and $\gamma$ are already the last out-of-order pair of reductions in $Y$, $Y_1 = X$, and $Y \sim_c X \sim_c^* X_{cbv}$ is already the canonical sequence from $Y$ therefore $Y_{cbv} = X_{cbv}$.
\item $Y \sim_c X$ forwards and $\alpha$ is disjoint from $\gamma$ and $\beta$: In this case, the sequence $(X_i)_i$ starts by swapping $\alpha$ and $\beta_1$ by case a, then because $\gamma$ is disjoint from $\alpha$ and comes after is in CBV order, $\gamma$ is right of $\alpha$, therefore either all the remaining $\beta_i$s, are $\geq \gamma$ (if $\beta$ and $\gamma$ aren't disjoint) or there are no remaining $\beta_i$s (if $\beta$ and $\gamma$ are disjoint, in which case they swap by case a and there is only $\beta_1$). The sequence $(X_i)_i$ therefore proceeds to swap all the remaining $\beta_i$s then $\gamma$, in order, below $\alpha$, resulting in $X_i$ for some $i$ having as its relevant sub-reduction-sequence $\alpha, \gamma, \beta_k, \dots, \beta_1$. The other sequence $(Y_j)_j$ starts by swapping $\gamma$ then $\beta$ below $\alpha$, then swapping $\gamma$ and $\beta$. Because $\alpha$ is disjoint from $\gamma$, the reduction there does not affect the sub-skeleton at position $\gamma$, therefore this last $\sim_c$ step proceeds identically to how it did in $Y \sim_c X$, resulting in $\gamma, \beta_k, \dots, \beta_i$. Overall, this results in $Y_3$ having the relevant sub-reduction-sequence $\alpha, \gamma, \beta_k, \dots, \beta_1$, therefore $Y_3 = X_i$ for the aforementioned value of $i$.
\item $Y \sim_c X$ forwards, $\alpha \leq \gamma$ and $\alpha$ (and therefore also $\gamma$) is disjoint from $\beta$: In this case, $(X_i)_i$ proceeds by swapping $\beta_1 = \beta$ below $\alpha$ by case a, then swapping $\gamma$ below $\alpha$ by one of the other cases, and $(Y_j)_j$ proceeds by swapping $\gamma$ and $\alpha$, then swapping $\beta$ below $\alpha$ then all of the images of $\gamma$ (which are $\geq \alpha$ and therefore left of $\beta$). In both cases, the subterm at $\alpha$ when $\alpha$ and $\gamma$ are swapped is unaffected by the reduction at $\beta$, therefore it produces the same result in both cases, therefore the overall sequence in both cases is $\alpha$ then the images of $\gamma$ then $\beta$.
\item Both $\gamma$ and $\beta$ are $> \alpha$ but they're disjoint from each other, and either the skeleton at position $\alpha$ is of the form $\tif{\skeletonPlaceholder > 0}{A}{B}$ or it is of the form $A B$ and either both $\gamma$ and $\beta$ are $> \alpha;@_1$ or they are both $> \alpha;@_2$: In this case, $(X_i)_i$ proceeds by swapping $\beta$ then $\gamma$ below $\alpha$, in each case possibly forming 0 or multiple images. The only way the number of images of each of these positions can be different is if the subskeleton at $\alpha$ is an $\textsf{if}$, and there are 0 of one of them and 1 of the other, in which case they're trivially in the correct order already. Otherwise, all of the images of $\beta$ and $\gamma$ are disjoint from one another, therefore none of the positions, or their relative order, change when they are swapped, therefore the next part of the sequence $(X_i)_i$ is just insertion-sort running on the images of $\beta$ and $\gamma$ with $\sim_c$ case a swaps until they're in the correct order. The sequence $(Y_j)_j$ starts by swapping $\gamma$ then $\beta$ below $\alpha$, then as before, sorting the images into the correct order by case a swaps. The images originally formed of $\gamma$ and $\beta$ are the same in both cases, therefore the final order is the same, so $X_i = Y_j$ for some $i$ and $j$.
\item $Y \sim_c X$ forwards, $\alpha;@_1 < \gamma$, and $\alpha;@_2 < \beta$: Let $x$ be the variable involved in the $\beta$-reduction at $\alpha$, so that the sub-skeleton at $\alpha$ is $(\lambda x. A) B$ for some $A, B$, where $\gamma = \alpha;@_1;\lambda;\gamma'$ and $A \to A'$ at $\gamma'$, and $\beta = \alpha;@_2;\beta'$, and $B \to B'$ at $\beta'$. In this case, $(X_i)_i$ starts by swapping $\beta$ below $\alpha$ by case e, producing one image of $\beta$ for each $x$ in $A'$, then swapping $\gamma$ below $\alpha$ by case d, producing a single image $\alpha;\gamma'$. For each of the instances of $x$ in $A$ left of $\gamma'$, the reduction at $\alpha;\gamma'$ is then swapped below the corresponding image of $\beta$. The sequence $(Y_j)_j$ starts by swapping $\gamma$ below $\alpha$ then $\beta$ below $\alpha$, but in this case the images of $\beta$ produced may be different. In this case the swap happens earlier in the reduction sequence than $\gamma$, so that the body of the lambda is still $A$ rather than $A'$, and the reduction at $\gamma$ may rearrange or change the number of instances of $x$. Next, each of the images of $\beta$ to the right of $\alpha;\gamma'$ are swapped with it by case a. At this point, the next few reductions before $\alpha;\gamma'$ will in general be images of $\beta$ at positions inside $\alpha;\gamma'$. The subskeleton at position $\alpha;\gamma'$ before these reductions is $(A|\gamma')[B/x]$, then by the images of $\beta$ this reduces to $(A|\gamma')[B'/x]$, then it reduces at its root position ($\alpha;\gamma'$ in the overall skeleton) to $(A'|\gamma')[B'/x]$. For each position of an $x$ in $(A|\gamma')$, there are 0 or more corresponding positions of $x$ in $(A'|\gamma')$, depending on what type of reduction $A \to A'$ is. Because $B$ and $B'$ cannot contain any instances of the variable involved in the reduction $A \to A'$ (if it's a $\beta$-reduction or $\tY$-reduction), each instance of $B$ in $(A|\gamma')[B/x]$ has the same set of images in $(A'|\gamma')[B/x]$ as the corresponding $x$ in $(A|\gamma')$. When these images of $\beta$ that overlap with the image of $\gamma$ are swapped below it, each of them has its own images, one at each position of $B$ in $(A'|\gamma')[B/x]$ corresponding to its original copy of $B$ in $(A|\gamma')[B/x]$. After all of them are swapped below $\alpha;\gamma'$ (and rearranged among themselved by case a), there is therefore one reduction at each copy of $B$ in $(A'|\gamma')[B/x]$, i.e.~one at position $\beta'$ relative to each $x$ in $(A'|\gamma')$. They are therefore the same as the images of $\beta$ in $X_1$ that overlap with $\alpha;\gamma'$, because those are also one reduction at position $\beta'$ relative to each $x$ in $(A'|\gamma)$. Both $(X_i)_i$ and $(Y_j)_j$ therefore eventually reach a point where the relevant sub-reduction-sequence is $\alpha$, the images of $\beta$ to the left of $\alpha;\gamma'$, $\alpha;\gamma'$ itself, the aforementioned images of $\beta$ that overlap with $\alpha;\gamma'$, then all the images of $\beta$ to the right of $\alpha;\gamma'$.
\item $Y \sim_c X$ forwards, $\gamma$ and $\beta$ are disjoint and both $> \alpha$, and the reduction at $\alpha$ is a $\tY$-reduction: This is similar to the previous case, but more complicated, so it will need some definitions to explain properly what's going on. Let the term at $\alpha$ before any of the reductions be $\tY \lambda x. A$, let $\gamma = \alpha;\tY;\lambda;\gamma'$, let $\beta = \alpha;\tY;\lambda;\beta'$, let $A | \beta' = B \to B'$ with the reduction at $\cdot$, let $A | \gamma' = C \to C'$ with the reduction at $\cdot$, let the positions in $A$ where $x$ occurs that are left of $\gamma'$, between $\gamma'$ and $\beta'$ (but still disjoint from both), and right of $\beta'$ respectively be $(\delta^l_i)$, $(\delta^m_i)$ and $(\delta^r_i)$, let the positions in $B$, $C$, $B'$ and $C'$ respectively where $x$ occurs be $(\delta^B_i)$, $(\delta^C_i)$, $(\delta^{B'}_i)$ and $(\delta^{C'}_i)$ (so that all the positions in $A$ where $x$ occurs, in left-to-right order, are $(\delta^l_i)_i, (\delta^C_i)_i, (\delta^m_i)_i, (\delta^B_i)_i, (\delta^r_i)_i$). The sequence $(X_i)_i$ starts by swapping $\beta$ then $\gamma$ below $\alpha$. With the shorthand that $\gamma'(\epsilon) = \alpha;\lambda;@_1;\epsilon;\tY;\lambda;\gamma'$, $\gamma'_0 = \alpha;\lambda;@_1;\gamma'$, and similarly for $\beta'$, the relevant portion of $X_2$ is then $\alpha$, $(\gamma'(\delta^l_i))_i$, $\gamma'_0$, $(\gamma'(\delta^{C'}_i))_i$, $(\gamma'(\delta^m_i))_i$, $(\gamma'(\delta^B_i))_i$, $(\gamma'(\delta^r_i))_i$, $(\beta'(\delta^l_i))_i$, $(\beta'(\delta^{C'}_i))_i$, $(\beta'(\delta^m_i))_i$, $\beta'_0$, $(\beta'(\delta^{B'}_i))_i$, $(\beta'(\delta^r_i))_i$. Next in $(X_k)_k$, for each $i$, $\gamma'(\delta^r_i)$ are swapped past all the images of $\beta$ until it is immediately before $\beta'(\delta^r_i)$ by case a, then for each $i$, $\gamma'(\delta^B_i)$ is swapped past all the images of $\beta$ until $\beta'_0$ by case a, then swapped with $\beta'_0$ by one of the other cases depending on what type of reduction $B \to B'$ is and the relative positions. Even if it's case d or f, all of the images of $\gamma'(\delta^B_i)$ are disjoint from each other (and from the positions of the other reductions after $\beta'_0$) because the redex, $C$, doesn't contain any instances of the variable involved in the reduction $B \to B'$. Expanding the definitions, this is swapping $\alpha;\lambda;@_1;\delta^B_i;\tY;\lambda;\gamma'$ with $\alpha;\lambda;@_1;\beta'$ at a point in the reduction sequence where the subskeleton at $\alpha;\lambda;@_1;\beta'$ is $B$ with a mixture of $\tY \lambda x. A$ and $\tY \lambda x. A[C'/\gamma']$ substituted for its occurrences of $x$, and one of these occurrences of $x$ is at $\delta^B_i$. For each $\delta^B_j$, there are some corresponding $\delta^{B'}_k$, where the images of the $x$ at $\delta^B_j$ end up after the reduction $B \to B'$. These cover all the $\delta^{B'}_k$, and uniquely, and for each $\delta^B_i$, the images of $\zeta;\delta^B_i;\theta$ after swapping it with $\zeta;\beta'$ are precisely $(\zeta;\delta^{B'}_k;\theta)$ for those same values of $k$, therefore after swapping all of the $(\gamma'(\delta^B_i))_i$ down past $\beta'_0$, their images are $(\gamma'(\delta^{B'}_i))_i$. After swapping with $\beta'_0$, each of these images than swaps by case a to take its place among the $(\beta'(\delta^{B'}_i))_i$. Next up in $(X_i)_i$, the $(\gamma'(\delta^m_i))_i$s and the $(\gamma'(\delta^{C'}_i))_i$s swap down past some images of $\beta$ they're disjoint from to take their place before their matching image of $\beta$, then $\gamma'_0$ swaps past the $(\beta'(\delta^l_i))_i$, stopping immediately before $\beta'(\delta^{C'}_1)$, then the $(\gamma'(\delta^l_i))_i$ and the $(\beta'(\delta^l_i))_i$ mix. The end result of the rearrangement of (this subsequence of) the reduction sequence is therefore $\alpha$, $(\gamma'(\delta^l_i), \beta'(\delta^l_i))_i$, $\gamma'_0$, $(\gamma'(\delta^{C'}_i), \beta'(\delta^{C'}_i))_i$, $(\gamma'(\delta^m_i), \beta'(\delta^m_i))_i$, $\beta'_0$, $(\gamma'(\delta^{B'}_i), \beta'(\delta^{B'}_i))_i$, $(\gamma'(\delta^r_i), \beta'(\delta^r_i))_i$.

  The other sequence, $(Y_j)_j$, proceeds similarly, but with the images of $\gamma$ starting out after the images of $\beta$. It swaps all the images of $\beta$ that are right of $\gamma_0$ to the appropriate places by case a, then swaps all the $(\beta'(\delta^C_i)_i$ first to then past $\gamma'_0$, resulting in $(\beta'(\delta^{C'}_i))_i$ for the same reason as with swapping those images of $\gamma$ that overlapped with $\beta'_0$ past it in the $(X_i)_i$ case above, then finally swapping the $(\beta'(\delta^l_i))_i$s and the $(\gamma'(\delta^l_i))_i$s into the correct order. As required, this produces the same result as in $(X_i)_i$.
  \item $Y \to X$ forwards, and $\alpha < \gamma < \beta$: Let $\gamma'$ be such that $\gamma = \alpha;\textsf{if}_i;\gamma'$, $\alpha;@_1;\lambda;\gamma'$, $\alpha;@_2;\gamma'$ or $\alpha;\tY;\lambda;\gamma'$, so that $\gamma'$ is the freely varying later part of $\gamma$ as in the definition of $\sim_c$, whichever case applies to swapping $\gamma$ and $\alpha$. Similarly, let $\beta'$ be the freely varying part of $\beta$ relative to $\gamma$. Let the images of $\gamma$ after swapping it with $\alpha$ (where this swap occurs later in the reduction sequence than $\beta$) then be $(\alpha;\delta_i;\gamma')_i$, and the images of $\beta$ after swapping it with $\gamma$ be $(\gamma;\epsilon_i;\beta')_i$. Depending on the skeleton at $\alpha$, and $\gamma$'s position within it, $(\delta_i)_i$ may be empty (case b), a singleton containing $\cdot$ (cases c and d), all the positions of the relevant variable in the lambda in the skeleton at $\alpha$ (case e), or $\lambda;@_1$ and $\lambda;@_1;\zeta;\tY;\lambda$ for each position $\zeta$ of the relevant variable (case f), but in all cases (except a, which is excluded because none of $\alpha, \gamma$ and $\beta$ are disjoint) the set of images has this general structure. Furthermore, the positions $(\delta_i)_i$ are determined only by the general position of $\gamma$ within $\alpha$ (the part excluded from $\gamma'$), and the positions of the relevant variable within the skeleton at $\alpha$. They do not depend on $\alpha$ or $\gamma$, so that if both initial positions were moved somewhere else the relative positions of the images of $\gamma$ would be unaffected, and similarly if some position within $\gamma$ were swapped with $\alpha$, the relative positions of its images would be the same (except to the extent that, in case f, the positions of the relevant variable are taken after the inner reduction takes place, so that some of those may still change). Because there are so many of them and they don't actually affect the multiset of positions where reductions take place, I will be ignoring swaps by case a in the proof of this case, and just assuming that all the final positions end up in the correct order.
  
  The final set of reduction positions that I will be proving both $(X_i)_i$ and $(Y_j)_j$ reach is $\alpha, (\alpha;\delta_i;\gamma', (\alpha;\delta_i;\gamma';\epsilon_j;\beta')_j)_i$, where as usual the sequences are expanded out to the full list. For the sequence $(X_i)_i$, first the images of $\beta$ in $X_0$ are $(\gamma,\epsilon_i,\beta')$. For each of these in turn, it is swapped with $\alpha$. By the fact mentioned above that sub-positions are mapped to the same set of images except in case f, the images of $\gamma,\epsilon_j,\beta'$ after this swap are $(\alpha;\delta_i^j;\gamma',\epsilon_j,\beta')_i$, where $(\delta_i^j)_i$ is the same as $(\delta_i)_i$ except that in the case where this is not the first image of $\beta$ swapped below $\alpha$ and the reduction at $\alpha$ is a $\tY$-reduction (and therefore swaps with it proceed by case f), the positions of the relevant variable are taken after the reductions at $\gamma$ and $\gamma;\epsilon_k;\beta'$ for $k \leq j$ (this implies that the first of these, $\delta_i^n = \delta_i$). All the images $\alpha;\delta_i^j;\gamma';\epsilon_j;\beta'$ where $\delta_i^j > \lambda;@_1;\gamma';\epsilon_{j+1};\beta'$ are then swapped with $\alpha;\lambda;@_1;\gamma';\epsilon_{j+1};\beta'$ (which is $\alpha;\delta_i^{j+1};\gamma';\epsilon_{j+1};\beta'$ for some $i$). As in the case where $\alpha;\tY < \gamma, \beta$ and $\gamma$ and $\beta$ are disjoint, the effects of these swaps may duplicate or delete the reductions in such a way that all the remaining images of $\gamma;\epsilon_j;\beta'$ are $(\alpha;\delta_i^{j+1};\gamma';\epsilon_j;\beta'$. This is repeated with $\delta_i^{j+2}$ and so on until these images are all in the correct order, at which point they are $(\alpha;\delta_i;\gamma';\epsilon_j;\beta')_i$. After all of these are done, $\gamma$ is swapped with $\alpha$, yielding $(\alpha;\delta_i^{-1};\gamma')_i$. As with the images of $\beta$, the subset of these which are inside $\alpha;\lambda;@_1;\gamma';\epsilon_j;\beta'$ for each $j$ in turn are swapped with it, until they are all in the correct order and the images left of $\gamma$ are $(\alpha;\delta_i;\gamma')_i$. At this point, $X_i$ has the desired value.
  
  The sequence $(Y_j)_j$ proceeds similarly. First $\gamma$ swaps with $\alpha$, yielding $\alpha$ and $(\alpha;\delta_i;\gamma')_i$, then $\beta$ swaps with $\alpha$, yielding one image of $\beta$ for each $\delta_i$, except that, in case f, those images at positions inside $\alpha;\lambda;@_1;\gamma'$ may differ because they are earlier in the reduction sequence than any of the images of $\gamma$. Each of the images of $\beta$ in turn it moved to the corresponding image of $\gamma$, except that those overlapping with $\alpha;\lambda;@_1;\gamma'$ may be duplicated or deleted on the way. The images of $\beta$ after just this process (which does not actually correspond to any term in $(Y_j)_j$, but it's sufficiently independent that the order doesn't matter that much) are $(\alpha;\delta_i;\gamma';\beta'')$ (where $\gamma;\beta'' = \beta$). The images of these for each $i$, after swapping with $\alpha;\delta_i;\gamma'$, is $(\alpha;\delta_i;\gamma';\epsilon_j;\beta')_j$, because the reductions produced by a swap are unaffected by its overall location, and the skeleton at $\alpha;\delta_i;\gamma'$ at the relevant point in the sequence is equal to the skeleton at $\gamma$ initially, therefore this is equivalent to immediately swapping $\beta$ and $\gamma$ as in $Y \sim_c X$ (except that in the case where the reduction at $\alpha$ is a $\tY$-reduction, the skeleton at $\alpha;\lambda;@_1;\gamma'$ is not actually equal to the skeleton initially at $\gamma$ because something was substituted in for the variable bound at $\alpha;\tY$, but this doesn't affect the variable involved in the reduction at $\gamma$, so this doesn't actually matter). Therefore also in this case, eventually the set of reductions in the relevant portion of $Y_j$ is $\alpha, (\alpha;\delta_i;\gamma', (\alpha;\delta_i;\gamma';\epsilon_j;\beta')_j)_i$, therefore it is equal to some $X_i$.
\end{itemize}
In summary, if the swap in $Y \sim_c X$ doesn't overlap with the swap $X \sim_c X_0$, the sequence of swaps can be rearranged until it does, and in any other case, there is some $Y_j$ that's equal to some $X_i$, therefore these sequences reach the same end point and if $Y \sim_c X$, then $Y_{cbv} = X_{cbv}$, and by chaining these together, if $Y \sim_c^* X$, $Y_{cbv} = X_{cbv}$.

\paragraph{}
If $Y$ is in CBV order and $Y \sim_c^* X$, then $Y_{cbv} = X_{cbv}$ but also $Y_{cbv} = Y$ because the sequence $(Y_i)_i$ terminates immediately therefore $X_{cbv}$ is the unique reduction sequence in call-by-value order that's related to $X$ by $\sim_c^*$.
\end{proof}

\lemmaSimMN*

\begin{proof}
$\sim_N^*$ is trivially a subset of $\sim_M^*$ because $\sim_N$ is a subset of $\sim_M$.

In the other direction, suppose $(N_1, \alpha_1) \sim_M^* (N_2, \alpha_2)$ where both $N_1$ and $N_2$ are descendants of $N$. By \Cref{downAcrossUp}, take $(N_1, \alpha_1) \sim_{p,M}^* (N'_1, \alpha') \sim_{c,M}^* (N'_2, \alpha') \sim_{p,M}^* (N_2, \alpha_2)$. 
The $\sim_p^*$ steps remain within $Rch(N)$, and $\sim_p$ does not depend on the history of the reduction sequences, therefore $(N_1, \alpha_1) \sim_{p,N}^* (N'_1, \alpha') \sim_{c,M}^* (N'_2, \alpha') \sim_{p,N}^* (N_2, \alpha_2)$.

Let $X$ be the call-by-value reduction sequence related to both $N'_1$ and $N'_2$ by $\sim_{c,M}$ given by \Cref{canonicalCousins}. 
As $M \to N$ is the first reduction in the sequence $N'_1$, it is the last to be affected by the $\sim_{c,M}$ sequence $N'_1 \sim_{c,M}^* X$ given by \Cref{canonicalCousins} therefore it can be split into $N'_1 \sim_{c,M}^* Y_1 \sim_{c,M}^* X$ where $Y_1$ is in CBV order except possibly for its first reduction, which is still $M \to N$. Let the position of the reduction $M \to N$ be $\beta$. 
\akr{TODO: fill in the details that are missing here. It shouldn't be too hard, it's just a bit sketchy at present.} 
The rearrangement of the reduction sequence $Y_1 \sim_{c,M} X$ consists of moving the reduction at $\beta$ down past the other reductions in $Y_1$, possibly duplicating or deleting it in the process, but not affecting the positions or the correct order for any of the other reductions. 
The reductions derived from $M \to N$ can be identified as follows:

%\begin{definition}
A position in some (reduction sequence of) term(s) is \emph{derived from} another if it is related by the reflexive transitive closure of $\rightsquigarrow$, where $(V,\gamma) \rightsquigarrow (W,\delta)$ just if $V \to W$ and one of the following cases holds:
\begin{itemize}
    \item $(V,\gamma) \sim_p (W,\delta)$
    \item $\gamma = \delta$, $V \to W$ at $\epsilon$, and $\epsilon > \gamma$
    \item $V \mid \epsilon = (\lambda x.U) Z, U | \zeta = x, \gamma = \epsilon;@_2;\theta$ and $\delta = \epsilon;\zeta;\theta$
    \item $V \mid \epsilon = \tY (\lambda x.U), \gamma = \epsilon;\tY;\lambda;\zeta$ and $\delta = \epsilon;\lambda;@_1;\zeta$
    \item $V \mid \epsilon = \tY (\lambda x.U), U | \zeta = x, \gamma = \epsilon;\theta$ and $\delta = \epsilon;\lambda;@_1;\zeta;\theta$
\end{itemize}
%\end{definition}

Crucially, the reductions in $X$ can be partitioned into 2 sets: those at positions derived from $(M,\beta)$, and those at positions equal to the reductions in $Y_1$ (in the same order as they occur in $Y_1$). Using the same construction for $N'_2$ shows that the positions of the reductions in $Y_2$ are also the positions of the reductions in $X$ other than those derived from $(M,\beta)$, therefore $Y_1 = Y_2$ therefore $N'_1 \sim_{c,M} Y_1 \sim_{c,M} N'_2$ and every reduction sequence in this sequence starts with $M \to N$ therefore $N'_1 \sim_{c,N} N'_2$.

Combining this with the $\sim_{p,N}^*$s at the beginning and end then yields the desired result that $(N_1,\alpha_1) \sim_N^* (N_2, \alpha_2)$, therefore the restriction of $\sim_M^*$ to $\sim_N$'s domain ($Rch(N)$) is a subset of $\sim_N^*$ therefore the two versions of $\sim$ match on this domain, as desired.
\end{proof}

\churchRosser*

\begin{proof}
Suppose that $(M,s) \Rightarrow^* (M_1,s_1)$ and also $(M,s) \Rightarrow^* (M_2,s_2)$, then it is required to prove that there is some $(M',s')$ such that both $(M_1,s_1) \Rightarrow^* (M',s')$ and $(M_2,s_2) \Rightarrow^* (M',s')$. First consider the special case where $(M,s) \Rightarrow (M_1,s_1)$ and $(M,s) \Rightarrow (M_2,s_2)$, with only a single step in each case. Let the positions of the redexes in $M \to M_1$ and $M \to M_2$ be $\alpha_1$ and $\alpha_2$ respectively.

First consider the case that $\alpha_1$ and $\alpha_2$ are disjoint. Let $M_1 = M[X_1/\alpha_1]$ and similarly for $X_2$, then let $M' = M[X_1/\alpha_1][X_2/\alpha_2]$. As the positions are disjoint, the substitutions commute and both $M_1$ and $M_2$ reduce (with $\to$) to $M'$. 
Let $s' = s \circ i(M \to M_1) \circ i(M_1 \to M')$. 
The injection $i(M \to M_1) \circ i(M_1 \to M')$ consists of prepending $M \to M_1 \to M'$ to each reduction sequence, but by case a of $\sim_c$, this is equivalent to prepending $M \to M_2 \to M'$, which is $i(M \to M_2) \circ i(M_2 \to M')$. 
In the case that the reduction $M \to M_2$ isn't a $\tsample$-reduction, this is enough to establish that $(M_1, s_1) \Rightarrow (M', s')$ (and similarly if the redex of $M \to M_1$ isn't $\tsample$, $(M_2,s_2) \Rightarrow (M',s')$).
If it is $\tsample$ though, in order for it to be the case that $(M_1, s_1) \Rightarrow (M', s')$, it is additionally necessary that $X_2$, the result of the reduction at $\alpha_2$, be $s_1(M_1, \alpha_2)$, which follows from the fact that $(M, \alpha_2) \sim (M_1, \alpha_2)$ by case 1 of $\sim_p$. 
The case that the reduction $M \to M_1$ is a $\tsample$-reduction is similar.

The case that $\alpha_1 = \alpha_2$ is trivial, because there is at most one possible $\Rightarrow$ reduction at any given position, therefore $(M_1, s_1) = (M_2, s_2)$ already.

The remaining case is that $\alpha_1 < \alpha_2$ or $\alpha_1 > \alpha_2$. 
Assume without loss of generality that $\alpha_1 < \alpha_2$. 
For each possible case of what type of redex $M | \alpha_1$ is, and $\alpha_2$'s position within it, there is a corresponding case of $\sim_c$, and similarly to the case where $\alpha_1$ and $\alpha_2$ are disjoint, the term $O_1 = O_2$ from the definition of $\sim_c$ is a suitable value of $M'$. 
The term $M \mid \alpha_1$ can't be $\tsample$, because it has strict subterms, but the case that $M \mid \alpha_2 = \tsample$ is still somewhat more complicated. 
$\alpha_2$ can't be within $\alpha_1 ; @_2$ or $\alpha_1 ; Y$ (cases e and f of $\sim_c$) because those are not valid positions to reduce a $\tsample$, 
and the case that $\alpha_2$ is within the branch of an if statement that is deleted by the reduction at $\alpha_1$ (case b) doesn't actually present a problem because there is no corresponding reduction to $M \to M_2$ in the other branch, 
which leaves cases c and d, that $\alpha_2$ is in the $\tif{\cdot}{\cdot}{\cdot}$ branch that isn't deleted, and that $\alpha_2 < \alpha_1 ; @_1 ; \lambda$.  
The values that the $\tsample$s take in these cases match because $(M,\alpha) \sim_p (M_1, \alpha;\beta)$ by cases 3 and 2 of $\sim_p$ respectively.

\paragraph{}
In the case that $(M,s) \Rightarrow^* (M_1,s_1)$ or $(M,s) \Rightarrow^* (M_2,s_2)$ by multiple steps, it is possible to repeatedly replace a pair of the form $A \Leftarrow B \Rightarrow C$ by $A \Rightarrow^* D \Leftarrow^* C$ by the construction above, but it is not immediate that this process terminates, because each $\Rightarrow$ or $\Leftarrow$ may be replaced by multiple, so the sequence of $\Rightarrow$s and $\Leftarrow$s from $(M_1,s_1)$ to $(M_2,s_2)$ may get longer at some steps. However, termination can be proved by noting that the structure of this process is identical to the process of swapping $\sim_\uparrow$ and $\sim_\downarrow$ steps in \Cref{downAcrossUp}. To be more precise, in this case there is a sequence of $\Leftarrow$ and $\Rightarrow$ steps, each of which has an associated reduction position and initial and final terms, and if a $\Leftarrow$ immediately precedes a $\Rightarrow$, they may be swapped to produce some number of $\Rightarrow$s, followed by some number of $\Leftarrow$s. In the case of \Cref{downAcrossUp}, there is a sequence of $\sim_\uparrow$ and $\sim_\downarrow$ steps, each of which has an associated reduction position and initial and final skeletons, and if a $\sim_\uparrow$ immediately precedes a $\sim_\downarrow$, they may be swapped to produce some number of $\sim_\downarrow$s followed by some number of $\sim_\uparrow$s. In both cases, the number and reduction positions of the resultant steps are determined by the case of $\sim_c$ that matches the way the initial reduction positions overlap, and the initial skeleton (or the skeleton of the initial term). The same argument that the process in \Cref{downAcrossUp} terminated is therefore applicable here too. At every stage, the sequence of reductions that results from one of the initial $\Rightarrow$s is a parallel sequence, and swapping a parallel sequence of $\Rightarrow$s with a $\Leftarrow$ always terminates, therefore each $\Rightarrow$ in turn can be moved past all of the $\Leftarrow$s, and the process as a whole will terminate in a state where all of the $\Rightarrow$s precede all of the $\Leftarrow$s, i.e.~a pair of reduction sequences $(M_1,s_1) \Rightarrow^* (M',s') \Leftarrow^* (M_2,s_2)$.
\end{proof}

\begin{lemma} \label{simpDoesn'tMix}
If $A$ is some descendant of $M$ and $(A,\gamma) \sim^* (M,\delta)$, then $(A,\gamma) \sim_p^* (M,\delta)$, with the length of the reduction sequences decreasing by one each step from $A$ to $M$.
\end{lemma}
\begin{proof}
This is a simple induction on $\sim^*$. In the base case, $(A,\gamma) = (M,\delta)$ therefore $(A,\gamma) \sim_p^* (M,\delta)$ trivially. Otherwise, suppose that $(M,\delta) \sim_p^* (B,\epsilon) \sim (A,\gamma)$. Either $(B,\epsilon) \sim_p (A,\gamma)$ or $(B,\epsilon) \sim_c (A,\gamma)$. In the first case, either $B \to A$, in which case the result follows directly, or $A \to B$, in which case the fact that each (reduction sequence of) skeleton(s) has only one parent implies that a $(M,\delta) \sim_p^* (A,\gamma)$ directly as a subsequence of the path to $(B,\epsilon)$.

In the $\sim_c$ case, consider the definition of $\sim_c$. Either $O'_1 = A$ and $O'_2 = B$ or vice-versa. As $M \to^* N \to^* O_1 \to^* B$ and $(M,\delta) \sim_p^* (B,\epsilon)$, and $\sim_p$ only relates positions in a term and its parent, there are some positions $\zeta, \theta$ such that $(M,\delta) \sim_p^* (N,\zeta) \sim_p^* (O_1, \theta) \sim_p^* (B,\epsilon)$. It follows that $(O_2, \theta) \sim_p^* (A,\gamma)$ by following the same path, therefore it suffices to provide the only missing portion of the path from $M$ to $A$, i.e.~to prove that $(N,\zeta) \sim_p^* (O_2,\theta)$ given $(N,\zeta) \sim_p^* (O_1,\theta)$ (or vice-versa).

If $\zeta$ is disjoint from all the positions of reduction from $N$ to $O_1$ and $O_2$ (and consequently $\zeta = \theta$), this follows from case 1 of $\sim_p$. Otherwise, this can be proved by taking cases from the definition of $\sim_c$. This is rather long, but all of the cases are similar. The general idea is that the reductions from $N$ to $O_1$ correspond to the reductions from $N$ to $O_2$, so that if a position is related by $\sim_p$ across that reduction, it is related in the other branch for the same reason. Case d, where $B = O'_1$ rather than $O'_2$, is given here in more detail as an illustrative example:

Let $I$ be $N$ reduced at $\alpha$, and $J$ be $N$ reduced at $\alpha;@_1;\lambda;\beta$, so that $N \to I \to O_1$ and $N \to J \to O_2$. All of the reduction positions are $\geq \alpha$, and $\zeta$ is not disjoint from all of them, therefore $\zeta$ is not disjoint from $\alpha$. Let $\iota$ be the position such that $(N,\zeta) \sim_p (I,\iota) \sim_p (O_1,\theta)$. The fact that $(N,\zeta) \sim_p (I,\iota)$ implies that $\zeta > \alpha;@_1;\lambda$. Let $\zeta = \alpha;@_1;\lambda;\kappa$ and $\theta = \alpha;\kappa'$. If $\kappa$ is disjoint from $\beta$, then $(N,\zeta) \sim_p (J,\zeta) \sim_p (O_2,\alpha;\kappa') = (O_2;\theta)$ by cases 1 and 2 of $\sim_p$. In the other case, that $\kappa$ isn't disjoint from $\beta$, $\kappa > \beta$ because none of the positions $<= \alpha;@_1;\lambda;\beta$ in $N$ are related to any position in $I$ by $\sim_p$. As $(I,\alpha;\kappa) \sim_p (O_1,\alpha;\kappa')$ (with the redex at $\alpha;\beta$), for exactly the same reason $(N,\alpha;@_1;\lambda;\kappa) \sim_p (J,\alpha;@_1;\lambda;\kappa')$ (with the redex at $\alpha;@_1;\lambda;\beta$). Because $(N,\alpha;@_1;\lambda;\kappa) \sim_p (J,\alpha;@_1;\lambda;\kappa')$, $N|\alpha;@_1;\lambda;\kappa = J|\alpha;@_1;\lambda;\kappa'$, and $N|\alpha;@_1;\lambda;\kappa \neq $ the variable of $N|\alpha;@_1$, therefore $J|\alpha;@_1;\lambda;\kappa'$ is also not the variable therefore $(J,\alpha;@_1;\lambda;\kappa') \sim_p (O_2, \alpha;\kappa') = (O_2, \theta)$ by case 2 of $\sim_p$. Combining these results, $(N,\zeta) \sim_p^* (O_2;\theta)$ as desired.
\end{proof}

\begin{lemma} \label{lem:redexDestroyed}
If $M \to N$, with the redex at position $\alpha$, then no position in any term reachable from $N$ is related by $\sim^*$ to $(M,\alpha)$.
\end{lemma}
\begin{proof}
Suppose on the contrary that $(M,\alpha) \sim^* (A,\beta)$, where $N \to^* A$, then by \Cref{simpDoesn'tMix}, $(M,\alpha) \sim_p^* (A,\beta)$. $M \neq A$ therefore $(M,\alpha) \sim_p$ some position in $N$, but in all the cases of the definition of $\sim_p$, no position in the child term is related to the position of the redex.
\end{proof}

Essentially what \Cref{lem:redexDestroyed} demonstrates is that the samples taken during any reduction sequence are independent of each other. This is made more precise in the following lemmas.

\begin{lemma}
For any skeletons $M \to N$, with the redex at position $\alpha$, and measurable set of samples $S \subset I^{L_s(N)}$, $\mu(S) = \mu(\{s \in I^{L_s(M)} \mid s \circ i(M \to N) \in S\})$, and furthermore, if $M | \alpha = \tsample$, for any $S \subset I \times I^{L_s(N)}$, $\mu(S) = \mu(\{s \in I^{L_s(M)} \mid (s(M,\alpha), s \circ i(M \to N)) \in S\})$.
\end{lemma}
\begin{proof}
If the result holds for all sets $S$ of the form $\{s \in I^{L_s(N)} \mid \forall j : s(j) \in x_i\}$, where $(x_j)_{j \in J}$ is a family of measurable subsets of $I$ indexed by some finite set $J \subset L_s(N)$ of positions, it also holds for all other $S$ by taking limits and disjoint unions.

Take such a $(x_j)_{j \in J}$, where $x_j = x_k$ and define $K = i(M \to N)[J]$. Because $i(M \to N)$ is injective, it defines a bijection between $J$ and $K$. We can then calculate the measure $\mu(\{s \in I^{L_s(M)} \mid s \circ i(M \to N) \in S\}) = \mu(\{s \in I^{L_s(M)} \mid \forall k : s(k) \in x_{i(M \to N)^{-1}(k)}\}) = \prod_{k \in K} \mu_I(x_{i(M \to N)^{-1}(k)}) = \prod_{j \in J} \mu_I(x_j) = \mu(S)$.

In the case that $M|\alpha = \tsample$, we can similarly consider only those sets $S$ of the form $x_\tsample \times \{s \in I^{L_s(N)} \mid \forall j : s(j) \in x_i\}$. The position $\alpha$ in $M$ is not related to any position in $L_s(N)$, therefore $(M,\alpha) \not \in K$. Again, we can calculate the measure $\mu(\{s \in I^{L_s(M)} \mid (s(M,\alpha), s \circ i(M \to N)) \in S\}) = \mu(\{s \in I^{L_s(M)} \mid s(M,\alpha) \in x_\tsample, \forall k : s(k) \in x_{i(M \to N)^{-1}(k)}\}) = \mu_I(x_\tsample) \prod_{k \in K} \mu_I(x_{i(M \to N)^{-1}(k)}) = \mu_I(x_\tsample) \prod_{j \in J} \mu_I(x_j) = \mu(S)$.
\end{proof}

\begin{lemma} \label{lem:independentSamples}
For any initial term $M$, reduction strategy $f$ on $M$, natural number $n$, skeleton $N$ with $k$ holes, measurable set $T \subset \mathbb R ^ k$ and measurable set $S$ of samples in $I^{L_s(N)}$, $\mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in S : (M,s) \Rightarrow_f^n (N[r], s')\}) = \mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in I^{L_s(N)} : (M,s) \Rightarrow_f^n (N[r], s')\}) \mu(S)$
\end{lemma}
\begin{proof}
Suppose, to begin with, that $n = 0$. Either $\exists r \in T : N[r] = M$, in which case both sides of the equation are $\mu(S)$, or there is no such $r$, in which case both sides of the equation are 0.

For $n > 0$, suppose for induction that the lemma is true for $n - 1$, for all $N, T$ and $S$. If $\exists r \in T, s' \in S : (M,s) \Rightarrow_f^n (N[r], s')$, the $r$ and $s'$ are necessarily unique, therefore we may assume that $T$ is the product of $k$ measurable setsets of $\mathbb R$, $(T_j)_{0 \leq j < k}$, as all of the other cases follow by taking unions and limits of these rectangles. Let\begin{align*}
R_s & = \{\mathit{Sk}(O) \mid O \in Rch(M), r \in \mathbb R^k, O | f(O) = \tsample, O \to N[r]\} \\
R_d & = \{\mathit{Sk}(O) \mid O \in Rch(M), r \in \mathbb R^k, O | f(O) \neq \tsample, O \to N[r]\}.
\end{align*}
Together, $R_s$ and $R_d$ contain all of the skeletons of terms that can reduce to $N$, and they're disjoint. For each of these skeletons, there is a map $g_O$ from the reals in $O$ to the reals in $N$, i.e. $g_O(r) = r'$ if $O[r] \to N[r']$ for $O \in R_d$, and $g_O(s, r) = r'$ if $O[r] \to N[r']$ with $s$ as the value the sample takes in the reduction. All of these functions are measurable, as they are just rearrangements of vector components, or in the case that $O|f(O) = \underline x(\skeletonPlaceholder, \dots, \skeletonPlaceholder)$, a combination fo the measurable function $x$ with a rearrangement of vector components. In the case that $O \in R_s$, the function $g_O$ simply inserts the sample into the vector of reals at a certain index, call it $h_O$.
\clearpage
\begin{align*}
& \mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in S : (M,s) \Rightarrow_f^n (N[r], s')\}) \\
= & \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_s, r \in \mathbb R^{k-1}, s' \in I^{L_s(O)}, r' \in T, s'' \in S : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\})\\
    + & \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_d, r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}, r' \in T, s'' \in S : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \mathbb R^{k-1}, s' \in I^{L_s(O)}:\\
    & g_O(r,s'(O,f(O))) \in T, s' \circ i(O[r] \to N[g_O(r,s'(O,f(O)))]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\})\\
     + & \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}:\\
     & g_O(r) \in T, s' \circ i(O[r] \to N[g_O(r)]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)}:\\
    & s'(O,f(O)) \in T_{h(O)}, s' \circ i(O[r] \to N[g_O(r,s'(O,f(O)))]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\})\\
    + & \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}:\\
    & g_O(r) \in T, s' \circ i(O[r] \to N[g_O(r)]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)} : (M,s) \Rightarrow_f^{n-1} (O[r], s')\})\\
    & \mu(\{s' \in I^{L_s(O)} \mid s'(O,f(O)) \in T_{h(O)}, s' \circ i(O[r] \to N[g_O(r,s'(O,f(O)))]) \in S\})\\
    + & \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\})\\
    & \mu({s' \in I^{L_s(O)} \mid s' \circ i(O[r] \to N[g_O(r)]) \in S}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)} : (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(T_{h(O)}) \mu(S)\\
    + & \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)} : s'(O,f(O)) \in T_{h(O)}, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S)\\
    + & \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \mathbb R^{k-1}, s' \in I^{L_s(O)} : g_O(r,s'(O,f(O))) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S)\\
    + & \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) \\
= & \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_s, r \in \mathbb R^{k-1}, s' \in I^{L_s(O)}, r' \in T, s'' \in I^{L_s(N)} : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) \mu(S)\\
    + & \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_d, r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}, r' \in T, s'' \in I^{L_s(N)} : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) \mu(S) \\
= & \mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in I^{L_s(N)} : (M,s) \Rightarrow_f^n (N[r], s')\}) \mu(S)
\end{align*}
\end{proof}
\clearpage

\AstEquivalence*
\begin{proof}
For both $\red$ and $\Rightarrow_{\cbv}$, the probability measure on the sample space can be used to define a measure on the reduction sequences of each finite length. In the case that the reduction sequences terminate after a finite number of steps by reaching a value, the reduction sequence defined using $\Rightarrow_{\cbv}$ should be taken to continue by repeating the last term indefinitely, as $\red$ does, even though $\Rightarrow_{\cbv}$ specifies no next step in this case. The distributions are equal for each length, by induction, as follows.

The base case, length 0, is trivial, because in both cases the distribution has probability 1 on the sequence containing only $M$.

For the inductive case, suppose that the distributions of reduction sequences of length $n$ are equal.

For any term $N$ which is not a value, there is a unique environment $E$ and redex $R$ such that $N = E[R]$. The position of the hole in $E$ is equal to $\cbv(N)$, so that $N|\cbv(N) = R$, as the cases in the definition of $\cbv$ match the cases in the definition of environments. If $R \neq \tsample$, let $R'$ be the result of reducing $R$ (which is equal in both versions of the semantics), then $E[R'] = N[R' / \cbv(N)]$ therefore $\red(N,s) = (E[R'],s)$ and $(N,s) \Rightarrow_{\cbv} (E[R'],s)$. The distribution of next terms, conditional on the previous term, in the case that that previous term reduces deterministically, is therefore equal for $\red$ and $\Rightarrow_{\cbv}$.

In the other case that $R = \tsample$, the first (term) part of $\red(N,s)$ is $E[\underline{\pi_h(s)}]$. If $k$ is the number of samples already taken in this reduction sequence, this is equivalent to $E[\underline{s_0(k)}]$, where $s_0$ is the initial sample. For any sufficiently small neighbourhood of $N$ (i.e.~containing only reduction sequences with the same skeletons), the probability of reaching this neighbourhood is independent of $s(k)$, because it depends only on the samples taken so far, and the samples are independent. The distribution of $s(k)$ conditional on reaching $N$ is therefore the uniform distribution on $I$. Similarly for the other semantics, if the distribution of remaining samples after $n$ steps is independent of the term by \Cref{lem:independentSamples}, therefore the distribution of $s(\mathit{Sk}(N),\cbv(N))$ conditional on $N$ is the uniform distribution on $I$. In either case, the distribution of the next term conditional on the previous term, in the case that it reduces randomly, is equal to the image under $r \mapsto E[\underline r]$ of the uniform distribution on $I$.

In any case, the distribution of reduction sequences after $n+1$ steps, conditional on the reduction sequence after $n$ steps, as defined by $\Rightarrow_{\cbv}$ and by $\red$, is equal, any by the inductive hypothesis the distributions after $n$ steps are equal, therefore by integrating over the reduction sequence up to step $n$, the distributions on reduction sequences up to step $n+1$ are equal.

The distributions on reduction sequences of any finite length as defined by $\Rightarrow_{\cbv}$ and $\red$ are therefore equal, therefore so is the probability of having reached a value after $n$ steps, therefore so is its limit, the probability of termination, therefore the probability of termination is 1 iff the probability of termination with respect to $\cbv$ is 1.
\end{proof}

\CbvIsTerminatingest*
\begin{proof}
  Suppose $M$ terminates with the reduction strategy $f$ and samples $s$. Let $(M,s) = (M_0,s_0) \Rightarrow_f \dots \Rightarrow_f (M_n,s_n)$, where $M_n$ is a value. By \Cref{canonicalCousins}, the reduction sequence $(\mathit{Sk}(M_i))_i$ is related by $\sim_c^*$ to some skeletal reduction sequence $X$ which is in call-by-value order. For each skeletal reduction sequence $X_j$ in the sequence $(\mathit{Sk}(M_i))_i \sim_c \dots \sim_c X$, it is possible to define a reduction sequence of terms such that $(M,s) = (N_{j,0},s_{j,0}) \Rightarrow \dots \Rightarrow (N_{j,n_j},s_{j,n_j})$ and $\mathit{Sk}(N_{j,k}) = X_{j,k}$, by at each step applying the reduction at the same place as in $X_j$, essentially just filling in the reals in the skeletal reduction sequence to make it a reduction sequence of terms (it will be shown shortly that the correct branch is taken in each $\textsf{if}$-reduction). Assume for induction on $j$ that $(N_{j,n_j},s_{j,n_j}) = (M_n,s_n)$. To prove that this holds true for $j+1$ as well, it suffices to show that the corresponding term and samples immediately after the reductions involved in this $\sim_c$ step match, as from that point on, the $\Rightarrow$-sequences will match, as they start from the same point and have all the same reduction positions. Letting the term and remaining samples immediately before the reductions involved in this $\sim_c$ step be $(N,t)$ (which is the same in both $N_j$ and $N_{j+1}$ for the same reason again, they start at the same point and the sequences of reduction positions up to this point are the same), and the terms and samples after these reductions be $(O_1,t_1)$ and $(O_2,t_2)$, so that these match the $N$, $O_1$ and $O_2$ in the definition of $\sim_c$, $t_1 = t_2$ because both of them are, by \Cref{lem:sim-M-N}, equal to the restriction of $t$ to the subset of $Rch(N)$ that starts with the reduction sequences to $O_1$ and $O_2$ respectively, and the potential positions in those subsets of $Rch(N)$ are identified with each other by $\sim$ because of the same case of $\sim_c$ that means $N_j$ and $N_{j+1}$ are related. If none of the relevant reductions are $\tsample$-reductions, $O_1 = O_2$ trivially. If there are samples taken (which must be the arbitrary reduction at $\beta$), it is also required that the same samples are taken in both branches. Similarly to \Cref{churchRosser}, the positions of these $\tsample$-reductions in each branch are related by $\sim_p^*$, therefore they correspond to the same sample in $t$, therefore $O_1 = O_2$. The fact that the terms are equal also implies that the same branch of an $\textsf{if}$ statements will be taken.

  \paragraph{}
  There is therefore a reduction sequence $(M,s) = (N_0,s_0') \Rightarrow \dots \Rightarrow (N_m, s_m')$ which ends in a value ($N_m = M_n$) and is in CBV order. It doesn't immediately follow that this is the $\Rightarrow_{\cbv}$ reduction sequence starting from $(M,s)$ because the fact that $(N_i)_i$ is in CBV order only means that the reductions that do take place in this reduction sequence are in the correct order, not that the reduction sequence finishes once it reaches a value, or that there are no other reductions that would come earlier in CBV order that don't occur at all in $(N_i)_i$. Suppose that at some point $j$ in the reduction sequence, the reduction that occurs is not the next reduction in CBV order, but something else, and that the reduction that should be next is at position $\alpha = \cbv(N_i)$, and the position of the actual reduction $N_j \to N_{j+1}$ is $\beta$. Suppose further, for a contradiction, that $N_j$ is not a value. Either $\alpha = \cdot$, the root position, or the reduction at the root position is dependent in some way on the reduction at $\alpha$ (e.g. $\alpha = \textsf{if}_1$, or $\alpha = \underline{+};@_1$). In any case, all the reductions in the rest of the reduction sequence are after $\alpha$ in CBV order, therefore none of them are $\leq \alpha$, and there continues to be an unreduced redex at $\alpha$, therefore the reduction at the root position never occurs, therefore $N_m$ is not a value, which is a contradiction, therefore at the point (if any) where $(N_i)_i$ departs from the $\Rightarrow_{\cbv}$ reduction sequence starting at $(M,s)$, it has already reached a value, therefore $(M,s) \Rightarrow_{\cbv} \dots$ does reach a value eventually, i.e.~$M$ terminates with the reduction strategy $\cbv$ and samples $s$.
\end{proof}

\confluentNoStronger*
\begin{proof}
Suppose that $(M,s) \Rightarrow^*_{\cbv} (F,s')$ for some trace $s$ and term $F$ which is not AST. Let $P$ be the finite set of potential positions in $M$ such that the corresponding sample is used in the reductions $(M,s) \Rightarrow^*_{\cbv} (F,s')$. As $F$ is not AST, the set $T_0 = \{t \in I^{L_s(M) \setminus P} \mid M \text{ terminates with } (t,s|_P) \text{ and } \cbv \}$ has non-zero measure.

Take some order $p_1, \dots$ on the elements of $P$, and define $T_n$ recursively as follows. If the sample at $p_n$ is eventually used in the sequence $(M,(t,s|_P)) \Rightarrow^*_r \dots$ for some positive measure subset of $T_{n-1}$, then let $T_n$ be the subset of $T_{n-1}$ where it is used within $m$ steps, for the minimal $m$ such that this is of positive measure. Otherwise, let $T_n$ be the subset of $T_{n-1}$ where the sample at $p_n$ is never used. Let $T$ be the final such $T_n$. Also, let $P_i$ be the set of $p_n$ such that the second case was taken in the definition of $T_n$ i.e.~the sample at $p_n$ is never used in the reduction sequences starting at $(M,(t,s|_P))$ with strategy $r$, and let $k$ be the maximum value of $m$.

Let $T' = T \times \prod_{p \in P_i} I \times \prod{p \in P \setminus P_i} \{s(p)\}$. Although $T'$ itself may have a measure of 0 as a subset of $I^{L_s(M)}$, it still has a natural measure of its own. It also has the properties that $M$ does not terminate with $\cbv$ and any trace in $T'$ (therefore $M$ also does not terminate with $r$ and any trace in $T'$, by \Cref{thm:CbvIsTerminatingest}), and for any trace $t \in T'$, if $(M,t) \Rightarrow^k_r (M',t')$, none of the potential positions in $M'$ correspond to any of the elements of $P \setminus P_i$. There are finitely many skeletal reduction sequences $k$ steps long starting at $M$ and following $r$, each one corresponding to some measurable subset of $I^{L_s(M)}$, so pick one such that its intersection with $T'$ is a non-null subset of $T'$, let this subset be $Q$.

Pick some $p \in Q$, and let $(M,p) \Rightarrow^k_r (N,p')$. The injection $i(M \to^k N)$ is the same as it would be for any other element of $Q$, and its image is disjoint from $P \setminus P_i$. The inverse image under composition with $i(M \to^k N)$ of $Q$ is therefore a non-null subset of $I^{L_s(N)}$, and $N$ fails to terminate with $r$ and every trace in this set, therefore $N$ is not AST with respect to $r$, therefore $N$ is not AST, but $N \in Rch_r(M)$, therefore not every element of $Rch_r(M)$ is AST.

Taking the contrapositive of this, we have the desired result.
\end{proof}

\Cref{ex:sum of powers}
Recall that
\begin{align*}
M =&\ (\lambda n. P[n]) \lfloor \tsample^{-1/2} \rfloor \\
P[n] =&\ (\lambda p. \Xi[p] n) (\lambda x. \Theta[x] n)) \\
\Xi[p] =&\ \tY \lambda f n. \tif{n = 0}{\underline 0}{p n + f (n-1)} \\
\Theta[x] =&\ \tY \lambda f n. \tif{n = 0}{\underline 1}{x \times f (n-1)}.
\end{align*}

Let $r$ be the reduction strategy which reduces $M$ in the order
\begin{align*}
&\ M \\
\to^* &\ P[\underline n] \\
\to^* &\ (\lambda p.\ \Xi[p]\ \underline n)\ (\lambda x. \underbrace{x \times \dots \times x \times}_n \underline 1) \\
\to^{\phantom *} &\ \Xi[\lambda x. \underbrace{x \times \dots \times x \times}_n \underline 1]\ \underline n \\
\to^{\phantom *} &\ \tif{\underline n = 0}{\underline 0}{\underline{n^n} + \Xi[\lambda x. x \times \dots \underline 1]\ \underline{n-1}} \\
\to^* &\ \underline{n^n} + \underline{(n-1)^n} + \dots + \underline 0 \\
\to^* &\ \underline{\sum_{k=0}^n k^n}
\end{align*}
(this is a little underspecified, but the details are not important). A suitable sparse ranking function for it is
\begin{align*}
M \mapsto & \frac{\pi^2}{3} \\
P[\underline n] \mapsto & 2n+2 \\
(\lambda p.\ \Xi[p]\ \underline n)\ (\lambda x. \underbrace{x \times \dots \times x \times}_{n-k} (\Theta[x]\ \underline k)) \mapsto & n + k + 2 \\
\Xi[\lambda x. \underbrace{x \times \dots \times x \times}_n \underline 1]\ \underline n \mapsto & n + 1 \\
\underline{n^n} + \dots + \underline{(n-k+1)^n} + \Xi[\lambda x. x \times \dots \underline 1]\ \underline k \mapsto & k + 1 \\
\underline{\sum_{k=0}^n k^n} \mapsto & 0.
\end{align*}

The first step is justified because the probability of reaching $P[\underline n]$ for any specific $n \geq 0$ is $(n+1)^{-2} - (n+2)^{-2} = \frac{2n + 3}{(n+1)^2(n+2)^2}$, therefore the expected next value of the ranking function is $\sum_{n=0}^{\infty} \frac{(2n+2)(2n + 3)}{(n+1)^2(n+2)^2} = \frac{\pi^2}{3}$. The other steps are deterministic, and involve 1 or 0 $\tY$-reductions each. Given that all of the reduction steps after the first are deterministic, and the number of $\tY$ reduction steps is not that hard to count, defining the sparse ranking function's values only at $M$ and $P[\underline n]$ would also have been reasonable, although in a similar term which had more random samples throughout, that would not have been so simple.

Providing a ranking function of some sort for this term in a similar level of detail would have been rather more complicated using only the standard reduction strategy. The recursion in $\Theta$ would have had to be evaluated separately for every time it was used, so there would have been more (and more complex) terms to consider in the reduction sequence. Also, the number of $\tY$-reductions from $P[\underline n]$ would have been $n^2 + 2n + 1$ instead of $2n + 2$, therefore the expected number of $\tY$ reductions starting from $M$ would be infinite, i.e.~$M$ is not $\tY$-PAST, therefore it is not even rankable. It would be possible to define an antitone sparse ranking function for it, but it would have to include terms such as
\begin{align*}
& \underline{n^n} + \dots + \underline{(n-k_1+1)^n} + \\
& (\underbrace{\underline{n-k_1} \times \dots \times \underline{n-k_1} \times}_{n-k_2-1} (\Theta[\underline{n-k_1}] (\underline{k_2} - 1))) + \\
& \Xi[\lambda x. \Theta[x] \underline n] (\underline{n-k_1} - 1) \mapsto \log((n+1)k_1 + k_2)
\end{align*}
as only one of the several cases, which is significantly more complicated and difficult to use.

It wouldn't even be possible in this case to give a sparser version of the ranking function defined only at $M$ and $P[\underline n]$, because the amount that an antitone sparse ranking function must decrease at each $\tY$-reduction step depends on the value of the ranking function at the next term where it is defined, therefore it is necessary to have these intermediate steps in order for the ranking function to be able to change sufficiently slowly.
