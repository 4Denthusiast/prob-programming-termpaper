% !TEX root = main.tex
\section{Antitone ranking functions}
\label{sec:antitone}

\begin{example}[Random walk]
\label{ex:ac-ranking}
\begin{enumerate}
\item 1D biased (towards 0) random walk
\[
M_1 = 
\big(\tY \, \lambda f n . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{3/2} f \, (n + 1)}\big) \, 10
\]

\item 1D unbiased random walk
\[
M_2 = 
\big(\tY \, \lambda f n . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{1/2} f \, (n + 1)}\big) \, 10
\]

\item 1D biased (away from 0) random walk
\[
M_3 = 
\big(\tY \, \lambda f n . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{1/3} f \, (n + 2)}\big) \, 10
\]


%While {x > 0} do {x := x-1 \oplus_2/3 x:= x+1}

%M2 = While {x > 0} do {x := x-1 \oplus_2/3 x:= x+2}
\end{enumerate}
\end{example}

The term $M_1$ is rankable, terminating in 31 $\tY$-steps on average, and $M_3$ only has a $1/1024$ chance of terminating, but in between, $M_2$ is AST, but isn't $\tY$-PAST, therefore it isn't rankable and \Cref{thm:rankable implies termination} is insufficient to prove its termination. We therefore want to find a generalised notion of ranking function so that $M_2$ becomes rankable, and then prove it sound i.e.~rankable in this generalised sense implies AST.

%\paragraph*{Idea} 

\begin{definition}
The definition of \emph{antitone ranking function} $f$ for $M \in \Lambda^0$ is the same as that of ranking function except that in the case of $\tY$-redex, 
we require the existence of an antitone (meaning: $r < r'$ implies $\epsilon(r) \geq \epsilon(r')$) function $\epsilon : \nnReal \to \pReal$ such that the ranking function $f :\mathit{Rch}(M) \to \nnReal$ satisfies
\[f(E[R']) \leq f(E[R]) - \epsilon(f(E[R])) \] 
where $R \to R'$ is the $\tY$-redex rule.
Any closed term for which an antitone ranking function exists is called \emph{antitone rankable}.
\iffalse
Given a ranking function $f$ on $M \in \Lambda^0$, we say that $f$ is \emph{antitone} if there exists an antitone function\footnote{i.e.~$r < r'$ implies $\epsilon(r) \geq \epsilon(r')$} $\epsilon : \nnReal \to \pReal$ such that %the ranking function $f$ satisfies
\[
f(E[R']) \leq f(E[R]) - \epsilon(f(E[R])) 
\]
for all $E[R'] \in \mathit{Rch}(M)$ where $R \to R'$ is the $\tY$-redex rule.
\fi
\end{definition}

Note that antitone ranking functions are actually a generalisation of ranking functions, even though the way we reference them may suggest that they are a type of ranking functions.

\iffalse
\lo{An aside: This CBV $\tY$-rule seems cleaner:
\[
\big(\tY f^{A \to B} \, x^A \, . \, \theta^B \big) \, v \to \theta[(\tY f \, x \, . \, \theta) / f, v / x].
\]
We assume $\tY f^{A \to B} \, x^A \, . \, \theta^B$ is a value.}
\fi

\begin{definition}
Given a probability space $(\Omega, \calF, \mathbb{P})$, and a supermartingale $(Y_n)_{n \geq 0}$ and a stopping time $T$ adapted to filtration $(\calF_n)_{n \geq 0}$,
we say that $(Y_n)_{n \geq 0}$ is an \emph{antitone strict supermartingale w.r.t.~$T$} if for all $n \geq 0$, we have $Y_n \geq 0$, and there exists an antitone function $\epsilon : \nnReal \to \pReal$ satisfying
\(
\expect{Y_{n+1} \mid \calF_n} \leq Y_n - \epsilon (Y_n) \cdot {\bf 1}_{\set{T > n}}
\).
%Any closed term for which an antitone ranking function exists is called \emph{antitone rankable}.
\end{definition}

%\lo{The preceding definition does not place any constraint on the stopping time $T$ (other than that it is w.r.t.~the filtration $(\calF_n)_{n \geq 0}$). However for the definition to make sense, $T$ must be the stopping time $T(s) := \min \set{n \mid Y_n(s) = 0}$.}
%\akr{The definition you gave for an antitone ranking function is incorrect, because the antitone ranking functions are a proper subset of ranking functions. Although we may have been referring to antitone ranking functions as a type of ranking function, they're a generalisation, so they don't actually meet the definition of a ranking function in general.}

\begin{theorem}
\label{thm:a-c strict}
Let $(Y_n)_{n \geq 0}$ be an antitone strict supermartingale w.r.t.~stopping time $T$. 
Then $T < \infty$ a.s.
\end{theorem}

\begin{proof}
First, as $(Y_n)$ is a supermartingale, $\expect{Y_n} \leq \expect{Y_0}$. 
Therefore
\begin{calculation}
\expect{Y_n \mid T > n}
   \step[=]{rearranging terms}
% \dfrac{\mathbb P[T > n] \, \expect{Y_n \mid T > n} 
% + 
% \mathbb P[T \leq n] \, \expect{Y_n \mid T \leq n} 
% - 
% \mathbb P[T \leq n] \, \expect{Y_n \mid T \leq n}}{\mathbb P[T > n]}
\dfrac{
\begin{array}{l}
\mathbb P[T > n] \, \expect{Y_n \mid T > n} 
+ 
\mathbb P[T \leq n] \, \expect{Y_n \mid T \leq n} 
\\
{} - 
\mathbb P[T \leq n] \, \expect{Y_n \mid T \leq n}
\end{array}
}{\mathbb P[T > n]}
  \step[=]{definition of conditional expectation}
\dfrac{\expect{Y_n} - \mathbb P[T \leq n] \, \expect{Y_n \mid T \leq n}}{\mathbb P[T > n]}
  \step[\leq]{}
\dfrac{\expect{Y_n}}{\mathbb P[T > n]}
%  \step[\leq]{}
%\dfrac{\expect{Y_0}}{\mathbb P[T > n]}
\mathbin{=} \dfrac{\expect{Y_0}}{\mathbb P[T > n]}
\end{calculation}

\emph{Claim}: For all $0 < x \leq 1$, 
\(
\mathbb P[T > B_x] \leq x,
\)
where $B_x = \left \lceil \dfrac{\expect{Y_0}+1}{x \; \epsilon(\expect{Y_0} \, x^{-1})} \right \rceil$ and $\epsilon : \nnReal \to \pReal$ is the antitone function. 

\smallskip

As the convex hull of $\epsilon$ (the greatest convex function less than or equal to it) satisfies all the conditions assumed of $\epsilon$, in addition to being convex, assume wlog that $\epsilon$ is convex.

Assume for a contradiction that $\mathbb P[T > B_x] > x$.
Then, take $n \leq B_x$. 
We have
%\lo{Change of justification of the second equality below: event, not discrete r.v. Recall: expectation conditioning on r.v.~(resp.~event) is a r.v.~(resp.~number).}
\begin{calculation}
\expect{Y_n-Y_{n+1}}
%  \step[=]{}
\mathbin{=}
\expect{Y_n - \expect{Y_{n+1} \mid \calF_n}}
  \step[\geq]{}
\expect{\epsilon(Y_n) \cdot {\bf 1}_{\set{T > n}}}
%  \step[=]{definition of conditional expectation for a discrete variable}
\step[=]{}
\mathbb P[T > n]\ \expect{\epsilon(Y_n) \mid T > n}
  \step[\geq]{Jensen's inequality}
\mathbb P[T > n]\ \epsilon(\expect{Y_n \mid T > n})
  \step[\geq]{proved earlier}
\mathbb P[T > n]\ \epsilon\Big(\dfrac{\expect{Y_0}}{\mathbb P[T > n]}\Big)
  \step[>]{assumption, $\mathbb P[T > n] \geq \mathbb P[T > B_x] > x$}
x \; \epsilon\big(\expect{Y_0} \, x^{-1}\big).
\end{calculation}
Therefore, by a telescoping sum
\begin{align*}
\expect{Y_{B_x}} 
&= \expect{Y_{B_x}-Y_0+Y_0} \leq \expect{Y_0} - B_x \; x \; \epsilon(\expect{Y_0} \, x^{-1})\\
&\leq -1 < 0 
\end{align*}
%\lo{$\expect{Y_0 - B_x \; x \; \epsilon(\expect{Y_0} \, x^{-1})}$}
which is a contradiction, therefore the claim must be true, therefore $P[T > n]$ tends to 0 as $n$ tends to infinity, therefore $T < \infty$ almost surely.
\end{proof}

Note that it is essential in this proof that $\epsilon$ be defined for all $\mathbb R_{\geq 0}$, not just the values that $(Y_n)_{n \geq 0}$ actually takes (or at least if $(Y_n)_{n \geq 0}$ is uniformly bounded, $\epsilon$ must be positive at the supremum as well as the actual values). 

\begin{example}
Take the probability space $(\Omega, \calF, \mathbb{P})$ where $\Omega$ is the closed interval of reals $[-1, 1]$, $\calF$ the Borel $\sigma$-algebra, and $\mathbb{P}$ the corresponding Lebesgue probability measure.
Let $\omega \in [-1, 1]$. Define random variables $T$ and $(Y_k)_{k \in \omega}$:
\begin{align*}
T(\omega) & := 
\begin{cases}
\min\set{n \in \mathbb{N} \mid \omega > 2^{-n}} & \hbox{if $\omega \in (0, 1]$}\\
\infty & \hbox{otherwise}
\end{cases} 
\\
Y_k(\omega) & := 
\begin{cases}
4-2^{-k} & \hbox{if $\omega \in [-1, 2^{-k}]$ (equivalently $k < T(\omega)$)}\\
0 & \hbox{otherwise}
\end{cases} 
\end{align*}
Plainly, $T$ is a stopping time, and $(Y_n)_{n \geq 0}$ is a supermartingale, adapted to the filtration $(\calF_n)_{n \geq 0}$ where $\calF_n = \calF$ for all $n$. 
In this case, $(Y_n)_{n \geq 0}$ either tends to $4$ as $n \to \infty$, or drops to 0 at some point, with an exponentially decreasing probability. 
With respect to the antitone function $\epsilon(x) = \frac{4-x}{4}$ and stopping time $T$, we have that $(Y_n)_{n \geq 0}$ is an antitone strict supermartingale, except that $\epsilon(4) = 0$, and $T = \infty$ with probability $\frac 1 2$, even though $4$ is larger than any value $(Y_n)_{n \geq 0}$ can actually take, and $\epsilon(Y_n)$ never actually reaches $0$.
%\lo{@Andrew: I refer to the preceding highlighted claim. Did you mean: for all $n, s$, $\epsilon(Y_n(s)) > Y_n(s)$?} \akr{No. This phrasing should be clearer.}
\end{example}

%\changed[lo]{Let $f$ be a ranking function on a closed PPCF term $M$. We say that $M$ is \emph{antitone rankable} by $f$ if the supermartingale $(f(M_n))_{n \geq 0}$ and the stopping time $T_M^{\tY}$, both adapted to the fitration $(\calF_n)_{n \geq 0}$ where $\calF_n := \sigma(M_1, \ldots, M_n)$, are antitone strict.}

\begin{theorem}[Antitone ranking function soundness] \label{thm:antitone rankable implies termination}
If a closed PPCF term $M$ is antitone rankable, then $T_M^{\tY} < \infty$ a.s.~(equivalently, $M$ is AST).
\end{theorem}
\begin{proof}
As in %Theorem \ref{thm:basicRankingSoundness}, 
\Cref{thm:rankable implies termination},
take the probability space $(\entrosp, \Sigma_\entrosp, \mu_\entrosp)$, and define the same random variables $T_n, X_n, Y_n, T_M^{\tY}$. 
Thanks to \Cref{thm:rankable and strict rankable}, $(Y_n)_{n \geq 0}$ is a supermartingale; and because $M$ is now assumed to be antitone rankable, it is an antitone strict supermartingale w.r.t.~stopping time $T_M^{\tY}$. 
Thus, by \Cref{thm:a-c strict}, $T_M^{\tY} < \infty$ a.s.
\end{proof}

%\paragraph{}
As before, constructing antitone ranking fuctions completely is not necessary, and there is a corresponding notion of an antitone sparse ranking function.

Define an \emph{antitone sparse ranking function} on a closed term $M$ to be a partial function $F : Rch(M) \rightharpoonup \mathbb{R}$ such that for some antitone function $\epsilon : \mathbb{R}_{\geq 0} \to \mathbb{R}_{>0}$
\begin{itemize}
\item $f(N) \geq 0$ for all $N$ where $f$ is defined.
\item $f$ is defined at $M$.
\item For any $N$ in the domain of definition of $f$, evaluation of $N$ will eventually reach some $O$ which is either a value or in the domain of definition of $f$, and $f(N) \geq \mathbb E [f(O) + \epsilon(f(O)) \times \text{the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}

\begin{restatable}{theorem}{AntitonePartial}  \label{thm:antitone partial implies rankable}
  Every antitone sparse ranking function is a restriction of an antitone ranking function.
\end{restatable}

As a corollary, any term which admits an antitone sparse ranking function terminates almost surely.

\paragraph*{\Cref{ex:ac-ranking} revisited}
Programs $M_1$ and $M_2$ are AST. For $M_1$, the sparse ranking function 
\begin{align*}
\big(\tY \, \lambda f n . \, \tif{n=0}{0}{f \, (n-1) \oplus_{3/2} f \, (n+1)}\big) \, \underline x \mapsto 3x + 1
\end{align*}
suffices to prove its termination (and could equivalently be considered an antitone sparse ranking function with the constant antitone function $\epsilon_1(x) = 1$).

\begin{example}[Unbiased random walk]
\label{ex:unbiased random walk}
For $M_2$, define the function $g_1 : \nnReal \to \nnReal$ by $g_1(x) = \ln(x+1) + 1$.
Using shorthand 
$\Theta_2 = \tY \, \lambda f n . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{1/2} f \, (n + 1)}$, 
we can define an antitone sparse ranking function $f_2 : \mathit{Rch}(M_2) \rightharpoonup \nnReal$ 
%with antitone function $\epsilon_2$ defined as follows (for $n \in \mathbb N$):
by
\[
f: {\Theta_2} \, n 
\mapsto 
g_2(n), \qquad 
0 \mapsto 0
\]
for $n \in \mathbb N$.
% \begin{align*}
% {\Theta_2} \, n 
% &\mapsto 
% g_2(n)
% \\
% 0 &\mapsto 0.
% \end{align*}
For $n \geq 1$, $\Theta_2 \, n$ reduces in several steps to either $\Theta_2 \, (n-1)$ or $\Theta_2 \, (n+1)$,\footnote{Technically this is not quite correct, because of the distinction between $\underline n + 1$ and $\underline{n + 1}$, but this ranking function can still be made to work by appealing to \Cref{thm:confluent ranking}, as can the others in this section with similar issues.} each with probability $1/2$, with one $\tY$-reduction.
Now
\begin{align*}%\textstyle
& g_2(n) - \frac{g_2(n-1) + g_2(n+1)} 2 \\
  &=  \ln \Big(\frac{n+1}{\sqrt{n(n+2)}}\Big) 
  =  \frac 1 2 \ln\left(\frac{(n+1)^2}{n(n+2)}\right) \\
  &=  \frac 1 2 \ln\left(1 + \frac 1 {n(n+2)}\right) 
  >  \frac 1 {n(n+2) + 1} \\
  &>  \frac 1 2 \frac 1 {(n+1)^2} + \frac 1 2 \frac 1 {(n+3)^2}
\end{align*}
Moreover $\Theta_2 \, 0$ reduces to $0$ with one $\tY$-reduction with a reduction in $g_2$ of $1$.
Therefore by setting $\epsilon_2(x) = \frac 1 {(e^{x-1}+1)^2}$, the condition on how much $g_2$ must decrease is met. It is also defined at $M_2 = \Theta_2 \, 10$, and is non-negative, therefore it is an antitone sparse ranking function, and $M_2$ is AST.
\end{example}

All of the following examples also have antitone ranking functions, provided in Appendix \ref{apx:antitone}.


Sampling from continuous distributions is an essential feature of statistical probabilistic programming languages. (See e.g.~Church \cite{DBLP:conf/uai/GoodmanMRBT08}, Stan \cite{carpenter2017stan}, Anglican \cite{DBLP:conf/pkdd/TolpinMW15}, Gen \cite{cusumano-towner2019Gen}, Pyro \cite{bingham2019Pyro}, Edward \cite{tran2016edward} and Turing \cite{ge2018Turing}.)
Methods of proving AST of probabilistic computation have been developed for probabilistic programs with discrete distributions (see e.g.~\cite{DBLP:journals/toplas/LagoG19,DBLP:journals/jacm/KaminskiKMO18,DBLP:conf/lics/OlmedoKKM16,DBLP:conf/lics/KobayashiLG19,DBLP:conf/mfcs/KaminskiK15,DBLP:series/mcs/McIverM05}).
To our knowledge, the problem of proving AST of probabilistic programs with continuous distribution is new.

\begin{example}[Continuous random walk]\label{ex:raven complex}
In PPCF we can construct a function whose argument changes by a random amount at each recursive call: $\Theta \, 10$ where
\[
%\underbrace{\big
\Theta := \tY \, \lambda f x . \tif{x \leq 0}{0}{f(x - \tsample)}
%}_{\Theta} \, 10
\]

For a more complex (not $\tY$-PAST) example, consider the following ``continuous random walk'': $\Xi \, 10$ where
\[
%\underbrace{\big(
\Xi := \tY \, f \, x . \tif{x \leq 0}{0}{f(x - \tsample + 1/2)} 
%\big)}_{\Xi} \, 10
\]
\end{example}

\begin{example}[Fair-in-the-limit random walk]
\label{ex:Fair-in-the-limit random walk}\cite[\S 5.3]{DBLP:journals/pacmpl/McIverMKK18}
\[
\big
(\tY \, f \, x . 
\tif{x \leq 0}{0}{f(x - 1) \oplus_{\frac{x}{2x+1}} f(x + 1)} \big)
\, 10
\]
\end{example}

\begin{example}[Escaping spline]
\label{ex:escaping spline}\cite[\S 5.4]{DBLP:journals/pacmpl/McIverMKK18}
\[
%\underbrace{
\big(\tY \, f \, x . \,
0 \oplus_{\frac{1}{x+1}} f(x + 1) 
\big)
%}_{\Xi} 
\, 10
\]
\end{example}

\begin{example}[Non-affine recursion] \label{ex:non-affine recursion}
\[
(\tY \lambda f x. x \oplus_{2/3} f (f (x + 1)))\ \underline 1
\]
\end{example}


\begin{example}[Higher-order recursion] \label{ex:higher-order recursion}
Consider the higher-order function
$\Xi : (\textsf{R} \to \textsf{R} \to \textsf{R}) \to \textsf{R} \to \textsf{R} \to \textsf{R}$
recursively defined\footnote{Inspired by examples in \cite{DBLP:journals/pacmpl/BurnOR18,DBLP:conf/lics/OngW19}.} by
\[
\begin{array}{l}
\Xi := \tY \lambda \varphi \, f^{\textsf{R} \to \textsf{R} \to \textsf{R}} \, s^\textsf{R} \, n^\textsf{R} . \\
\qquad\tif{n \leq 0}{s}{
f \, n \, (\varphi \, f \, s \, (n - 1))
\oplus_p
f \, n \, (\varphi \, f \, s \, (n + 1))
}
\end{array}
\]
For any $F : \textsf{R} \to \textsf{R} \to \textsf{R}$ such that $F\ \underline n\ \underline m$ terminates with no $\tY$ reductions for all $m$ and $n$, $\Xi\ F\ \underline x\ \underline y$ is antitone rankable.
\end{example}

\paragraph*{Completeness}
It seems very likely that this method is complete in the case that every reachable term is AST, but we have been unable to actually prove this. It is certainly at least capable of proving the termination of terms which terminate arbitrarily slowly (in the sense that there is no similar limitation to Theorem \ref{thm:rankable implies termination}, which can only prove termination of $\tY$-PAST terms).

The following theorem does not prove completeness, but is suggestive in that direction:
\begin{restatable}{theorem}{antitoneSeemsComplete}
For any stopping time $T$ which is almost surely finite, if $(\calF_n)_n$ is the coarsest filtration to which $T$ is adapted, then there is a supermartingale $(Y_n)_n$ adapted to $(\calF_n)_n$ and an antitone function $\epsilon$ such that $(Y_n)_n$ is an antitone ranking supermartingale with respect to $T$ and $\epsilon$.
\end{restatable}
