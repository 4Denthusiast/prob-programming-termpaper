% !TEX root = main.tex
\section{Constructing ranking functions}
\label{sec:ranking}

Although rankability implies almost sure termination, the converse does not hold in general. For example,
\begin{equation*}
\tif{{-}\tsample < 0}{\underline{0}}{(\tY \lambda x. x) \, \underline 0}
\label{ex:0 probability reachable}
\end{equation*}
terminates in 3 steps with probability 1, but isn't rankable because $(\tY \lambda x. x) \, \underline 0$ is reachable, although that has probability 0. 
Not only is this counterexample AST, it's PAST. %positively almost surely terminating i.e.~the expected time to termination is finite.

A ranking function can be constructed under the stronger assumptions that, for every $N$ reachable from $M$, the expected number of $\tY$-reduction steps from $N$ to a value is finite. 
In particular, the expected number of $\tY$-reduction steps from each reachable term is a ranking function. 
However, a finite number of expected $\tY$-reduction steps does not necessarily imply a finite number of expected total reduction steps.

\begin{example}
\label{ex:tY finite does not imply t finite}
The term $M = \Xi \, (\lambda x. x+1)$ where
\[
\Xi := \tY \lambda f \, n. \tif{\tsample - 0.5 < 0}{n \, \underline 0}{f (\lambda x. n (n \, x))}
\] 
terminates with only 2 $\tY$-reductions on average, i.e.,~$\expect{T_M^\tY} = 2$, but applies the increment function $2^n$ times with probability $2^{-n-1}$ for $n \geq 0$, which diverges, i.e.,~$\expect{T_M} = \infty$.
\end{example}

\begin{restatable}{theorem}{ThmMinimal} \label{thm:minimal}
Given a closed term $M$, the function $f:Rch(M) \to \mathbb R$ given by 
\[
f(N) := \mathbb E [\text{number of }\tY\text{-reduction steps from }N\text{ to a value}]
\] 
if it exists, is the least of all possible ranking functions of $M$.
\end{restatable}

\subsection{Sparse ranking functions}
Even in the case of reasonable simple terms, explicitly constructing a ranking function would be a lot of work, and \Cref{thm:minimal} makes even stronger assumptions than almost sure termination, so it isn't useful for proving it.

\begin{example}[Geometric distribution]
% Consider the term
% \begin{align*}
% &(\tY \lambda f, n. \\
% &\quad \tif{\tsample - 0.5 < 0}{n}{f (n+1)} \\
% &) \, \underline{0}
% \end{align*}
%\[
%\big(\tY \lambda f, n. \tif{\tsample - 0.5 < 0}{n}{f (n+1)} \\
%\big) \, \underline{0}
%\]
Let
\[
\Theta := \lambda f, n. \tif{\tsample - 0.5 < 0}{n}{f (n+1)}. 
\]
The term  $(\tY \, \Theta) \, \underline 0$ generates a geometric distribution.
Despite its simplicity, its $\mathit{Rch}$ contains all the terms in \Cref{fig:geometric distribution}, for each $i \in \mathbb N, r \in I$.
%\lo{TODO: reduce the figure to avoid overflow.}\akr{It still overflows the gutter, but not the actual text of the other column.}\lo{OK. Thanks.} 
The meanings of the substitutions (to be applied to the contractum) and conditions that label some of the reductions in the diagram should be self-explanatory.

\end{example}

Even in this simple case, defining a ranking function explicitly is awkward because of the number of cases, although in most cases, because the value need only be greater than or equal to that of the next term in sequence, it suffices to take the ranking function as having the same value as the next term, so that overall it takes only 3 distinct values.
(We will explain why later.)

The definition of rankability is also inconvenient for syntactic sugar. It could be useful, for example, to define $M \oplus_p N := \tif{\tsample - p < 0} M N$, where $M \oplus_p N$ reduces to $M$ or $N$, depending on the first value of $s \in \entrosp$, with probability $p$ resp.~$(1-p)$. Technically though, it reduces first to $\tif{\underline r - p < 0} M N$ for all $r \in I$, so those terms all need values of the ranking function too.

%\paragraph{}
In both of these cases, there are only some values of the ranking function that are semantically important. 
\begin{definition}
Define a \defn{sparse ranking function} on a closed term $M$ to be a partial function $f : Rch(M) \rightharpoonup \mathbb R$ such that
\begin{inparaenum}[(i)]
    \item $f(N) \geq 0$ for all $N \in \dom(f)$,
    \item $M \in \dom(f)$,
    \item for any $N \in \dom(f)$, evaluation of $N$ will eventually 
    %\akr{It would be nicer for this to be only almost certain, to make this more neatly in-between rankability and AST, but that doesn't actually work because of terms reachable with 0 probability. Of course, ASTness doesn't imply rankability, so any definition of sparse ranking function that includes all Y-PAST terms wouldn't always extend to a total ranking function.}
     reach some $O$ which is either a value or in $\dom(f)$, and $f(N) \geq \mathbb E[f(O) + \text{ the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of $\dom(f)$).
\end{inparaenum}

\iffalse
\begin{itemize}
    \item $f(N) \geq 0$ for all $N$ where $f$ is defined.
    \item $f$ is defined at $M$.
    \item For any $N$ in the domain of definition of $f$, evaluation of $N$ will eventually 
    %\akr{It would be nicer for this to be only almost certain, to make this more neatly in-between rankability and AST, but that doesn't actually work because of terms reachable with 0 probability. Of course, ASTness doesn't imply rankability, so any definition of sparse ranking function that includes all Y-PAST terms wouldn't always extend to a total ranking function.}
     reach some $O$ which is either a value or in the domain of definition of $f$, and $f(N) \geq \mathbb E[f(O) + \text{ the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}
\fi
\end{definition}
A sparse ranking function that is total is just a ranking function. Providing a sparse ranking function is essentially part way between providing a ranking function and directly proving almost sure termination.

\begin{restatable}[Sparse function]{theorem}{PartialImpliesRankable} \label{thm:partial implies rankable}
Every sparse ranking function is a restriction of a ranking function.
\end{restatable}


As a corollary, any term which admits a sparse ranking function terminates almost surely.

\subsection{Examples}
Let $M \oplus_p N = \tif{\tsample - p < 0} M N$, for $p \in (0,1]$, then there are the pseudo-reduction relations
\[
\begin{array}{c}
E[M \oplus_p N] \to^3  E[M] \qquad
E[M \oplus_p N] \to^3  E[N] \\[2mm]
\red^3(E[M \oplus_p N], s) = \left\{
    \begin{array}{ll}
        (E[M],\pi_t(s)) & \text{if } \pi_h(s) < p \\
        (E[N],\pi_t(s)) & \text{if } \pi_h(s) \geq p. 
    \end{array} \right .
\end{array}
\]
% \begin{align*}
% E[M \oplus_p N] \to^3 & E[M] & \\
% E[M \oplus_p N] \to^3 & E[N] & \\
% \red^3(E[M \oplus_p N], s) = & \left\{
%     \begin{array}{ll}
%         (E[M],\pi_t(s)) & \text{if } \pi_h(s) < p \\
%         (E[N],\pi_t(s)) & \text{if } \pi_h(s) \geq p. \\
%     \end{array} \right .
% \end{align*}
A sparse ranking function could be defined with respect to this shortcut reduction simply by replacing $\to$ and $\red$ in the definition of a sparse ranking function by a version that goes straight from $N \oplus_p O$ to $N$ or $O$. Such a pseudo-sparse ranking function would then be a partial function from a subset of $Rch(M)$, so it could also be considered as a partial function from all of $Rch(M)$, and it would in fact also be an actual sparse ranking function. It is therefore possible to prove rankability directly using the shortcutted reductions.

A similar procedure would work for other forms of syntactic sugar. If a closed term $N$ eventually reduces to one of a set of other terms $\{N_i \mid i \in I\}$ with certain probabilities, a sparse ranking function defined w.r.t.~a reduction sequence that skips straight from $N$ to $N_i$ is also a valid sparse ranking function for the original reduction function, and therefore its existence implies almost sure termination. There is a caveat, however, that $\tY$-reduction steps skipped over in the shortcut still need to be counted for the expected number of $\tY$-reduction steps.

%\paragraph{}
With this abbreviation, the geometric distribution example from earlier can be written as $(\tY \lambda f n.\ n \oplus_{0.5} f(n+1)) \, \underline 0$. It is then easy to see that the following is a sparse ranking function:
\begin{align*}
(\tY \lambda f n.\ n \oplus_{0.5} f(n+1))\, N & \mapsto 2 \\
\underline i \oplus_{0.5} (\tY \lambda f n.\ n \oplus_{0.5} f(n+1))\, (\underline i + 1) & \mapsto 1 \\
\underline i & \mapsto 0.
\end{align*}

In fact, even the partial function $\tY \Theta N \mapsto 2$ alone is a sparse ranking function for this term.
