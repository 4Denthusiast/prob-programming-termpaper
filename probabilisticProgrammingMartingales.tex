\documentclass{article}

%% BEGIN {Luke's Macros}
%%
\newif\ifdraft
\drafttrue %% To hide all comments and highlighting, just comment this line out 

\input{Style/comment}
\usepackage{calculation}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning, graphs, quotes, graphdrawing, shapes.geometric}
\usegdlibrary{layered, circular}
\usepackage[square]{natbib}
\setcitestyle{aysep={}}

\definecolor{oxblue}{RGB}{0,33,71}
\definecolor{oxgold}{HTML}{a0630a}
\usepackage[final,
bookmarks,
bookmarksopen,
colorlinks,
final,
linkcolor=red,
citecolor=oxgold,
pdfstartview=FitH ]{hyperref}

\usepackage{amsmath} % cleveref must be loaded afer amsmath
\usepackage{cleveref}
\Crefname{theorem}{Theorem}{Theorems}
\Crefname{corollary}{Corollary}{Corollary}
\crefname{proposition}{Proposition}{Propositions}
\Crefname{claim}{Claim}{Claims}
\Crefname{definition}{Definition}{Definitions}
\Crefname{fact}{Fact}{Facts}
\Crefname{conj}{Conjecture}{Conjectures}
\Crefname{example}{Example}{Examples}
\Crefname{remark}{Remark}{Remarks}
\Crefname{convention}{Convention}{Conventions}
\Crefname{lemma}{Lemma}{Lemmas}
\Crefname{assumption}{Assumption}{Assumptions}
\Crefname{section}{Section}{Sections}
\Crefname{appendix}{Appendix}{Appendices}
\Crefname{figure}{Figure}{Figures}
%\Crefname{equation}{Eq.}{Equations}

% \addtotheorempostheadhook[theorem]{\crefalias{thmlisti}{theorem}}
% \addtotheorempostheadhook[lemma]{\crefalias{thmlisti}{lemma}}
% \addtotheorempostheadhook[proposition]{\crefalias{thmlisti}{proposition}}
% \addtotheorempostheadhook[corollary]{\crefalias{thmlisti}{corollary}}
% \addtotheorempostheadhook[claim]{\crefalias{thmlisti}{claim}}
% \addtotheorempostheadhook[fact]{\crefalias{thmlisti}{fact}}
% \addtotheorempostheadhook[example]{\crefalias{thmlisti}{example}}
% \addtotheorempostheadhook[definition]{\crefalias{thmlisti}{definition}}
% \addtotheorempostheadhook[remark]{\crefalias{thmlisti}{remark}}
% \addtotheorempostheadhook[assumption]{\crefalias{thmlisti}{assumption}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\nnReal}{\mathbb{R}_{{\geq}0}}
\newcommand{\pReal}{\mathbb{R}_{{>}0}}

\newcommand\expect[1]{\mathbb{E}[#1]}
\newcommand\set[1]{\{#1\}}
\newcommand\dif{\mathrm{d}}
\newcommand\calF{\mathcal{F}}
\newcommand\calI{\mathcal{I}}
\newcommand\Leb{\mathrm{Leb}}
\newcommand\ndraw[2]{\#\mathrm{draw}_{#1}(#2)}

\makeatletter
\newcommand{\dotDelta}{{\vphantom{\Delta}\mathpalette\d@tD@lta\relax}}
\newcommand{\d@tD@lta}[2]{%
  \ooalign{\hidewidth$\m@th#1\mkern-1mu\cdot$\hidewidth\cr$\m@th#1\Delta$\cr}%
}

%\usepackage[bbgreekl]{mathbbol} %for \bblambda
%%
%% END {Luke's Macros}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{enumitem}
\newcommand{\tY}{\mathsf{Y}}
%\newcommand{\tif}[3]{\textsf{if }#1\textsf{ then }#2\textsf{ else }#3}
\newcommand{\tif}[3]{\mathsf{if}(#1, #2, #3)} % space-saving concrete syntax
\newcommand{\tsample}{\mathsf{sample}}
\newcommand{\tscore}{\mathsf{score}}
\newcommand{\skeletonPlaceholder}{\mathsf{X}} % I'm not sure what the best notation here is.
\DeclareMathOperator{\red}{red}
\DeclareMathOperator{\nnext}{next}
\DeclareMathOperator{\cbv}{cbv}


\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\theoremstyle{lemma}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\title{Supermartingales and Termination Analysis of Statistical PCF}

\begin{document}

\maketitle

\lo{Tentative title}

\begin{abstract}
\lo{TODO}
\end{abstract}

\iffalse
\akr{Following your suggestion, I will be providing a criterion for termination of programs in PPCF \citep{DBLP:journals/pacmpl/EhrhardPT18} based on ranking supermartingales. 
As it's more convenient for this proof, a sampling-based semantics \lo{You need to provide reference(s) for sampling-based semantics.} will be used instead of the original distributional semantics. 
I assume some roughly applicable equivalence is proven somewhere, but it doesn't seem that hard anyway.}
\lo{Why not provide a proof as an appendix?}
\fi

\tableofcontents

Various theorems about probabilistic programs rely on the assumption that the program terminates almost surely. Proof rules based on relating the program state to supermartingales already exist for first-order imperative programs \citep{DBLP:journals/pacmpl/McIverMKK18}. This paper's contribution is to extend this method to a higher-order setting.

\section{Statistical PCF}

\subsection{Syntax of SPCF}

The language SPCF is a simply-typed lambda calculus with sampling of real numbers from $[0,1]$ and unbounded scoring, following \cite{MakOP20b}. Types and terms are defined as follows, where $r$ is a real number, $x$ is a variable, $f : \mathbb{R}^n \to \mathbb{R}$ is any measurable function, and $\Gamma$ is an environment:
\begin{align*}
  & \text{types } A, B ::= \textsf{R}  \mid  A \to B \\
  & \text{values } V ::= \lambda x.M  \mid  \underline{r} \\
  & \text{terms } M, N ::= V  \mid  x  \mid  M_1 \, M_2  \mid  \underline{f}(M_1,\dots ,M_n)  \mid  \tY M  \mid  \tif{M < 0}{N_1}{N_2}  \mid  \tsample  \mid  \tscore(M)
\end{align*}
\begin{align*}
  \frac{}{\Gamma ; x:A \vdash x:A} \qquad
  \frac{\Gamma ; x:A \vdash M : B}{\Gamma \vdash \lambda x.M : A \to B} \qquad
  \frac{}{\underline{r} : \textsf{R}} \qquad
  \frac{\Gamma \vdash M:A \to B \quad \Gamma \vdash N : A}{\Gamma \vdash M \, N : B} \\ \\
  \frac{\Gamma \vdash M_1:\textsf{R} \dots \Gamma \vdash M_n:\textsf{R}}{\Gamma \vdash \underline{f}(M_1,\dots,M_n) : \textsf{R}} \ f : \mathbb{R}^n \to \mathbb{R} \qquad
  \frac{\Gamma \vdash M : (A \to B) \to (A \to B)}{\Gamma \vdash \tY M : (A \to B)} \\ \\
  \frac{\Gamma \vdash M : \textsf{R} \quad \Gamma \vdash N_1 : A \quad \Gamma \vdash N_2 : A}{\Gamma \vdash \tif{M < 0}{N_1}{N_2} : A} \qquad
  \frac{}{\Gamma \vdash \tsample : \textsf{R}} \qquad
  \frac{\Gamma \vdash s : \textsf{R}}{\Gamma \vdash \tscore (s) : \textsf{R}}
\end{align*}

Terms are identified up to $\alpha$-equivalence, as usual. The set of all terms is denoted $\Lambda$, and the set of closed terms is denoted $\Lambda^0$.

\paragraph{}
To define the reduction relation, let \emph{evaluation contexts} be of the form:
\begin{align*}
  E ::= & \, [\,] \mid E \, M \mid V \, E \mid \underline{f}(\underline r_1,\dots ,\underline r_{k-1}, E, M_{k+1}, \dots, M_n) \\ & \mid \tY E \mid \tif{E<0}{M_1}{M_2} \mid \tscore (E)
\end{align*}
then a term reduces if it is formed by substituting a redex in a context i.e.
\begin{align*}
  E[(\lambda x.M) N] & \to E[M[N/x]] \\
  E[\underline f (\underline r_1, \dots , \underline r_n)] & \to E[\underline{f(r_1,\dots,r_n)}] \\
  E[\tY \lambda x. M] & \to E[\lambda z. M[(\tY \lambda x. M)/x] z] \text{ where $z$ is not free in $s$}\\
  E[\tif{\underline r < 0}{M_1}{M_2}] & \to E[M_1] \text{ where }r < 0 \\
  E[\tif{\underline r < 0}{M_1}{M_2}] & \to E[M_2] \text{ where }r \geq 0 \\
  E[\tsample] & \to E[\underline r] \text{ where } r \in [0,1] \\
  E[\tscore(\underline r)] & \to E[\underline r].
\end{align*}
\changed[lo]{We write $\to^\ast$ for the reflexive, transitive closure of $\to$.}
\lo{The $\tscore$ rule should have a side condition: if $r > 0$.}

Every closed well-typed term either is a value or reduces to another term.

\subsection{Sampling semantics}
\label{sec:sampling semantics}
This version of the reduction relation allows $\tsample$ to reduce to any number in $[0,1]$. To more precisely specify the probabilities, an additional argument is needed to determine the outcome of random samples. Let $ I = [0,1] \subset \mathbb{R} $, and let $S = I^{\mathbb{N}}$, with the \changed[lo]{Borel $\sigma$-algebra $\calF$} and the probability measure, denoted $\mu$, given by the limit of $1 \gets I \gets I^2 \gets \cdots$, where the maps are the projections that ignore the last element. Equivalently, a basis of measurable sets is $\prod_{i=0}^\infty X_i$ where $X_i$ are all Borel and all but finitely many are $I$, and $\mu (\prod_{i=0}^\infty X_i) = \prod_{i=0}^\infty \Leb(X_i)$.
The maps $\pi_h:S \to I, \; \pi_t:S \to S$ popping the first element are then measurable.
\changed[lo]{Following \cite{DBLP:conf/esop/CulpepperC17}, we call the probability space $(S, \calF, \mu)$ the \emph{entropy space}.}

The $\sigma$-algebra and measure on $\Lambda$ are defined by considering it as a disjoint union of equivalence classes under replacing all the real constants by a placeholder, where the measure on each class is that of $\mathbb{R}^n$, where $n$ is the number of real constants.
\changed[lo]{Precisely, following \cite{DBLP:conf/icfp/BorgstromLGS16},
we view $\Lambda$ as $\bigcup_{m\in\omega} \big(\mathsf{Sk}_m \times \Real^m \big)$,
where $\mathsf{Sk}_m$ is the set of SPCF terms with exactly $m$ place-holders (a.k.a.~\emph{skeleton terms}) for numerals.
Thus identified, we give $\Lambda$ the countable disjoint union topology of the product topology of the discrete topology on $\mathsf{Sk}_m$ and the standard topology on $\Real^m$.
Note that the connected components of $\Lambda$ have the form $\{M\} \times \Real^m$, with $M$ ranging over $\mathsf{Sk}_m$, and $m$ over $\omega$. 
We fix the Borel algebra of this topology to be the $\sigma$-algebra on $\Lambda$.}

The one-step reduction is given by the function $\red : \Lambda^0 \times S \to \Lambda^0 \times S$ where
\begin{equation}
\red(M,s) = \left\{
    \begin{array}{ll}
        (E[N],s) & \text{if } M = E[R], R \to N \text{ and } R \neq \tsample \\
        (E[\underline{\pi_h(s)}],\pi_t(s)) & \text{if } M = E[\tsample] \\
        (M,s) & \text{if } M \text{ is a value}
    \end{array} \right .
\end{equation}
\lo{Both $\to$ and $\red$ are called reduction relation.}

The result after $n$ steps is then simply $\red^n(M,s) = \overbrace{\red(...\red(}^n M,s)...)$, and the limit $\red^\infty$ can then be defined as a partial function as $\lim_{n \to \infty} \red^n(M,s)$ whenever that sequence becomes constant by reaching a value. A term $M$ terminates for a sample sequence $s$ if the limit $\red^\infty(M,s)$ is defined.

The reduction function is measurable, and the set of values is measurable, therefore the set of $s$ such that $M$ terminates at $s$ within $n$ steps is measurable for any $n$, therefore $\{s \mid M \text{ terminates at } s \}$ is measurable. A term $M$ is said to terminate almost surely if $\mu(\{s \mid M \text{ terminates at } s\}) = 1$.

\lo{Alternatively, define the \emph{runtime of $M$} to be the random variable 
\[
T_M(s) := 
\begin{cases}
\min \set{n \mid \pi_0(\red^n(M, s)) \textrm{ is a value}} & \hbox{if $\red^\infty(M,s)$ is defined}\\
\infty & \hbox{otherwise}
\end{cases}
\]
Equivalently, we say that $M$ is \emph{almost-surely terminating} (AST) if $T_M < \infty$ a.s.; 
and $M$ is \emph{positively almost-surely terminating} (PAST) if $\expect{T_M} < \infty$.}

For example, the term 
\[
(\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)} ) \, \underline{0},
\] 
which generates a geometric distribution, terminates on the set $S \setminus [0.5,1]^\mathbb N$, which has measure 1, therefore it terminates almost surely, whereas 
\[
\tif{\tsample - 0.5 < 0}{\underline 0}{(\tY \lambda x. x) \underline 0},
\] 
which terminates on the set $\pi_h^{-1}[[0,0.5]]$, has probability 0.5 of failing to terminate.

\paragraph{}
Note that the requirement that $M$ be closed is not important for the proofs of the theorems, but is simply required to ensure that $M$ reaches a value, rather than a general normal form like $x \, y$.

\paragraph{}
This definition of almost-sure termination is equivalent to that given in \citep{MakOP20b}, although the program semantics is stated in a slightly different way. In particular, the argument to $\tscore$ is not relevant to termination (except for the possibility that its argument's evaluation wouldn't terminate).

\iffalse
\lo{Your operational semantics does not maintain a record of the current weight of the reduction.
A.s.~termination does depend on $\tscore$: see \cite[\S 4.3]{DBLP:journals/corr/abs-2004-03924}\footnote{\url{https://arxiv.org/abs/2004.03924}}.
I think it important to take the behaviour of $\tscore$ into account;
you should do it as a future task.}
\fi

\section{Supermartingales}
One approach to proving that a term terminates almost surely is to find some variant that is bounded below and, on average, decreases sufficiently quickly that it must eventually reach 0, similarly to the approach taken in \citep{DBLP:journals/pacmpl/McIverMKK18} for imperative programs.

These variants are defined as functions from reachable terms (i.e.~possible states of the program's execution) to real numbers. Specifically, let the set of reachable terms from a given closed starting term $M$ be $Rch(M) := \{N \in \Lambda \mid M \to^* N \}$, with the $\sigma$-algebra induced as a subset of $\Lambda$.

\begin{definition}\rm
A \emph{ranking function on $M$} is a measurable function $f:\mathit{Rch}(M) \to \mathbb{R}$ such that $f(N) \geq 0$ for all $N$, and
\begin{enumerate}
    \item $f(E[\tY \lambda x. N]) \geq 1+ f(E[\lambda z. N[(\tY \lambda x. N)/x] z) \text{ where $z$ is not free in $N$}$
    \item $f(E[\tsample]) \geq \int_I f(E[\underline{x}]) \, \Leb(\mathrm{d}x)$

    \item $f(E[R]) \geq f(E[R'])$ for any other redex $R$, where $R \to R'$.
\end{enumerate}
We say that the ranking function $f$ is \emph{strict} if there exists $\epsilon > 0$ such that $f(N) = 0$ iff $N$ is a value, and for all $E$ and $R \to R'$, $f(E[R']) \leq f(E[R]) - \epsilon$.

Any closed term for which a ranking (respectively, strict ranking) function exists is called \emph{rankable} (respectively, \emph{strictly rankable}). 
\changed[lo]{For example, the term $(\tY \lambda x.x) \, \underline 0$ is not rankable.}
It will be demonstrated later that for any rankable term $M$, if $(M_n)_{n \in \omega}$ is the reduction sequence starting from $M$, then $(f(M_n))_{n\in\omega}$ is a supermartingale, and $M$ terminates almost surely, but first, some preliminaries about supermartingales.
\end{definition}

\input{ranking-supermartingales}

\section{Constructing Ranking Functions}
Although rankability implies almost-sure termination, the converse does not hold in general. For example,
\begin{equation}
\tif{{-}\tsample < 0}{\underline{0}}{(\tY \lambda x. x) \, \underline 0}
\label{ex:0 probability reachable}
\end{equation}
terminates in 3 steps with probability 1, but isn't rankable because $(\tY \lambda x. x) \, \underline 0$ is reachable, although that has probability 0. 
Not only is this counterexample AST, it's PAST. %positively almost surely terminating i.e.~the expected time to termination is finite.

A ranking function can be constructed under the stronger assumptions that, for every $N$ reachable from $M$, the expected number of $\tY$-reduction steps from $N$ to a value is finite. In particular, the expected number of $\tY$-reduction steps from each reachable term is a ranking function. Note that a finite number of expected $\tY$-reduction steps does not necessarily imply a finite number of expected total reduction steps.

\begin{example}
\label{ex:tY finite does not imply t finite}
The term
\[
M = \big(\tY \lambda f n. \tif{\tsample - 0.5 < 0}{n \, \underline 0}{f (\lambda x. n (n x))}\big) (\lambda x. x+1)
\] 
terminates with only 2 $\tY$-reductions on average i.e.~$\expect{T_M^\tY} = 2$, but applies the increment function $2^n$ times with probability $2^{-1-n}$ for $n \geq 0$, which diverges i.e.~$\expect{T_M} = \infty$.
\end{example}

\begin{theorem} \label{thm:minimal}
Given a closed term $M$, the function $f:Rch(M) \to \mathbb R$ given by $f(N) = \mathbb E [\text{the number of }\tY\text{-reduction steps from }N\text{ to a value}]$, if it exists, is the least of all possible ranking functions of $M$.
\end{theorem}
\begin{proof}
Let $f$ be the candidate least ranking function defined above, and suppose $g$ is another ranking function such that $f(N) > g(N)$ for some $N \in Rch(M)$. The restrictions of $f$ and $g$ to $Rch(N)$ have the same properties assumed of $f$ and $g$, so assume w.l.o.g.~that $N=M$. The difference $g - f$ is then a supermartingale (with the same setup as in the proof of %\Cref{thm:rankable gives supermartingale}
%Theorem \ref{rankable implies ast}) 
\Cref{thm:rankable and strict rankable});
therefore $\mathbb E[g(M_n)] \leq \mathbb E[f(M_n)] + g(M)-f(M)$, for all $n$.

We have $\mathbb E[f(M_n)] = \sum_{k=n}^\infty \mathbb P[M_k = E[\tY N] \text{ for some }E, N] \to 0 \text{ as } n \to \infty$; 
therefore as $g(M) - f(M) < 0$, eventually $\mathbb E[g(M_n)] < 0$, which is impossible. 
It follows that $g \geq f$ as required.

In order for $f$ to be the least ranking function of $M$, it also has to actually be a ranking function itself. Each of the conditions on a ranking function is easily verified from the definition of $f$.
\end{proof}

\subsection{Partial ranking functions}
Even in the case of reasonable simple terms, explicitly constructing a ranking function would be a lot of work, and \Cref{thm:minimal} makes even stronger assumptions than almost-sure termination, so it isn't useful for proving it.

\begin{example}[Geometric distribution]
Consider the term
\begin{align*}
&(\tY \lambda f, n. \\
&\quad \tif{\tsample - 0.5 < 0}{n}{f (n+1)} \\
&) \, \underline{0}
\end{align*}
which generates a geometric distribution (let $\Theta = \lambda f, n : \tif{\tsample \underline{- 0.5} < 0}{n}{f (n\underline{+ 1})}$, then the term is $(\tY \Theta)\ \underline 0$).
    Despite its simplicity, its $Rch$ contains all the terms

\begin{tikzpicture}[node distance=0.5em, anchor=west]
    \node (1) at (0,-0) {$\tY\ \Theta\ \underline 0$};
    \node (2) at (0,-1) {$(\lambda z. (\lambda n. \tif{\tsample \underline{- 0.5} < 0}{n}{\tY\ \Theta\ (n \underline{+ 1})}) z)\ \underline i$};
    \node (3) at (0,-2) {$(\lambda n. \tif{\tsample \underline{- 0.5} < 0}{n}{\tY\ \Theta\ (n \underline{+ 1})})\ \underline i$};
    \node (4) at (0,-3) {$\tif{\tsample \underline{- 0.5} < 0}{\underline i}{\tY\ \Theta\ (\underline i\ \underline{+ 1})}$};
    \node (5) at (0,-4) {$\tif{\underline r\ \underline{- 0.5} < 0}{\underline i}{\tY\ \Theta\ (\underline i\ \underline{+ 1})}$};
    \node (6) at (0,-5) {$\tif{\underline r < 0}{\underline i}{\tY\ \Theta\ (\underline i\ \underline{+ 1})}$};
    \node (7) at (2,-6) [text width=] {$\underline i$};
    \node (8) at (0,-7) {$\tY\ \Theta\ (\underline i\ \underline{+ 1})$};
    \node (9) at (0,-8) {$(\lambda z. (\lambda n. \tif{\tsample \underline{- 0.5} < 0}{n}{\tY\ \Theta\ (n \underline{+ 1})}) z)\ (\underline i\ \underline{+ 1})$};
    
    \draw [->] (1.south west) to [bend right=45] (2.north west);
    \draw [->] (2.south west) to [bend right=45] (3.north west);
    \draw [->] (3.south west) to [bend right=45] (4.north west);
    \draw [->] (4.south west) to [bend right=45] (5.north west);
    \draw [->] (5.south west) to [bend right=45] (6.north west);
    \draw [->] (6.south west) to [bend right=45] (8.north west);
    \draw [->] (8.south west) to [bend right=45] (9.north west);
    
    \draw [->] (9.west) to [bend left=45] (2.west);
    \draw [->] (6) to (7);
\end{tikzpicture}

\end{example}

Even in this simple case, defining a ranking function explicitly is awkward because of the number of cases, although in most cases, because the value need only be greater than or equal to that of the next term in sequence, it suffices to take the ranking function as having the same value as the next term, so that overall it takes only 3 distinct values.
(We will explain why later.)

The definition of rankability is also inconvenient for syntactic sugar. It could be useful, for example, to define $M \oplus_p N = \tif{\tsample - p < 0} M N$, where $M \oplus_p N$ reduces to $M$ or $N$, depending on the first value of $s$\lo{Need a name for $s$.}, with probability $p$ resp.~$(1-p)$. Technically though, it reduces first to $\tif{\underline r - p < 0} M N$ for all $r \in I$, so those terms all need values of the ranking function too.

\paragraph{}
In both of these cases, there are only some values of the ranking function that are semantically important. 
\begin{definition}
Define a \emph{partial ranking function} on a closed term $M$ to be a partial function $f : Rch(M) \rightharpoonup \mathbb R$ such that
\begin{itemize}
    \item $f(N) \geq 0$ for all $N$ where $f$ is defined.
    \item $f$ is defined at $M$.
    \item For any $N$ in the domain of definition of $f$, evaluation of $N$ will eventually \akr{It would be nicer for this to be only almost certain, to make this more neatly in-between rankability and ast, but that doesn't actually work because of terms reachable with 0 probability. Of course, astness doesn't imply rankability, so any definition of partial ranking function that includes all Y-past terms wouldn't always extend to a total ranking function.} reach some $O$ which is either a value or in the domain of definition of $f$, and $f(N) \geq \mathbb E[f(O) + \text{ the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}
\end{definition}
A partial ranking function that is total is just a ranking function. Providing a partial ranking function is essentially part way between providing a ranking function and directly proving almost sure termination.

\begin{theorem} \label{thm:partial implies rankable}
Every partial ranking function is a restriction of a ranking function.
\end{theorem}
\begin{proof}
Take a closed term $M$ and partial ranking function $f$ on $M$. Define $f_1 : Rch(M) \rightharpoonup \mathbb R$ by
\begin{align*}
    f_1(N) & := f(N) \text{ whenever $f(N)$ is defined,} \\
    f_1(V) & := 0 \text{ for values $V$ not in the domain of $f$.}
\end{align*}

Define $(\nnext(N,s),\_) = \red^n(N,s)$ for the least $n \geq 0$ such that it's in the domain of $f_1$, and $g(N,s) := \left | \{m < n \mid \red^m(N,s) \text{ is of the form } (E[\tY N'],s') \} \right |$. 
The function $\nnext$ is well-defined (i.e.~$n$ is finite) for all $N \in Rch(M)$ by induction on the path from $M$ to $N$, by the third condition on partial ranking functions. Define $f_2(N) = \int_S f_1(\nnext(N,s)) + g(N,s) \, \mu(\mathrm d s)$. The (total) function $f_2$ agrees with $f$ on $f$'s domain, and it is a ranking function on $M$ (in fact, the least ranking function of which $f$ is a restriction).
\lo{TODO. The preceding sentence deserves a proof.}
\end{proof}


As a corollary, any term which admits a partial ranking function terminates almost surely.

\subsection{Examples}
Let $M \oplus_p N = \tif{\tsample - p < 0} M N$, for $p \in (0,1]$, then there are the pseudo-reduction relations
\begin{align*}
E[M \oplus_p N] \to^3 & E[M] & \\
E[M \oplus_p N] \to^3 & E[N] & \\
\red^3(E[M \oplus_p N], s) = & \left\{
    \begin{array}{ll}
        (E[M],\pi_t(s)) & \text{if } \pi_h(s) < p \\
        (E[N],\pi_t(s)) & \text{if } \pi_h(s) \geq p. \\
    \end{array} \right .
\end{align*}
A partial ranking function could be defined with respect to this shortcut reduction simply by replacing $\to$ and $\red$ in the definition of a partial ranking function by a version that goes straight from $N \oplus_p O$ to $N$ or $O$. Such a pseudo-partial ranking function would then be a partial function from a subset of $Rch(M)$, so it could also be considered as a partial function from all of $Rch(M)$, and it would in fact also be an actual partial ranking function. It is therefore possible to prove rankability directly using the shortcutted reductions.

A similar procedure would work for other forms of syntactic sugar. If a closed term $N$ eventually reduces to one of a set of other terms $\{N_i \mid i \in I\}$ with certain probabilities, a partial ranking function defined with respect to a reduction sequence that skips straight from $N$ to $N_i$ is also a valid partial ranking function for the original reduction function, and therefore its existence implies almost sure termination. There is a caveat, however, that $\tY$-reduction steps skipped over in the shortcut still need to be counted for the expected number of $\tY$-reduction steps.

\paragraph{}
With this abbreviation, the geometric distribution example from earlier can be written as $(\tY \lambda f n.\ n \oplus_{0.5} f(n+1)) \, \underline 0$. It is then easy to see that the following is a partial ranking function:
\begin{align*}
(\tY \lambda f n.\ n \oplus_{0.5} f(n+1))\ N & \mapsto 2 \\
\underline i \oplus_{0.5} (\tY \lambda f n.\ n \oplus_{0.5} f(n+1))\ (\underline i + 1) & \mapsto 1 \\
\underline i & \mapsto 0.
\end{align*}

In fact, even the partial function $\tY \Theta N \mapsto 2$ alone is a partial ranking function for this term.
\section{Antitone ranking functions}

\begin{example}[Random walk]
\label{ex:ac-ranking}
\begin{enumerate}
\item 1D biased (towards 0) random walk
\[
M_1 = 
\big(\tY \, \lambda f n . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{3/2} f \, (n + 1)}\big) \, 10
\]

\item 1D unbiased random walk
\[
M_2 = 
\big(\tY \, \lambda f n . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{1/2} f \, (n + 1)}\big) \, 10
\]

\item 1D biased (away from 0) random walk
\[
M_3 = 
\big(\tY \, \lambda f n . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{1/3} f \, (n + 2)}\big) \, 10
\]


%While {x > 0} do {x := x-1 \oplus_2/3 x:= x+1}

%M2 = While {x > 0} do {x := x-1 \oplus_2/3 x:= x+2}
\end{enumerate}
\end{example}

The term $M_1$ is rankable, terminating in 31 $\tY$-steps on average, and $M_3$ only has a $1/1024$ chance of terminating, but in between, $M_2$ is AST, but isn't $\tY$-PAST, therefore it isn't rankable and \Cref{thm:rankable implies termination} is insufficient to prove its termination. We therefore want to find a generalised notion of ranking function so that $M_2$ becomes rankable, and then prove it sound i.e.~rankable in this generalised sense implies AST.

\paragraph{Idea.} 

The definition of antitone-convex ranking function $f$ (say) for $M \in \Lambda^0$ is the same as that of ranking function except that in the case of $\tY$-redex, 
we require the existence of an antitone (meaning: $r < r'$ implies $\epsilon(r) \geq \epsilon(r')$) function $\epsilon : \nnReal \to \pReal$ such that the ranking function $f :\mathit{Rch}(M) \to \nnReal$ satisfies
\[
f(E[R’]) \leq f(E[R]) - \epsilon(f(E[R])) 
\]
where $R \to R’$ is the $\tY$-redex rule.

\lo{An aside: This CBV $\tY$-rule seems cleaner:
\[
\big(\tY f^{A \to B} \, x^A \, . \, \theta^B \big) \, v \to \theta[(\tY f \, x \, . \, \theta) / f, v / x].
\]
We assume $\tY f^{A \to B} \, x^A \, . \, \theta^B$ is a value.}

\begin{definition}
We say that a supermartingale $(Y_n)_{n \in \omega}$ and a stopping time $T$ adapted to filtration $(\calF_n)_{n \in \omega}$ are \emph{antitone strict} if $Y_n \geq 0$, and there exists an antitone function $\epsilon : \nnReal \to \pReal$ satisfying
\[
\expect{Y_{n+1} \mid \calF_n} \leq Y_n - \epsilon (Y_n) \cdot 1_{\set{T > n}}
\]
\end{definition}

%\lo{The preceding definition does not place any constraint on the stopping time $T$ (other than that it is w.r.t.~the filtration $(\calF_n)_{n \in \omega}$). However for the definition to make sense, $T$ must be the stopping time $T(s) := \min \set{n \mid Y_n(s) = 0}$.}

\begin{theorem}
\label{thm:a-c strict}
Let $(Y_n)_{n \in \omega}$ be an antitone strict supermartingale via function $\epsilon : \nnReal \to \pReal$, and stopping time $T$. 
Then $T < \infty$ almost surely.
\end{theorem}

\begin{proof}
First, as $(Y_n)$ is a supermartingale, $\expect{Y_n} \leq \expect{Y_0}$. 
Therefore

\begin{calculation}
\expect{Y_n \mid T > n}
  \step[=]{rearranging terms}
\dfrac{\mathbb P[T > n] \, \expect{Y_n \mid T > n} 
+ 
\mathbb P[T \leq n] \, \expect{Y_n \mid T \leq n} 
- 
\mathbb P[T \leq n] \, \expect{Y_n \mid T \leq n}}{\mathbb P[T > n]}
  \step[=]{definition of conditional expectation}
\dfrac{\expect{Y_n} - \mathbb P[T \leq n] \, \expect{Y_n \mid T \leq n}}{\mathbb P[T > n]}
  \step[\leq]{$Y_n \geq 0$ always}
\dfrac{\expect{Y_n}}{\mathbb P[T > n]}
  \step[\leq]{$(Y_n)_n$ is a supermartingale}
\dfrac{\expect{Y_0}}{\mathbb P[T > n]}
\end{calculation}

\emph{Claim}: For all $0 < x \leq 1$, 
\(
\mathbb P[T > B_x] \leq x,
\)
where $B_x = \left \lceil \dfrac{\expect{Y_0}+1}{x \; \epsilon(\expect{Y_0} \, x^{-1})} \right \rceil$. 

\smallskip

As the convex hull of $\epsilon$ (the greatest convex function less than or equal to it) satisfies all the conditions assumed of $\epsilon$, in addition to being convex, assume wlog that $\epsilon$ is convex.

Assume for a contradiction that $\mathbb P[T > B_x] > x$.
Then, take $n \leq B_x$. 
We have
%\lo{Change of justification of the second equality below: event, not discrete r.v. Recall: expectation conditioning on r.v.~(resp.~event) is a r.v.~(resp.~number).}
\begin{calculation}
\expect{Y_n-Y_{n+1}}
  \step[=]{$\calF_n \subseteq \calF_{n+1}$, definition and linearity of conditional expectation}
\expect{Y_n - \expect{Y_{n+1} \mid \calF_n}}
  \step[\geq]{antitone strict assumption}
\expect{\epsilon(Y_n) \cdot 1_{\set{T > n}}}
%  \step[=]{definition of conditional expectation for a discrete variable}
\step[=]{definition of expectation conditioning on an \emph{event}}
\mathbb P[T > n]\ \expect{\epsilon(Y_n) \mid T > n}
  \step[\geq]{Jensen's inequality}
\mathbb P[T > n]\ \epsilon(\expect{Y_n \mid T > n})
  \step[\geq]{proved earlier}
\mathbb P[T > n]\ \epsilon\Big(\dfrac{\expect{Y_0}}{\mathbb P[T > n]}\Big)
  \step[>]{assumption, $\mathbb P[T > n] \geq \mathbb P[T > B_x] > x$}
x \; \epsilon\big(\expect{Y_0} \, x^{-1}\big).
\end{calculation}
Therefore, by a telescoping sum
\[
\expect{Y_{B_x}} = \expect{Y_{B_x}-Y_0+Y_0} \leq \expect{Y_0} - B_x \; x \; \epsilon(\expect{Y_0} \, x^{-1}) \leq -1 < 0 
\]
%\lo{$\expect{Y_0 - B_x \; x \; \epsilon(\expect{Y_0} \, x^{-1})}$}
which is a contradiction, therefore the claim must be true, therefore $P[T > n]$ tends to 0 as $n$ tends to infinity, therefore $T < \infty$ almost surely.
\end{proof}

Note that it is essential in this proof that $\epsilon$ be defined for all $\mathbb R_{\geq 0}$, not just the values that $(Y_n)_{n \in \omega}$ actually takes (or at least if $(Y_n)_{n \in \omega}$ is uniformly bounded, $\epsilon$ must be positive at the supremum as well as the actual values). 

\begin{example}
Consider a random variable $X$, uniformly distributed in the interval $[-1,1]$, $T := \min \set{n \mid 2^{-n} < X}$, 
\[
Y_k := 
\begin{cases}
4-2^{-k} & \hbox{if $k < T$}\\
0 & \hbox{otherwise}
\end{cases} 
\]
and $\epsilon(x) = \frac{4-x}{4}$. 
In this case, $(Y_n)_{n \in \omega}$ either tends to $4$ as $n \to \infty$, or drops to 0 at some point, with an exponentially decreasing probability. It is an antitone strict supermartingale except that $\epsilon$ goes to $0$ at $4$, and $T = \infty$ with probability $\frac 1 2$ even though $4$ is larger than any value $(Y_n)_{n \in \omega}$ can actually take and $\epsilon(Y_n)$ never actually reaches $0$.
\lo{@Andrew: I refer to the preceding highlighted claim. Did you mean: for all $n, s$, $\epsilon(Y_n(s)) > Y_n(s)$?} \akr{No. This phrasing should be clearer.}
\end{example}

\begin{theorem}[Antitone ranking function soundness] \label{thm:antitone rankable implies termination}
If a closed SPCF term $M$ is antitone rankable, then $T_M^{\tY} < \infty$ a.s.~(equivalently, $M$ is AST).
\end{theorem}
\begin{proof}
As in %Theorem \ref{thm:basicRankingSoundness}, 
\Cref{thm:rankable implies termination},
take the set $S$ of traces as a probability space, and define the same random variables $T_n, X_n, Y_n, T_M^{\tY}$. As before, $(Y_n)_{n \in \omega}$ is a supermartingale, and because $M$ is now assumed to be antitone rankable, it is an antitone strict supermartingale with respect to the stopping time $T_M^{\tY}$. Therefore, it follows from \Cref{thm:a-c strict} that $T_M^{\tY}$ is almost-surely finite.
\end{proof}

\paragraph{}
As before, constructing antitone ranking fuctions completely is not necessary, and there is a corresponding notion of an antitone partial ranking function.

Define an \emph{antitone partial ranking function} on a closed term $M$ to be a partial function $F : Rch(M) \rightharpoonup \mathbb{R}$ such that for some antitone function $\epsilon : \mathbb{R}_{\geq 0} \to \mathbb{R}_{>0}$
\begin{itemize}
\item $f(N) \geq 0$ for all $N$ where $f$ is defined.
\item $f$ is defined at $M$.
\item For any $N$ in the domain of definition of $f$, evaluation of $N$ will eventually reach some $O$ which is either a value or in the domain of definition of $f$, and $f(N) \geq \mathbb E [f(O) + \epsilon(f(O)) \times \text{the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}

\begin{theorem}  \label{thm:antitone partial implies rankable}
  Every antitone partial ranking function is a restriction of an antitone ranking function.
\end{theorem}
\begin{proof}
  Take a closed term $M$ and an antitone partial ranking function $f$ on $M$, with a corresponding antitone function $\epsilon$. Assume wlog that $\epsilon$ is convex (as if it isn't, we can just take its convex hull instead). As in Theorem \ref{thm:partial implies rankable}, define $f_1 : Rch(M) \rightharpoonup \mathbb R$ by
  \begin{align*}
    f_1(N) &= f(N) \text{ whenever $f(N)$ is defined},\\
    f_1(V) &= 0 \text{ for values $V$ not in the domain of $f$.}
  \end{align*}

  Define $(\nnext(N,s),\_) = \red^n(N,s)$ for the least $n \geq 0$ such that it's in the domain of $f_1$, and $g(N,s) = \left | \{m < n \mid \red^m(N,s) \text{ is of the form } (E[\tY N'],s') \} \right |$. 
  The function $\nnext$ is well-defined (i.e.~$n$ is finite) for all $N \in Rch(M)$ by induction on the path from $M$ to $N$, by the third condition on antitone partial ranking functions. Define $f_2(N) = \int_S f_1(\nnext(N,s)) + \epsilon(f_1(\nnext(N,s))) g(N,s) \, \mu(\mathrm d s)$. For any term $N$ where $f$ is defined, $f_2(N) = f(N)$, and the value that $f_2$ would have at $N$ if $f$ were not defined at $N$ is $\leq f(N)$, by the third condition on antitone partial ranking functions. In order to show that $f_2$ is an antitone partial ranking function, it therefore suffices to show that the value that $f_2$ would have had at each term if $f$ were not defined at that term is at least the expectation of $f_2$ after one reduction step (plus $\epsilon(f_2(N))$ if the reduction step is a $tY$-reduction). For any term $N$ which is not of the form $E[R]$ for some $\tY$-redex $R$, this is trivial. If $R$ is a $\tY$-redex, then $\epsilon(f_2(N)) \leq \int_S \epsilon(f_1(\nnext(N,s)))\, \mu(\mathrm d s)$ by the convexity of $\epsilon$, because $n$ is bounded. Therefore, the (total) function $f_2$, which agrees with $f$ on $f$'s domain, is an antitone ranking function on $M$ (with the same function $\epsilon$ if it's convex).
\end{proof}

As a corollary, any term which admits an antitone partial ranking function terminates almost surely.

\paragraph{\Cref{ex:ac-ranking} revisited}
\akr{All of these ranking functions are slightly incorrect, because $\Xi \, \underline n$ reduces to something like $\Xi \, (\underline n + 1)$ then $(\lambda z. \text{something}[\Xi/f]) (\underline n + 1)$, rather than reducing the argument first. This could be fixed by appealing to the confluent semantics version of the antitone ranking theorem, but it would be nice to have at least some examples here illustrating how antitone ranking functions work, rather than actually saving all these examples until the end.

Even though the difference is quite trivial, this was actually the original motivating example for the confluent trace semantics.}

Programs $M_1$ and $M_2$ are AST. For $M_1$, the partial ranking function 
\[
\big(\tY \, \lambda f n . \, \tif{n=0}{0}{f \, (n-1) \oplus_{3/2} f \, (n+1)}\big) \, \underline x \mapsto 3x + 1
\] 
suffices to prove its termination (and could equivalently be considered an antitone partial ranking function with the constant antitone function $\epsilon_1(x) = 1$).

For $M_2$, define the function $g_1 : \nnReal \to \nnReal$ by $g_1(x) = \log(x+1) + 1$.
Using shorthand 
$\Theta_2 = \tY \, \lambda f n . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{1/2} f \, (n + 1)}$, 
we can define an antitone partial ranking function $f_2 : \mathit{Rch}(M_2) \rightharpoonup \nnReal$ 
%with antitone function $\epsilon_2$ defined as follows (for $n \in \mathbb N$):
defined as: for $n \in \mathbb N$
\begin{align*}
{\Theta_2} \, n 
&\mapsto 
g_2(n)
\\
0 &\mapsto 0.
\end{align*}
For $n \geq 1$, $\Theta_2 \, n$ reduces in several steps to either $\Theta_2 \, (n-1)$ or $\Theta_2 \, (n+1)$, each with probability $1/2$, with one $\tY$-reduction.
Now
\begin{align*}
  & g_2(n) - \frac{g_2(n-1) + g_2(n+1)} 2 \\
  = & \log \Big(\frac{n+1}{\sqrt{n(n+2)}}\Big) \\
  = & \frac 1 2 \log\left(\frac{(n+1)^2}{n(n+2)}\right) \\
  = & \frac 1 2 \log\left(1 + \frac 1 {n(n+2)}\right) \\
  > & \frac 1 {4n(n+2)} \\
  > & \frac 1 2 \frac 1 {5(n+1)^2} + \frac 1 2 \frac 1 {5(n+3)^2}
\end{align*}
and $\Theta_2 \, 0$ reduces to $0$ with one $\tY$-reduction with a reduction in $g_2$ of $1$.
Therefore by setting $\epsilon_2(x) = \frac 1 {5(e^{x-1}+1)^2}$, the condition on how much $g_2$ must decrease is met. It is also defined at $M_2 = \Theta_2 \, 10$, and is non-negative, therefore it is an antitone partial ranking function, and $M_2$ is AST.

\begin{example}[Continuous random walk]\label{ex:raven complex}
In SPCF we can construct a function whose argument changes by a random amount at each recursive call:
\[
\underbrace{\big
(\tY \, \lambda f x . \tif{x \leq 0}{0}{f(x - \tsample))} \big)}_{\Theta} \, 10
\]
We can construct a partial ranking function $f$ as follows:
\begin{align*}
\Theta \, \underline l 
&\mapsto 
l + 2
\\
\tif{l \leq 0}{0}{\Theta \, (l - \tsample)}
&\mapsto
l + 1
\\
0 &\mapsto 0.
\end{align*}

For a more complex (not $\tY$-PAST) example, consider the following ``continuous random walk''
\[
\underbrace{\big
(\tY \, f \, x . \tif{x \leq 0}{0}{f(x - \tsample + 1/2)} \big)}_{\Xi} \, 10
\]
Let $g(x) := 2 + \log(x + 1)$, and let $\epsilon$ be the function specified by
\[
\epsilon(g(x-1/2)) = g(x) - \int_{x-1/2}^{x+1/2}g(y) \, \mathrm d y.
\]
The limit of this as $x \to \infty$ and $g(x+1/2) \to \infty$ is 0, and $\frac {\mathrm d}{\mathrm dx} \epsilon(g(x+1/2)) = \frac 1 {x+1} + \log(1 - \frac 1 {x + 1/2}) < 0$, and $g$ is monotonic increasing; 
therefore $\epsilon$ is antitone and bounded below by 0.
We define a partial antitone ranking function as follows:
\begin{align*}
\Xi \, l 
&\mapsto 
g(l)
\\
0 &\mapsto 0
\end{align*}
\end{example}
The value of $g$ after one $\tY$-reduction step is at least $g(l-1/2)$, therefore the expectation of $\epsilon$ after one $\tY$-reduction step is at most $\epsilon(g(l-1/2)) = g(x) - \int_{x-1/2}^{x+1/2}g(y) \, \mathrm d y$. 
Thus 
\begin{itemize}
\item in case $l > 0$, $g$ decreases by the required amount
\item in case $l \leq 0$, 
\(
g(\Xi \, l) \geq 2 + \log(1/2) > \log(\frac{3 \sqrt 3} 2) - 1 = \epsilon(g(0-1/2))
\) 
as well.
\end{itemize}
Hence this is a valid partial antitone ranking function and the term is AST.

\begin{example}[Fair-in-the-limit random walk]
\label{ex:Fair-in-the-limit random walk}\citep[\S 5.3]{DBLP:journals/pacmpl/McIverMKK18}
\[
\underbrace{\big
(\tY \, f \, x . 
\tif{x \leq 0}{0}{f(x - 1) \oplus_{\frac{x}{2x+1}} f(x + 1)} \big)}_{\Xi} 
\, 10
\]
To construct an antitone ranking function, we solve the recurrence relation:
\[
\left\{
\begin{array}{rll}
z_0 &=& 0\\
n \geq 1, \quad z_n &>& \dfrac{n}{2n + 1} \, z_{n-1} + \dfrac{n+1}{2n + 1} \, z_{n+1}.
\end{array}
\right.
\]
For $n > 2$, $z_n = \log(n-1)$ works. The expected decrease is $\frac 1 2(\log(1+\frac 1 {(n-1)^2-1}) - \frac 1 {2n+1} \log(1 + \frac 2 {n-2}))$, and using the fact that $\frac x {x+1} \leq \log (1+x) \leq x$ for $x > 0$, this is at least $\frac 1 2(\frac 1 {(n-1)^2} - \frac 1 {2n + 1} \frac 2 {n-2}) = \frac {n^2}{2(n-1)^2(2n+1)(n-2)}$, which (again for $n > 2$) is positive and antitone. For $n = 0, 1, 2$, we take $\epsilon$ to be 9/40 (the same as its value at 3), then set $z_2 = 2 \log 2 - \log 3 - 1, z_1 = 3 \log 2 - 2 \log 3 - 3, z_0 = 4 \log 2 - 3 \log 3 - 6$. Some of those values are negative, but it's still bounded below, so by adding a constant offset it can be corrected, and the term is AST.

\end{example}

\begin{example}[Escaping spline]
\label{ex:escaping spline}\citep[\S 5.4]{DBLP:journals/pacmpl/McIverMKK18}
\[
\underbrace{\big
(\tY \, f \, x . \,
0 \oplus_{\frac{1}{x+1}} f(x + 1) \big)}_{\Xi} 
\, 10
\]

In this case, the fact that the ranking function must decrease at each $\tY$-step (in an antitone partial ranking function) by the expected value of $\epsilon$ not at the current term, but at the next term where the ranking function is defined, is a little harder to deal with, because the variable $x$ can change all the way to $0$ in one step, therefore simply adding a small offset doesn't suffice to compensate for this fact.

Consider the candidate ranking function $\Xi \, n \mapsto n + 1$. For each $n$, $\Xi \, n$ reduces to either $0$ (with probability $\frac 1 {n + 1}$) or $\Xi \, (n + 1)$, therefore the expected value of the ranking function is $\frac{n(n+2)}{n+1} = n + 1 - \frac 1 {(n+1)^2}$, and the required decrease is $\frac{\epsilon(0) + n \epsilon(n + 2)}{n + 1}$, therefore eventually, the expected decrease isn't enough, whatever the value of $\epsilon(0)$ is.

If instead, we take the ranking function to be defined after the $\tY$-reduction but before the $\tsample$-reduction as well, this can be resolved. Letting $\Theta[n] = \underline 0 \oplus_{\frac 1 n} \Xi \, n$:
\begin{align*}
\Theta[n] &\mapsto n \\
\Xi \, 10 & \mapsto 12.
\end{align*}
For each $n$, $\Theta[n]$ reduces to either $0$ or $\Theta[n+1]$, with a $\tY$-reduction only in the latter case, and the condition that this is an antitone partial ranking function is that $n \geq \frac{(n-1)(n+1)}{n} + \frac{n-1}{n} \epsilon(n+1)$ and $12 \geq 11 + \epsilon(11)$. These are satisfied by setting $\epsilon(x) = \min(1, 1/x)$, therefore this term is AST.
\end{example}

\paragraph{Completeness}
It seems very likely that this method is complete in the case that every reachable term is AST, but we have been unable to actually prove this. It is certainly at least capable of proving the termination of terms which terminate arbitrarily slowly (in the sense that there is no similar limitation to Theorem \ref{thm:rankable implies termination}, which can only prove termination of $\tY$-PAST terms).

The following theorem does not prove completeness, but is suggestive in that direction:
\begin{theorem}
For any stopping time $T$ which is almost surely finite, if $(\calF_n)_n$ is the coarsest filtration to which $T$ is adapted, then there is a supermartingale $(Y_n)_n$ adapted to $(\calF_n)_n$ and an antitone function $\epsilon$ such that $(Y_n)_n$ is an antitone ranking supermartingale with respect to $T$ and $\epsilon$.
\end{theorem}

\input{relativised-completeness}

\input{y-stopping-time}

\section{Confluent Trace Semantics}
When proving almost sure termination in this way, it is necessary to consider which terms a given term may reduce to. Sometimes however, the reduction that the programmer has in mind may not be strictly the call-by-value order defined so far, or considering an alternative reduction order may be simpler or more intuitive.

Non-probabilistic lambda calculi generally have the Church-Rosser property, that if a term $A$ reduces to both $B_1$ and $B_2$, there is some $C$ 
%with reduction sequences $B_1 \to^* C$ and $B_2 \to^* C$, 
to which both $B_1$ and $B_2$ reduce,
so the reduction order mostly doesn't matter. 
In the probabilistic case, this may not be true, because $\beta$-reduction can duplicate $\tsample$s, so the outputs of the copies of the sample may be identical or independent, depending on whether the sample is taken before or after $\beta$-reduction. 
There are, however, some restricted variations on the reduction order that do not have this problem.

\paragraph{}
Even with this restriction, a sampling semantics in the style of the one already defined would not be entirely Church-Rosser, as, for example, $\red^3(\tsample - \tsample, (1,0,\dots))$ would be either $1$ or $-1$ depending on the order of evaluation of the $\tsample$s, as that determines which sample from the pre-selected sequence is used for each one. To fix this, rather than pre-selecting samples according to the order they'll be drawn in, select them according to the position in the term where they'll be used instead.

A \emph{position} 
%\akr{I'm not really sure how to phrase the citation, but this is adapted from \cite{Kennaway96infinitarylambda}.} 
%\lo{The idea is common enough. I think no citation necessary.}
is a finite sequence of steps into a term, defined inductively as
\begin{align*}
\alpha ::= \cdot \mid \lambda ; \alpha \mid @_1 ; \alpha \mid @_2 ; \alpha \mid \underline f_i ; \alpha \mid \tY ; \alpha \mid \textsf{if}_1 ; \alpha \mid \textsf{if}_2 ; \alpha \mid \textsf{if}_3 ; \alpha \mid \tscore ; \alpha.
\end{align*}
The \emph{subterm of $M$ at a position $\alpha$}, denoted $M \mid \alpha$, is defined as
\begin{align*}
M \mid \cdot & = M \\
\lambda x. M \mid \lambda ; \alpha & = M \mid \alpha \\
M_1 M_2 \mid @_i ; \alpha & = M_i \mid \alpha \quad \text{for } i = 1,2 \\
\underline f(M_1,\dots,M_n) \mid \underline f_i ; \alpha & = M_i \mid \alpha \quad \text{for }i \leq n \\
\tY M \mid \tY ; \alpha & = M \mid \alpha \\
\tif{M_1 < 0}{M_2}{M_3} \mid \textsf{if}_i ; \alpha & = M_i \mid \alpha \quad \text{for } i = 1,2,3 \\
\tscore(M) \mid \tscore ; \alpha & = M \mid \alpha
\end{align*}
so that every subterm is located at a unique position, but not every position corresponds to a subterm (e.g. $x \, y \mid \lambda$ is undefined). 
A position such that $M\mid \alpha$ does exist is said to \emph{occur} in $M$. 
%\changed[lo]{
\emph{Substitution} of $N$ at position $\alpha$ in $M$, written $M[N/\alpha]$, is defined similarly.
For example, let 
\[
M = \lambda x \, y. y \, (\tif{x < 0}{y \, (\underline f (x))}{\underline 3})
\qquad
\alpha =\lambda ; \lambda ; @_2 ; \textsf{if}_2 ; @_2
\]
then 
\(
M[\tsample / \alpha] = \lambda x \, y. y \, (\tif{x < 0}{y \, \tsample}{\underline 3}).
\)
%}

Two subterms $N_1$ and $N_2$ of a term $M$, corresponding to positions $\alpha_1$ and $\alpha_2$, can overlap in a few different ways. 
If $\alpha_1$ is an initial segment (i.e.~prefix) of $\alpha_2$ (written as $\alpha_1 \leq \alpha_2$), then $N_2$ is also a subterm of $N_1$. If neither $\alpha_1 \leq \alpha_2$ nor $\alpha_1 \geq \alpha_2$, the positions are said to be \emph{disjoint}. 
The notion of disjointness is mostly relevant in that if $\alpha_1$ and $\alpha_2$ are disjoint, performing a substitution at $\alpha_1$ will leave the subterm at $\alpha_2$ inaffected.
\lo{The preceding sentence is unclear. Typo?} \akr{It wasn't a typo, but I've made it a little more explicit now. Is this clearer?} 

With this notation, a more general reduction relation $\to$ can be defined. \akr{It would almost work to have one of the neater versions of $\tY$-reduction here.}
%\lo{On first reading, I struggled to parse the following clauses.}
\begin{definition}
\label{def:more general red}
The binary relation $\to$ is defined by the following rules, each is conditional on a redex occurring at position $\alpha$ in the term $M$:
\begin{align*}
  \text{if } M \mid \alpha = (\lambda x.N) V,\ & M \to M[N[V/x]/\alpha] \\
  \text{if } M \mid \alpha = \underline f (\underline r_1, \dots , \underline r_n),\ & M \to M[\underline{f(r_1,\dots,r_n)}/\alpha] \\
  \text{if } M \mid \alpha = \tY \lambda x. N,\ & M \to M[\lambda z. N[(\tY \lambda x. N)/x] z/\alpha] \text{ where $z$ is not free in $N$}\\
  \text{if } M \mid \alpha = \tif{\underline r < 0}{N_1}{N_2},\ & M \to M[N_1/\alpha] \text{ where }r < 0 \\
  \text{if } M \mid \alpha = \tif{\underline r < 0}{N_1}{N_2},\ & M \to M[N_2/\alpha] \text{ where }r \geq 0 \\
  \text{if } M \mid \alpha = \tsample & \text{ and $\lambda$ does not occur after $@_2$ or $\tY$ in $\alpha$},\\ & M \to M[\underline r/\alpha] \text{ where } r \in [0,1] \\
  \text{if } M \mid \alpha = \tscore(\underline r),\ & M \to M[\underline r/\alpha].
\end{align*}
In each of these cases, $M \mid \alpha$ is the \emph{redex}, and reduction takes place at $\alpha$.
\end{definition}

\paragraph{}
Labelling the pre-chosen samples by the positions in the term would also not work because in some cases, a $\tsample$ will be duplicated before being reduced, for example, in $(\lambda x. x \, {\underline 0} \, \mathbin{\underline{+}} \, {x \, \underline 0}) (\lambda y. \tsample)$, both of the $\tsample$ redexes that eventually occur originate at $@_2 ; \lambda$. 
It is therefore necessary to consider possible positions that may occur in other terms reachable from the original term. 
Even this is itself inadequate because some of the positions in different reachable terms need to be considered the same, and the number of reachable terms is in general uncountable, which leads to measure-theoretic issues.

Define a \emph{skeleton} to be a term but, instead of having real constants $\underline r$, it has a placeholder $\skeletonPlaceholder$, so that each term $M$ has a skeleton $Sk(M)$, and each skeleton $S$ can be converted to a term $S[r]$ given a vector $r$ of $n$ real numbers to substitute in, where $n$ is the number of occurrences of $\skeletonPlaceholder$ in $S$. \lo{Do we actually use the notation $S[r]$ in what follows?} \akr{Yes, in Lemma \ref{lem:independentSamples} for example.}
\lo{Skeleton is defined in Sec.~\ref{sec:sampling semantics}.}
The positions in a skeleton and the reduction relation on skeletons can be extended from the definitions on terms in the obvious way, with $\tif{\skeletonPlaceholder < 0}{A}{B}$ reducing nondeterministically to both $A$ and $B$, and $\tsample$ reducing to $\skeletonPlaceholder$.
For example, we have 
\(
(\lambda x . \tif{x < 0}{x}{\skeletonPlaceholder}) \, \tsample
\to
(\lambda x . \tif{x < 0}{x}{\skeletonPlaceholder}) \, \skeletonPlaceholder
\to
\tif{\skeletonPlaceholder < 0}{\skeletonPlaceholder}{\skeletonPlaceholder}
\to
\skeletonPlaceholder
\)

Given a closed term $M$, let $L_0(M)$ be the set of pairs, the first element of which is a $\to$-reduction sequence of skeletons starting at $Sk(M)$, and the second of which is a position in the final skeleton of the reduction sequence. As with the traces from $I^{\mathbb N}$ used to pre-select samples to use in the usual linear sampling semantics, there will be (with one more caveat introduced later) a sample from $I$ pre-selected for each element of $L_0(M)$, which will then be used if a $\tsample$ reduction is ever performed at that position.

Reduction sequences are used rather than reachable skeletons because if the same skeleton is reached twice, different samples may be needed. For example, consider the term $M = \tY (\lambda f x.\ \tif{\tsample - \underline{0.5} < 0}{f \, x}{x}) \, \underline 0$, which reduces after a few steps to $N = \tif{\tsample - \underline{0.5}  < 0}{M}{\underline 0}$. If the pre-selected sample for $(N,\textsf{if}_1;\underline{-}_1)$ is less than 0.5, $N$ reduces back to $M$, then $N$ again, then the same sample is used the next time, therefore it's an infinite loop, whereas if samples are labelled by reduction sequences, the samples for $M \to \dots \to N$ are independent from the samples for $M \to \dots \to N \to M \to \dots \to N$, and so on.

The reduction sequences of skeletons will often be discussed as though they were just skeletons, identifying them with their final skeletons. With this abuse of notation, a reduction sequence $N$ (actually $N_1 \to \dots \to N_n = N$) may be said to reduce to a reduction sequence $O$, where the reduction sequence implicitly associated with the final skeleton $O$ is $N_1 \to \dots \to N_n \to O$.

\paragraph{}
This is still not quite sufficient because sometimes the same samples must be used at corresponding positions in different reduction sequences. For example, the term $M = \tsample + \tsample$ has the reachable skeletons $N_1 = \skeletonPlaceholder + \tsample, N_2 = \tsample + \skeletonPlaceholder, O = \skeletonPlaceholder + \skeletonPlaceholder$ and $\skeletonPlaceholder$, with reductions $M \to N_1 \to O \to \skeletonPlaceholder$ and $M \to N_2 \to O \to \skeletonPlaceholder$. In the reduction $M \to N_1$, the sample labelled $(M, \underline{+}_2)$ is used, and in the reduction $N_2 \to O$, the sample labelled $(M \to N_2, \underline{+}_2)$ is used. Each of these samples becomes the value of the second numeral in $O$ in their respective reduction sequences, therefore in order for Church-Rosserness to be attained, they must be the same. Which elements of $L_0(M)$ must match can be described by the relation $\sim^*$:

\begin{definition}
The relation $\sim$ is defined as the union of the minimal symmetric relations $\sim_p$ (``$p$'' for parent-child) and $\sim_c$ (``$c$'' for cousin) satisfying
\begin{enumerate}
    \item If $N$ reduces to $O$ with the redex at position $\alpha$, and $\beta$ is a position in $N$ disjoint from $\alpha$, then $(N,\beta) \sim_p (O,\beta)$.
    
    \item If $N$ $\beta$-reduces to $O$ at position $\alpha$, $\beta$ is a position in $N \mid \alpha;@_1;\lambda$ and $N \mid \alpha;@_1;\lambda;\beta$ is not the variable involved in the reduction, $(N,\alpha;@_1;\lambda;\beta) \sim_p (O, \alpha;\beta)$
    
    \item If $N$ $\textsf{if}$-reduces to $O$ at position $\alpha$, with the first resp. second branch being taken, and $\alpha;\textsf{if}_i;\beta$ occurs in $N$ (where $i = 2$ resp. $3$), $(N,\alpha;\textsf{if}_i;\beta) \sim_p (O,\alpha;\beta)$
    
    \item If $N$, $O_1$ and $O_2$ match any of the following cases:
    \begin{enumerate}
        \item $N$ contains redexes at disjoint positions $\alpha_1$ and $\alpha_2$, $O_1$ is $N$ reduced first at $\alpha_1$ then $\alpha_2$ and $O_2$ is $N$ reduced first at $\alpha_2$ then at $\alpha_1$.
        
        \item $N \mid \alpha = \tif{\underline r < 0}{N_1}{N_2}$, where $r < 0$ (or, respectively, $r \geq 0$), $(N_2 \text{ resp. } N_1) \mid \beta$ is a redex, and $O_1$ is $N$ reduced at $\alpha$ and $O_2$ is $N$ reduced first at $\alpha;(\textsf{if}_3 \text{ resp. } \textsf{if}_2);\beta$ then at $\alpha$
        
        \item $N \mid \alpha = \tif{\underline r < 0}{N_1}{N_2}$, where $r < 0$ (or, respectively, $r \geq 0$), $(N_1 \text{ resp. } N_2) \mid \beta$ is a redex, and $O_1$ is $N$ reduced first at $\alpha$ then at $\alpha;\beta$ and $O_2$ is $N$ reduced first at $\alpha;(\textsf{if}_2 \text{ resp. } \textsf{if}_3);\beta$ then at $\alpha$
        
        \item $N \mid \alpha = (\lambda x. A) B$, there is a redex in $A$ at position $\beta$, $O_1$ is $N$ reduced first at $\alpha$ then at $\alpha;\beta$, and $O_2$ is $N$ reduced first at $\alpha;@_1;\lambda;\beta$ then at $\alpha$
        
        \item $N \mid \alpha = (\lambda x. A) B$, $B \mid \beta$ is a redex, $(\gamma_i)_i$ is a list of all the positions in $A$ where $A \mid \gamma = x$, ordered from left to right, $O_1$ is $N$ reduced first at $\alpha ; @_2 ; \beta$ then at $\alpha$, and $O_2$ is $N$ reduced first at $\alpha$ then at $\alpha;\gamma_i;\beta$ for each $i$ in order.
        
        \item $N \mid \alpha = \tY (\lambda x. A)$, $A$ reduced at $\beta$ is $A'$, $(\gamma_i)_i$ is a list of all the positions where $A' \mid \gamma  = x$, ordered from left to right, $O_1$ is $N$ reduced first at $\alpha;\tY;\lambda;\beta$ then at $\alpha$, and $O_2$ is $N$ reduced first at $\alpha$ then at $\alpha;\lambda;@_1;\gamma_i;Y;\lambda;\beta$ for each $i$ in order where $\gamma_i$ is left of $\beta$ then at $\alpha;\lambda;@_1;\beta$ then at $\alpha;\lambda;@_1;\gamma_i;Y;\lambda;\beta$ for the remaining values of $i$.
    \end{enumerate}
    (in which case $O_1$ and $O_2$ are equal as skeletons, but with different reduction sequences), $O_1'$ and $O_2'$ are the results of applying some reduction sequence to each of $O_1$ and $O_2$ (the same reductions in each case, which is always possible because they're equal terms), and $\delta$ is a position in $O_1'$ (or equivalently $O_2'$), then $(O_1',\delta) \sim_c (O_2',\delta)$.
\end{enumerate}
The $\sim_c$-rules are illustrated in the following diagrams.
\end{definition}


\include{sim-diagram}

The reflexive transitive closure $\sim^*$ of this relation is used to define the set of \emph{potential positions} $L(M) = L_0(M) / \sim^*$, and each equivalence class can be considered as the same position as it may occur across multiple reachable skeletons. 
If $(N,\alpha) \sim^* (O,\beta)$, then $N \mid \alpha$ and $O \mid \beta$ both have the same shape (i.e.~they're either both the placeholder $\skeletonPlaceholder$, both variables, both applications, both $\tsample$s etc.), therefore it's well-defined to talk of the set of potential positions where there is a $\tsample$, $L_s(M)$. 
Formally $L_s(M) := \{[(X, \alpha)]_{\sim^\ast_M} : X \mid \alpha = \tsample\}$.
The new sample space is then defined as $I^{L_s(M)}$, with the Borel $\sigma$-algebra and product measure.
\changed[lo]{Since $I^{L_s(M)}$ is a countable product, the measure space is well-defined \citep[Cor.~2.7.3]{AshDD00}.}

\paragraph{}
Although the six cases in the definition of $\sim_c$ are defined separately, there are some commonalities worth noting. Each case (except a, which is symmetrical) has a right branch and a left branch (the branches of case b are unfortunately displayed the wrong way around in the diagram). The right branch has reduction steps at two positions, $\beta$ then $\alpha$, where $\beta > \alpha$ (i.e.~$\beta$ is inside $\alpha$), and in the left branch, the order of these reductions are swapped, so the reductions are at $\alpha$ then $\beta_1, \dots, \beta_n$. The position $\alpha$ of one of the reductions is unchanged, and the other reduction may have multiple (in cases e and f) or 0 (in cases b and e) images in the left branch. The images $(\beta_i)$ are all still inside (greater than or equal to) $\alpha$, and although they are not generally equal to $\beta$, the subskeletons at these positions have a similar shape to the subskeleton (initially) at position $\beta$, so that the reductions at these positions are the same type of reduction (e.g.~both $\beta$-reduction or both $\textsf{if}$-reduction). Case a is also similar, except that the relevant positions are all disjoint rather than contained in one another, and either branch could be considered the right branch.

\iffalse
\changed[lo]{Since $L_s(M)$ is a countable set, there is a unique probability measure $\mu$ on the infinite product $\prod_{i \in L_s(M)} \Sigma_I$ of the Borel $\sigma$-algebras $\Sigma_I$ satisfying
\[
\mu\set{\omega \in \Omega \mid \omega_1 \in A_1, \cdots, \omega_n \in A_n} = \prod_{i=1}^n \Leb(A_i)
\]
where $\Leb$ is the Lebesgue measure on the measurable space $(I, \Sigma_I)$.}
\fi

\paragraph{}
Before defining the new version of the reduction relation $\red$, a few lemmas about the properties of $\sim$ are necessary for it to be well-defined.

\begin{lemma} \label{downAcrossUp}
If $(N_1,\alpha_1) \sim^* (N_2,\alpha_2)$, there is some descendants $N'_1, N'_2$ of $N_1$ resp.~$N_2$, and a position $\alpha'$ in both of them such that $(N_1,\alpha_1) \sim_p^* (N'_1, \alpha') \sim_c^* (N'_2, \alpha') \sim_p^* (N_2, \alpha_2)$.
\end{lemma}
\begin{proof}
  The $\sim_p$ relation can be split into $\sim_\downarrow \cup \sim_\uparrow$, where $(A,\alpha) \sim_\downarrow (B,\beta)$ if $A \to B$ and $(A,\alpha) \sim_p (B,\beta)$, and similarly $(A,\alpha) \sim_\uparrow (B,\beta)$ if $B \to A$ and $(A,\alpha) \sim_p (B,\beta)$. At each stage of this proof, it will be assumed that the $\sim_c$ steps and the $\sim_p$ steps in the sequence from $(N_1,\alpha_1)$ to $(N_2,\alpha_2)$ are rearranged such that there is never a $\sim_c$ immediately before a $\sim_\downarrow$, and there is never a $\sim_c$ immediately after a $\sim_\uparrow$. This rearrangement is always possible, because if there is some subsequence $(A,\alpha) \sim_c (B,\alpha) \sim_\downarrow (C,\beta)$, then there is an alternate path $(A,\alpha) \sim_\downarrow (B',\beta) \sim_c (C,\beta)$, where the reduction and the $\sim_\downarrow$ from $A$ to $B'$ are the same as those from $B$ to $C$, and the $\sim_c$ step is the same, but with the reduction sequences $O_1 \to^* O_1'$ and $O_2 \to^* O_2'$ in the definition of $\sim_c$ extended by the reduction $B \to C$. With these rearrangements assumed, it is sufficient to prove that the $\sim_p$ steps can be rearranged so that all of the $\sim_\downarrow$ steps come before all of the $\sim_\uparrow$ steps (possibly introducing some $\sim_c$ steps in the process).

  The rearrangement to put all of the $\sim_\downarrow$s before all of the $\sim_\uparrow$s proceeds by induction on the number of $\sim_\downarrow$s that occur after the first occurrence of $\sim_\uparrow$. If it's 0, all of the $\sim_p$s are already in the correct order, and we're done. Otherwise, take the subsequence from the first $\sim_\uparrow$ to the first $\sim_\downarrow$ after that. This must be rearranged to some $\sim_\downarrow$s followed by some $\sim_\uparrow$s, then once that's done, the number of $\sim_\downarrow$s after the first $\sim_\uparrow$ in the overall sequence will have decreased by 1.

  Let this subsequence be $A \sim_\uparrow^n B \sim_\downarrow C$. Suppose for induction that $A \sim_\uparrow^k (D,\delta) \sim_\downarrow^* (E,\epsilon) \sim_\uparrow^* C$ for some $D, \delta, E, \epsilon$, and $0 \leq k \leq n$, where in the reduction sequence $D \to^* E$, for any pair of reductions at positions which aren't disjoint, the reduction at the innermost (greater) position occurs first. Reduction sequences with this property will be called ``parallel'', as it is analagous to parallel reduction as in \akr{TODO: find this reference, for the proof of the Church-Rosser theorem by Tait and Martin-L\"of}. This induction is in reverse, with $k$ decreasing from $n$ to $0$. If $k = n$, simply set $(D,\delta) = B$ and $(E,\epsilon) = C$. If $k = 0$, then $A$ is related to $B$ by a sequence of $\sim_\downarrow$s then $\sim_\uparrow$s, as desired. In the intermediate steps, $k$ must be decreased by one, so a subsequence of one $\sim_\uparrow$ followed by a parallel sequence of $\sim_\downarrow$s must be replaced by a parallel sequence of $\sim_\downarrow$s followed by some $\sim_\uparrow$s.

  For any individual $\sim_\uparrow$ then $\sim_\downarrow$ pair, let $(A,\alpha) \sim_\uparrow (B,\beta) \sim_\downarrow (C,\gamma)$. The skeleton $B$ reduces to both $A$ and $C$. If these are the same reduction, then $(A,\alpha) = (C,\gamma)$ and this subsequence may be removed. If they are different, then the way they overlap corresponds to one of the cases in the definition of $\sim_c$, either case a if the reduction positions are disjoint, or one of the cases b--f if one of the reduction positions is inside the other. In any case, there is a reduction sequence from each of $A$ and $C$ to some common skeleton $D$ ($O_1$ and $O_2$ in the definition of $\sim_c$). For the same reason that $(B,\beta) \sim_\downarrow (C,\gamma)$, there is some $\delta$ such that $(A,\alpha) \sim_\downarrow^* (D,\delta)$, and similarly for the same reason that $(B,\beta) \sim_\downarrow (A,\alpha)$, for the same $\delta$, $(C,\gamma) \sim_\downarrow^* (D,\delta)$. There are a lot of cases to check for this statement, but all of them are rather simple, and similar to each other. There is therefore an alternative $\sim^*$ sequence from $(A,\alpha)$ to $(C,\gamma)$, of the form $(A,\alpha) \sim_\downarrow^* (D_1,\delta) \sim_c (D_2,\delta) \sim_\uparrow^* (C,\gamma)$, where $D_1$ and $D_2$ are the alternative reduction sequences leading to the same skeleton $D$. There are only multiple $\sim_\downarrow$ steps in the result if the position of the reduction $B \to C$ is inside the position of the reduction $B \to A$, and similarly, there are only multiple $\sim_\uparrow$ steps in the result if the position of the reduction $B \to A$ is inside the position of the reduction $B \to C$. If there are multiple $\sim_\downarrow$ steps, they are not necessarily parallel, but in the only case where they aren't (case f of $sim_c$), the subsequence of $\sim_\downarrow$ steps with the reductions at $\alpha;\lambda;@_1;\beta$ then $\alpha;\lambda;@_1;\gamma_i;\tY;\lambda;\beta$ for each $i$ where $\gamma_i \geq \beta$ (in the notation of the definition of $\sim_c$) can be replaced by $\sim_\downarrow$s with the reductions at $\alpha;\lambda;@_1;\gamma'_i;\tY;\lambda;\beta$ then $\alpha;\lambda;@_1;\beta$, then another $\sim_c$, where $(\gamma'_i)$ is the list of positions in $A$ (\emph{before} the reduction at $\beta$ to $A'$, unlike $(\gamma_i)$) where $A | \gamma'_i = x$ and $\gamma'_i > \beta$.

  The effect of this rearrangement is (ignoring the $\sim_c$s) to swap the order of the $\sim_\uparrow$ and the $\sim_\downarrow$, except that if the positions of one of these reductions is inside the other, that inner $\sim_p$ may be duplicated. If the positions are disjoint, both of them are unchanged (corresponding to case a of $\sim_c$), and if one is inside the other, then the outer position is unchanged and the other resultant positions, although they aren't equal to the original inner position, are still inside the outer position. Taking the sub-sequence of one $\sim_\uparrow$ followed by a parallel sequence of $\sim_\downarrow$s, this rearrangement can be repeatedly applied to move the $\sim_\uparrow$ further along the sequence. If at some point a $\sim_\uparrow$ and $\sim_\downarrow$ match (by relating the same (skeleton, position) pairs in opposite directions), they may be removed and the process may stop early. The $\sim_\uparrow$ may be duplicated if it passes a $\sim_\downarrow$ whose position is outside of the $\sim_\uparrow$'s position, but by the assumption that the sequence of $\sim_\downarrow$s is initially parallel, no position inside of this occurs later in the sequence, therefore all of the resultant $\sim_\uparrow$s pass the remaining $\sim_\downarrow$s without changing or duplicating them. The $\sim_\uparrow$s may be further duplicated, but the number of steps left in this process is bounded by the product, for all of the remaining $\sim_\downarrow$s, of $2 + \text{the number of occurrences of the variable relevant to the reduction}$, because for each $\sim_\downarrow$, that is more than the number of duplicates it could produce for any $\sim_\uparrow$ that passes it. It is also possible (earlier) for some of the $\sim_\downarrow$s to be duplicated, but the only case where this process would not terminate, that both the $\sim_\downarrow$s and the $\sim_\uparrow$s continue being duplicated forever so that the number of switches left to do never decreases, is prevented by the parallelness condition, as all of the duplications of $\sim_\downarrow$s occur before all of the duplications of $\sim_\uparrow$s.

  It still remains to be shown that the sequence of $\sim_\downarrow$s left at the end of this can be made parallel. The changes that may have occurred since the previous version of this sequence (which was known to be parrallel already), are that some of the $\sim_\downarrow$s may have been duplicated by passing the $\sim_\uparrow$, and if that $\sim_\uparrow$ matches one of the $\sim_\downarrow$s, that $\sim_\downarrow$ is removed. The position of the $\sim_\uparrow$'s reduction is the same for all of these duplications, because it is not changed until later in the sequence, when the $\sim_\downarrow$s with positions outside of that may occur. The resultant positions after a duplication are in the same position relative to each other, and therefore the order of the remaining $\sim_\downarrow$s is automatically parallel, except that (letting the position of the $\sim_\uparrow$'s reduction be $\alpha$) if there are $\sim_\downarrow$s with reductions at positions $\alpha;@_1;\lambda;\beta$ and $\alpha;@_2;\gamma$, or $\alpha;\tY;\lambda;\beta$ and $\alpha;\tY;\lambda;\gamma$, some of the resultant $\sim_\downarrow$s' positions may be inside each other where they were originally not. Similarly to the case where a $\tY$ reduction and a reduction at a position inside it are swapped though, the order can be fixed by swapping some of the $\sim_\downarrow$ steps with each other. In the first case, the reduction at position $\alpha;@_1;\lambda;\beta$ ends up at $\alpha;\beta$, and the reduction at position $\alpha;@_2;\gamma$ ends up at the positions $\alpha;\delta_i;\gamma$, where $(\delta_i)$ is the positions of the variable relevant to the $\sim_\uparrow$'s reduction inside its lambda. The positions of the variable in the lambda may be different before the reduction at $\beta$, but the positions of the redexes corresponding to the original redex at $\alpha;@_2;\gamma$ are changed similarly. Rather than reducing at $\alpha;\beta$ then at $\alpha;\delta_i;\gamma$, it is therefore possible to reach the same point by reducing at $\alpha;\delta'_i;\gamma$ then at $\alpha;\beta$, where $(\delta'_i)$ is the positions of the relevant variable within the lambda before the reduction at $\alpha;@_1;\lambda;\beta$ (and again, a $\sim_c$ of some sort must also be introduced). The other case, that the $\sim_\uparrow$'s reduction is a $\tY$-reduction, is similar. Each reduction which is duplicated has one image at $\alpha;\lambda;@_1;$its original relative position, and one for each occurrence of the variable. Those corresponding to variable occurrences may need to be moved to before those at $\lambda;@_1$.

  In summary, to reach the desired order of $\sim_p$ steps, each $\sim_\downarrow$ may have to be moved past some $\sim_\uparrow$ steps, and it may get duplicated in the process, but the resultant sequence of $\sim_\downarrow$s can all be put in parallel order, and it has been shown that moving a $\sim_\uparrow$ past a sequence of $\sim_\downarrow$s is always possible if they're in parallel order, therefore the overall process terminates and reaches a point where all of the $\sim_\downarrow$s precede all of the $\sim_\uparrow$s, and all of the $\sim_c$s are in between them. All of these rearrangements leave the end points of the sequence unchanged, therefore this is an alternative sequence of $\sim$ steps between $(N_1,\alpha_1)$ and $(N_2,\alpha_2)$ of the form $(N_1,\alpha_1) \sim_\downarrow^* (N_1', \alpha') \sim_c^* (N_2', \alpha') \sim_\uparrow^* (N_2,\alpha_2)$.
\end{proof}


If $(X,\alpha) \sim_c (Y,\alpha)$ then the final skeletons in $X$ and $Y$ are equal, and $(X,\beta) \sim_c (Y,\beta)$ for any $\beta$ that occurs in this skeleton, therefore $\sim_c$ can be considered to apply to reduction sequences even without positions, $X \sim_c Y$ iff $(X,\cdot) \sim_c (Y,\cdot)$. The structure of $\sim_c$ can be seen more clearly by choosing a canonical example from each equivalence class. For this, the member of the class that's in call-by-value order is used. A reduction sequence is in call-by-value order if for every pair of reductions in the sequence at positions $\alpha$ and $\beta$, where the reduction at $\alpha$ happens first, either $\alpha \leq \beta$ or $\alpha$ and $\beta$ are disjoint, with $\alpha$ to the left of $\beta$ (i.e.~the first elements of the sequences $\alpha, \beta$ where they differ are $@_1$ and $@_2$, $\underline f_i$ and $\underline f_j$ for $i < j$, or $\textsf{if}_i$ and $\textsf{if}_j$ for $i < j$, in that order). Note that although the reductions that occur in the sequence are in call-by-value order, it may not be the actual call-by-value reduction sequence starting from that point because some redexes may remain un-reduced even when other reductions that should happen afterwards occur. It is only required that the reductions that do occur occur in the correct order.

\begin{lemma} \label{canonicalCousins}
For any reduction sequence $X$, there is a unique reduction sequence $X_{cbv}$ that is related to $X$ by $\sim_c$ and is in call-by-value order, and if $X \sim_c^* Y$, then $X_{cbv} = Y_{cbv}$.
\end{lemma}

\lo{As defined, $\sim_c$ is not symmetric.}

\begin{proof}

Define $X_n$ recursively so that $X_0 = X$ and if $X_n$ is not in CBV order, take the last pair of reductions in $X_n$ which aren't in CBV order. They are adjacent in the sequence, as the CBV order is a total order. Let the positions of these reductions be $\alpha$ and $\beta$ respecively. If they are disjoint, they can be considered as one of the branches of $\sim_c$ case a. Otherwise, $\beta < \alpha$, and the sub-skeleton at position $\beta$ at the appropriate point in the reduction sequence is a redex with non-trivial subterms, therefore it's of one of the forms $\tif{\skeletonPlaceholder>0}{A}{B}$, $(\lambda x.A) B$ or $\tY \lambda x. A$, therefore the reductions $\alpha$ and $\beta$ form the $O_2$ branch of one of the cases b, c, d, e or f of $\sim_c$. In either case, define $X_{n+1}$ as equal to $X_n$ except taking the other (left) branch, so that the order of the reductions at $\alpha$ and $\beta$ are switched (although the equivalent(s) of the reduction at $\alpha$ may not actually be at that position).

This sequence of swaps rearranges the reductions in $X$ rather like insertion-sort (but in some cases duplicating or deleting the new element), and eventually reaches some $X_n$ that is in CBV order. This is defined to be $X_{cbv}$.

\paragraph{}
If $Y \sim_c X \sim_c^* X_{cbv}$, and the reductions involved in $Y \sim_c X$ are not the last pair of reductions in $Y$ that aren't in CBV order, the order of the $\sim_c$s can be rearranged so that the sequence $Y \sim_c^* X_{cbv}$ is the canonical sequence given above, therefore $Y_{cbv} = X_{cbv}$. To be more precise about this rearrangement, consider the sequence of $\sim_c$ steps from $Y$ to $X_{cbv}$. The first step takes some sub-sequence of the reduction sequence $Y$ and swaps the order of those reductions. If $X = X_{cbv}$, then $X = Y_1 = Y_{cbv}$ already. Otherwise, consider the cases according to whether the image of this subsequence in $X$ occurs earlier than the next pair of reductions to be swapped (i.e.~the last pair of reductions in $X$ that occur not in CBV order). If not, then either the step $Y \sim_c X$ changes this subsequence into CBV order, or out of it. If it is into, then it was the last out-of-order pair in $Y$ therefore the whole sequence $Y \sim_c^* X_{cbv}$ is already in the canonical order and $Y_{cbv} = X_{cbv}$. If it switches the pair to the wrong order, then the result of that switch is the last out-of-order pair in $X$ therefore $Y = X_1$, and the sequence $X_1 \sim_c^* X_{cbv}$ is just a suffix of the sequence $X \sim_c^* X_{cbv}$ therefore $(X_1)_{cbv} = X_{cbv}$ and again $Y_{cbv} = X_{cbv}$.

The remaining cases are when the subsequence of reduction steps involved in $Y \sim_c X$ occur before the last out-of-order pair in $X$, but possibly overlapping. If there is no overlap, then the $\sim_c$ steps do not interfere with each other at all, and can simply be performed in the other order. This gives a different sequence of $\sim_c$ steps $Y \sim_c Y_1 \sim_c X_1 \sim_c^* X_{cbv}$. As $Y_{cbv} = (Y_1)_{cbv}$, we can proceed by induction in this case and use the fact that $(Y_1)_{cbv} = (X_1)_{cbv} = X_{cbv}$ to acheive the result.

In the other case, there is some overlap between the regions involved in $Y \sim_c X$ and $X \sim_c X_1$, but the last reduction in $X \sim_c X_1$ is later than the last reduction in $Y \sim_c X$. There are only 2 reductions in $X$ involved in $X \sim_c X_1$, therefore the region involved in $Y \sim_c X$ ends on the first of the reductions involved in $X \sim_c X_1$. Now consider the specific sequence of positions of the reductions in $X$ involved in either of these steps. Let the last of these positions be $\alpha$, the first $\gamma$, and the second-last, third-last and so on $\beta_1, \beta_2, \dots$ respectively, and let the positions of the redexes in $Y$ involved in $Y \sim_c X$ be $\beta$ then $(\gamma_i)$ in order.. Then $X \sim_c X_1$ swaps $\beta_1$ and $\alpha$, and $\alpha$ comes before $\beta_1$ in CBV order (i.e.~$\alpha < \beta_1$ or $\alpha$ is left of $\beta_1$), and the step $Y \sim_c X$ either proceeds forwards towards CBV order, swapping $\beta$ below $\gamma_1 = \gamma$ to form $\gamma$ then $(\beta_i)$, or proceeds backwards, taking $Y$ even farther from CBV order, swapping the $(\gamma_i)$ to above $\beta$, forming $\gamma$ then $\beta_1 = \beta$. In each of the cases below, it is required to show that the canonical sequence $(X_i)_i$ eventually reaches some term which is the same as one in $(Y_j)_j$, and from that point on, they match, therefore they reach the same end result: $X_{cbv} = Y_{cbv}$.

\begin{itemize}
  \item $Y \sim_c X$ in the backwards direction (so that $Y$ is closer to CBV order than $X$ is): By swapping the roles of $X$ and $Y$, and $\beta$ and $\gamma$, this is equivalent to the $Y \sim_c X$ forwards case. The assumption that $\alpha$ is before $\beta_1$ in CBV order (so that $\beta_1, \alpha$ is the pair to be swapped in $X \sim_c X_1$) maps to the assumption that $\alpha$ is before $\gamma_1$ (so that the result doesn't just follow trivially by $Y_1$ being equal to $X$) and vice-versa. Then one of the cases below establishes that $X_i = Y_j$ for some $i$ and $j$, therefore after swapping back, $Y_i = X_j$.
\item $Y \sim_c X$ forwards and $\alpha$ comes after $\gamma$ in CBV order: In this case, the $\gamma$ and $\alpha$ reduction steps are already in the correct order in $Y$, therefore $\beta$ and $\gamma$ are already the last out-of-order pair of reductions in $Y$, $Y_1 = X$, and $Y \sim_c X \sim_c^* X_{cbv}$ is already the canonical sequence from $Y$ therefore $Y_{cbv} = X_{cbv}$.
\item $Y \sim_c X$ forwards and $\alpha$ is disjoint from $\gamma$ and $\beta$: In this case, the sequence $(X_i)_i$ starts by swapping $\alpha$ and $\beta_1$ by case a, then because $\gamma$ is disjoint from $\alpha$ and comes after is in CBV order, $\gamma$ is right of $\alpha$, therefore either all the remaining $\beta_i$s, are $\geq \gamma$ (if $\beta$ and $\gamma$ aren't disjoint) or there are no remaining $\beta_i$s (if $\beta$ and $\gamma$ are disjoint, in which case they swap by case a and there is only $\beta_1$). The sequence $(X_i)_i$ therefore proceeds to swap all the remaining $\beta_i$s then $\gamma$, in order, below $\alpha$, resulting in $X_i$ for some $i$ having as its relevant sub-reduction-sequence $\alpha, \gamma, \beta_k, \dots, \beta_1$. The other sequence $(Y_j)_j$ starts by swapping $\gamma$ then $\beta$ below $\alpha$, then swapping $\gamma$ and $\beta$. Because $\alpha$ is disjoint from $\gamma$, the reduction there does not affect the sub-skeleton at position $\gamma$, therefore this last $\sim_c$ step proceeds identically to how it did in $Y \sim_c X$, resulting in $\gamma, \beta_k, \dots, \beta_i$. Overall, this results in $Y_3$ having the relevant sub-reduction-sequence $\alpha, \gamma, \beta_k, \dots, \beta_1$, therefore $Y_3 = X_i$ for the aforementioned value of $i$.
\item $Y \sim_c X$ forwards, $\alpha \leq \gamma$ and $\alpha$ (and therefore also $\gamma$) is disjoint from $\beta$: In this case, $(X_i)_i$ proceeds by swapping $\beta_1 = \beta$ below $\alpha$ by case a, then swapping $\gamma$ below $\alpha$ by one of the other cases, and $(Y_j)_j$ proceeds by swapping $\gamma$ and $\alpha$, then swapping $\beta$ below $\alpha$ then all of the images of $\gamma$ (which are $\geq \alpha$ and therefore left of $\beta$). In both cases, the subterm at $\alpha$ when $\alpha$ and $\gamma$ are swapped is unaffected by the reduction at $\beta$, therefore it produces the same result in both cases, therefore the overall sequence in both cases is $\alpha$ then the images of $\gamma$ then $\beta$.
\item Both $\gamma$ and $\beta$ are $> \alpha$ but they're disjoint from each other, and either the skeleton at position $\alpha$ is of the form $\tif{\skeletonPlaceholder > 0}{A}{B}$ or it is of the form $A B$ and either both $\gamma$ and $\beta$ are $> \alpha;@_1$ or they are both $> \alpha;@_2$: In this case, $(X_i)_i$ proceeds by swapping $\beta$ then $\gamma$ below $\alpha$, in each case possibly forming 0 or multiple images. The only way the number of images of each of these positions can be different is if the subskeleton at $\alpha$ is an $\textsf{if}$, and there are 0 of one of them and 1 of the other, in which case they're trivially in the correct order already. Otherwise, all of the images of $\beta$ and $\gamma$ are disjoint from one another, therefore none of the positions, or their relative order, change when they are swapped, therefore the next part of the sequence $(X_i)_i$ is just insertion-sort running on the images of $\beta$ and $\gamma$ with $\sim_c$ case a swaps until they're in the correct order. The sequence $(Y_j)_j$ starts by swapping $\gamma$ then $\beta$ below $\alpha$, then as before, sorting the images into the correct order by case a swaps. The images originally formed of $\gamma$ and $\beta$ are the same in both cases, therefore the final order is the same, so $X_i = Y_j$ for some $i$ and $j$.
\item $Y \sim_c X$ forwards, $\alpha;@_1 < \gamma$, and $\alpha;@_2 < \beta$: Let $x$ be the variable involved in the $\beta$-reduction at $\alpha$, so that the sub-skeleton at $\alpha$ is $(\lambda x. A) B$ for some $A, B$, where $\gamma = \alpha;@_1;\lambda;\gamma'$ and $A \to A'$ at $\gamma'$, and $\beta = \alpha;@_2;\beta'$, and $B \to B'$ at $\beta'$. In this case, $(X_i)_i$ starts by swapping $\beta$ below $\alpha$ by case e, producing one image of $\beta$ for each $x$ in $A'$, then swapping $\gamma$ below $\alpha$ by case d, producing a single image $\alpha;\gamma'$. For each of the instances of $x$ in $A$ left of $\gamma'$, the reduction at $\alpha;\gamma'$ is then swapped below the corresponding image of $\beta$. The sequence $(Y_j)_j$ starts by swapping $\gamma$ below $\alpha$ then $\beta$ below $\alpha$, but in this case the images of $\beta$ produced may be different. In this case the swap happens earlier in the reduction sequence than $\gamma$, so that the body of the lambda is still $A$ rather than $A'$, and the reduction at $\gamma$ may rearrange or change the number of instances of $x$. Next, each of the images of $\beta$ to the right of $\alpha;\gamma'$ are swapped with it by case a. At this point, the next few reductions before $\alpha;\gamma'$ will in general be images of $\beta$ at positions inside $\alpha;\gamma'$. The subskeleton at position $\alpha;\gamma'$ before these reductions is $(A|\gamma')[B/x]$, then by the images of $\beta$ this reduces to $(A|\gamma')[B'/x]$, then it reduces at its root position ($\alpha;\gamma'$ in the overall skeleton) to $(A'|\gamma')[B'/x]$. For each position of an $x$ in $(A|\gamma')$, there are 0 or more corresponding positions of $x$ in $(A'|\gamma')$, depending on what type of reduction $A \to A'$ is. Because $B$ and $B'$ cannot contain any instances of the variable involved in the reduction $A \to A'$ (if it's a $\beta$-reduction or $\tY$-reduction), each instance of $B$ in $(A|\gamma')[B/x]$ has the same set of images in $(A'|\gamma')[B/x]$ as the corresponding $x$ in $(A|\gamma')$. When these images of $\beta$ that overlap with the image of $\gamma$ are swapped below it, each of them has its own images, one at each position of $B$ in $(A'|\gamma')[B/x]$ corresponding to its original copy of $B$ in $(A|\gamma')[B/x]$. After all of them are swapped below $\alpha;\gamma'$ (and rearranged among themselved by case a), there is therefore one reduction at each copy of $B$ in $(A'|\gamma')[B/x]$, i.e.~one at position $\beta'$ relative to each $x$ in $(A'|\gamma')$. They are therefore the same as the images of $\beta$ in $X_1$ that overlap with $\alpha;\gamma'$, because those are also one reduction at position $\beta'$ relative to each $x$ in $(A'|\gamma)$. Both $(X_i)_i$ and $(Y_j)_j$ therefore eventually reach a point where the relevant sub-reduction-sequence is $\alpha$, the images of $\beta$ to the left of $\alpha;\gamma'$, $\alpha;\gamma'$ itself, the aforementioned images of $\beta$ that overlap with $\alpha;\gamma'$, then all the images of $\beta$ to the right of $\alpha;\gamma'$.
\item $Y \sim_c X$ forwards, $\gamma$ and $\beta$ are disjoint and both $> \alpha$, and the reduction at $\alpha$ is a $\tY$-reduction: This is similar to the previous case, but more complicated, so it will need some definitions to explain properly what's going on. Let the term at $\alpha$ before any of the reductions be $\tY \lambda x. A$, let $\gamma = \alpha;\tY;\lambda;\gamma'$, let $\beta = \alpha;\tY;\lambda;\beta'$, let $A | \beta' = B \to B'$ with the reduction at $\cdot$, let $A | \gamma' = C \to C'$ with the reduction at $\cdot$, let the positions in $A$ where $x$ occurs that are left of $\gamma'$, between $\gamma'$ and $\beta'$ (but still disjoint from both), and right of $\beta'$ respectively be $(\delta^l_i)$, $(\delta^m_i)$ and $(\delta^r_i)$, let the positions in $B$, $C$, $B'$ and $C'$ respectively where $x$ occurs be $(\delta^B_i)$, $(\delta^C_i)$, $(\delta^{B'}_i)$ and $(\delta^{C'}_i)$ (so that all the positions in $A$ where $x$ occurs, in left-to-right order, are $(\delta^l_i)_i, (\delta^C_i)_i, (\delta^m_i)_i, (\delta^B_i)_i, (\delta^r_i)_i$). The sequence $(X_i)_i$ starts by swapping $\beta$ then $\gamma$ below $\alpha$. With the shorthand that $\gamma'(\epsilon) = \alpha;\lambda;@_1;\epsilon;\tY;\lambda;\gamma'$, $\gamma'_0 = \alpha;\lambda;@_1;\gamma'$, and similarly for $\beta'$, the relevant portion of $X_2$ is then $\alpha,\ (\gamma'(\delta^l_i))_i,\ \gamma'_0,\ (\gamma'(\delta^{C'}_i))_i,\ (\gamma'(\delta^m_i))_i,\ (\gamma'(\delta^B_i))_i,\ (\gamma'(\delta^r_i))_i,\ (\beta'(\delta^l_i))_i,\ (\beta'(\delta^{C'}_i))_i,\ (\beta'(\delta^m_i))_i,\ \beta'_0,\ (\beta'(\delta^{B'}_i))_i,\ (\beta'(\delta^r_i))_i$. Next in $(X_k)_k$, for each $i$, $\gamma'(\delta^r_i)$ are swapped past all the images of $\beta$ until it is immediately before $\beta'(\delta^r_i)$ by case a, then for each $i$, $\gamma'(\delta^B_i)$ is swapped past all the images of $\beta$ until $\beta'_0$ by case a, then swapped with $\beta'_0$ by one of the other cases depending on what type of reduction $B \to B'$ is and the relative positions. Even if it's case d or f, all of the images of $\gamma'(\delta^B_i)$ are disjoint from each other (and from the positions of the other reductions after $\beta'_0$) because the redex, $C$, doesn't contain any instances of the variable involved in the reduction $B \to B'$. Expanding the definitions, this is swapping $\alpha;\lambda;@_1;\delta^B_i;\tY;\lambda;\gamma'$ with $\alpha;\lambda;@_1;\beta'$ at a point in the reduction sequence where the subskeleton at $\alpha;\lambda;@_1;\beta'$ is $B$ with a mixture of $\tY \lambda x. A$ and $\tY \lambda x. A[C'/\gamma']$ substituted for its occurrences of $x$, and one of these occurrences of $x$ is at $\delta^B_i$. For each $\delta^B_j$, there are some corresponding $\delta^{B'}_k$, where the images of the $x$ at $\delta^B_j$ end up after the reduction $B \to B'$. These cover all the $\delta^{B'}_k$, and uniquely, and for each $\delta^B_i$, the images of $\zeta;\delta^B_i;\theta$ after swapping it with $\zeta;\beta'$ are precisely $(\zeta;\delta^{B'}_k;\theta)$ for those same values of $k$, therefore after swapping all of the $(\gamma'(\delta^B_i))_i$ down past $\beta'_0$, their images are $(\gamma'(\delta^{B'}_i))_i$. After swapping with $\beta'_0$, each of these images than swaps by case a to take its place among the $(\beta'(\delta^{B'}_i))_i$. Next up in $(X_i)_i$, the $(\gamma'(\delta^m_i))_i$s and the $(\gamma'(\delta^{C'}_i))_i$s swap down past some images of $\beta$ they're disjoint from to take their place before their matching image of $\beta$, then $\gamma'_0$ swaps past the $(\beta'(\delta^l_i))_i$, stopping immediately before $\beta'(\delta^{C'}_1)$, then the $(\gamma'(\delta^l_i))_i$ and the $(\beta'(\delta^l_i))_i$ mix. The end result of the rearrangement of (this subsequence of) the reduction sequence is therefore $\alpha,\ (\gamma'(\delta^l_i), \beta'(\delta^l_i))_i,\ \gamma'_0,\ (\gamma'(\delta^{C'}_i), \beta'(\delta^{C'}_i))_i,\ (\gamma'(\delta^m_i), \beta'(\delta^m_i))_i,\ \beta'_0,\ (\gamma'(\delta^{B'}_i), \beta'(\delta^{B'}_i))_i,\ (\gamma'(\delta^r_i), \beta'(\delta^r_i))_i$.

  The other sequence, $(Y_j)_j$, proceeds similarly, but with the images of $\gamma$ starting out after the images of $\beta$. It swaps all the images of $\beta$ that are right of $\gamma_0$ to the appropriate places by case a, then swaps all the $(\beta'(\delta^C_i)_i$ first to then past $\gamma'_0$, resulting in $(\beta'(\delta^{C'}_i))_i$ for the same reason as with swapping those images of $\gamma$ that overlapped with $\beta'_0$ past it in the $(X_i)_i$ case above, then finally swapping the $(\beta'(\delta^l_i))_i$s and the $(\gamma'(\delta^l_i))_i$s into the correct order. As required, this produces the same result as in $(X_i)_i$.
  \item $Y \to X$ forwards, and $\alpha < \gamma < \beta$: Let $\gamma'$ be such that $\gamma = \alpha;\textsf{if}_i;\gamma'$, $\alpha;@_1;\lambda;\gamma'$, $\alpha;@_2;\gamma'$ or $\alpha;\tY;\lambda;\gamma'$, so that $\gamma'$ is the freely varying later part of $\gamma$ as in the definition of $\sim_c$, whichever case applies to swapping $\gamma$ and $\alpha$. Similarly, let $\beta'$ be the freely varying part of $\beta$ relative to $\gamma$. Let the images of $\gamma$ after swapping it with $\alpha$ (where this swap occurs later in the reduction sequence than $\beta$) then be $(\alpha;\delta_i;\gamma')_i$, and the images of $\beta$ after swapping it with $\gamma$ be $(\gamma;\epsilon_i;\beta')_i$. Depending on the skeleton at $\alpha$, and $\gamma$'s position within it, $(\delta_i)_i$ may be empty (case b), a singleton containing $\cdot$ (cases c and d), all the positions of the relevant variable in the lambda in the skeleton at $\alpha$ (case e), or $\lambda;@_1$ and $\lambda;@_1;\zeta;\tY;\lambda$ for each position $\zeta$ of the relevant variable (case f), but in all cases (except a, which is excluded because none of $\alpha, \gamma$ and $\beta$ are disjoint) the set of images has this general structure. Furthermore, the positions $(\delta_i)_i$ are determined only by the general position of $\gamma$ within $\alpha$ (the part excluded from $\gamma'$), and the positions of the relevant variable within the skeleton at $\alpha$. They do not depend on $\alpha$ or $\gamma$, so that if both initial positions were moved somewhere else the relative positions of the images of $\gamma$ would be unaffected, and similarly if some position within $\gamma$ were swapped with $\alpha$, the relative positions of its images would be the same (except to the extent that, in case f, the positions of the relevant variable are taken after the inner reduction takes place, so that some of those may still change). Because there are so many of them and they don't actually affect the multiset of positions where reductions take place, I will be ignoring swaps by case a in the proof of this case, and just assuming that all the final positions end up in the correct order.
  
  The final set of reduction positions that I will be proving both $(X_i)_i$ and $(Y_j)_j$ reach is $\alpha, (\alpha;\delta_i;\gamma', (\alpha;\delta_i;\gamma';\epsilon_j;\beta')_j)_i$, where as usual the sequences are expanded out to the full list. For the sequence $(X_i)_i$, first the images of $\beta$ in $X_0$ are $(\gamma,\epsilon_i,\beta')$. For each of these in turn, it is swapped with $\alpha$. By the fact mentioned above that sub-positions are mapped to the same set of images except in case f, the images of $\gamma,\epsilon_j,\beta'$ after this swap are $(\alpha;\delta_i^j;\gamma',\epsilon_j,\beta')_i$, where $(\delta_i^j)_i$ is the same as $(\delta_i)_i$ except that in the case where this is not the first image of $\beta$ swapped below $\alpha$ and the reduction at $\alpha$ is a $\tY$-reduction (and therefore swaps with it proceed by case f), the positions of the relevant variable are taken after the reductions at $\gamma$ and $\gamma;\epsilon_k;\beta'$ for $k \leq j$ (this implies that the first of these, $\delta_i^n = \delta_i$). All the images $\alpha;\delta_i^j;\gamma';\epsilon_j;\beta'$ where $\delta_i^j > \lambda;@_1;\gamma';\epsilon_{j+1};\beta'$ are then swapped with $\alpha;\lambda;@_1;\gamma';\epsilon_{j+1};\beta'$ (which is $\alpha;\delta_i^{j+1};\gamma';\epsilon_{j+1};\beta'$ for some $i$). As in the case where $\alpha;\tY < \gamma, \beta$ and $\gamma$ and $\beta$ are disjoint, the effects of these swaps may duplicate or delete the reductions in such a way that all the remaining images of $\gamma;\epsilon_j;\beta'$ are $(\alpha;\delta_i^{j+1};\gamma';\epsilon_j;\beta'$. This is repeated with $\delta_i^{j+2}$ and so on until these images are all in the correct order, at which point they are $(\alpha;\delta_i;\gamma';\epsilon_j;\beta')_i$. After all of these are done, $\gamma$ is swapped with $\alpha$, yielding $(\alpha;\delta_i^{-1};\gamma')_i$. As with the images of $\beta$, the subset of these which are inside $\alpha;\lambda;@_1;\gamma';\epsilon_j;\beta'$ for each $j$ in turn are swapped with it, until they are all in the correct order and the images left of $\gamma$ are $(\alpha;\delta_i;\gamma')_i$. At this point, $X_i$ has the desired value.
  
  The sequence $(Y_j)_j$ proceeds similarly. First $\gamma$ swaps with $\alpha$, yielding $\alpha$ and $(\alpha;\delta_i;\gamma')_i$, then $\beta$ swaps with $\alpha$, yielding one image of $\beta$ for each $\delta_i$, except that, in case f, those images at positions inside $\alpha;\lambda;@_1;\gamma'$ may differ because they are earlier in the reduction sequence than any of the images of $\gamma$. Each of the images of $\beta$ in turn it moved to the corresponding image of $\gamma$, except that those overlapping with $\alpha;\lambda;@_1;\gamma'$ may be duplicated or deleted on the way. The images of $\beta$ after just this process (which does not actually correspond to any term in $(Y_j)_j$, but it's sufficiently independent that the order doesn't matter that much) are $(\alpha;\delta_i;\gamma';\beta'')$ (where $\gamma;\beta'' = \beta$). The images of these for each $i$, after swapping with $\alpha;\delta_i;\gamma'$, is $(\alpha;\delta_i;\gamma';\epsilon_j;\beta')_j$, because the reductions produced by a swap are unaffected by its overall location, and the skeleton at $\alpha;\delta_i;\gamma'$ at the relevant point in the sequence is equal to the skeleton at $\gamma$ initially, therefore this is equivalent to immediately swapping $\beta$ and $\gamma$ as in $Y \sim_c X$ (except that in the case where the reduction at $\alpha$ is a $\tY$-reduction, the skeleton at $\alpha;\lambda;@_1;\gamma'$ is not actually equal to the skeleton initially at $\gamma$ because something was substituted in for the variable bound at $\alpha;\tY$, but this doesn't affect the variable involved in the reduction at $\gamma$, so this doesn't actually matter). Therefore also in this case, eventually the set of reductions in the relevant portion of $Y_j$ is $\alpha, (\alpha;\delta_i;\gamma', (\alpha;\delta_i;\gamma';\epsilon_j;\beta')_j)_i$, therefore it is equal to some $X_i$.
\end{itemize}
In summary, if the swap in $Y \sim_c X$ doesn't overlap with the swap $X \sim_c X_0$, the sequence of swaps can be rearranged until it does, and in any other case, there is some $Y_j$ that's equal to some $X_i$, therefore these sequences reach the same end point and if $Y \sim_c X$, then $Y_{cbv} = X_{cbv}$, and by chaining these together, if $Y \sim_c^* X$, $Y_{cbv} = X_{cbv}$.

\paragraph{}
If $Y$ is in CBV order and $Y \sim_c^* X$, then $Y_{cbv} = X_{cbv}$ but also $Y_{cbv} = Y$ because the sequence $(Y_i)_i$ terminates immediately therefore $X_{cbv}$ is the unique reduction sequence in call-by-value order that's related to $X$ by $\sim_c^*$.
\end{proof}

\begin{lemma}
\label{lem:sim-M-N}
The relation $\sim$ is defined on $L_0(M)$ with reference to a particular starting term $M$, so different versions, $\sim_M$ and $\sim_N$, can be defined starting at different terms. If $M \to N$, then $\sim^*_N$ is equal to the restriction of $\sim^*_M$ to $L_0(N)$.
\end{lemma} 

\begin{proof}
$\sim_N^*$ is trivially a subset of $\sim_M^*$ because $\sim_N$ is a subset of $\sim_M$.

In the other direction, suppose $(N_1, \alpha_1) \sim_M^* (N_2, \alpha_2)$ where both $N_1$ and $N_2$ are descendants of $N$. By Lemma~\ref{downAcrossUp}, take $(N_1, \alpha_1) \sim_{p,M}^* (N'_1, \alpha') \sim_{c,M}^* (N'_2, \alpha') \sim_{p,M}^* (N_2, \alpha_2)$. 
The $\sim_p^*$ steps remain within $Rch(N)$, and $\sim_p$ does not depend on the history of the reduction sequences, therefore $(N_1, \alpha_1) \sim_{p,N}^* (N'_1, \alpha') \sim_{c,M}^* (N'_2, \alpha') \sim_{p,N}^* (N_2, \alpha_2)$.

Let $X$ be the call-by-value reduction sequence related to both $N'_1$ and $N'_2$ by $\sim_{c,M}$ given by Lemma~\ref{canonicalCousins}. 
As $M \to N$ is the first reduction in the sequence $N'_1$, it is the last to be affected by the $\sim_{c,M}$ sequence $N'_1 \sim_{c,M}^* X$ given by Lemma \ref{canonicalCousins} therefore it can be split into $N'_1 \sim_{c,M}^* Y_1 \sim_{c,M}^* X$ where $Y_1$ is in CBV order except possibly for its first reduction, which is still $M \to N$. Let the position of the reduction $M \to N$ be $\beta$. 
\akr{TODO: fill in the details that are missing here. It shouldn't be too hard, it's just a bit sketchy at present.} 
The rearrangement of the reduction sequence $Y_1 \sim_{c,M} X$ consists of moving the reduction at $\beta$ down past the other reductions in $Y_1$, possibly duplicating or deleting it in the process, but not affecting the positions or the correct order for any of the other reductions. 
The reductions derived from $M \to N$ can be identified as follows:

%\begin{definition}
A position in some (reduction sequence of) term(s) is \emph{derived from} another if it is related by the reflexive transitive closure of $\rightsquigarrow$, where $(V,\gamma) \rightsquigarrow (W,\delta)$ just if $V \to W$ and one of the following cases holds:
\begin{itemize}
    \item $(V,\gamma) \sim_p (W,\delta)$
    \item $\gamma = \delta$, $V \to W$ at $\epsilon$, and $\epsilon > \gamma$
    \item $V \mid \epsilon = (\lambda x.U) Z, U | \zeta = x, \gamma = \epsilon;@_2;\theta$ and $\delta = \epsilon;\zeta;\theta$
    \item $V \mid \epsilon = \tY (\lambda x.U), \gamma = \epsilon;\tY;\lambda;\zeta$ and $\delta = \epsilon;\lambda;@_1;\zeta$
    \item $V \mid \epsilon = \tY (\lambda x.U), U | \zeta = x, \gamma = \epsilon;\theta$ and $\delta = \epsilon;\lambda;@_1;\zeta;\theta$
\end{itemize}
%\end{definition}

Crucially, the reductions in $X$ can be partitioned into 2 sets: those at positions derived from $(M,\beta)$, and those at positions equal to the reductions in $Y_1$ (in the same order as they occur in $Y_1$). Using the same construction for $N'_2$ shows that the positions of the reductions in $Y_2$ are also the positions of the reductions in $X$ other than those derived from $(M,\beta)$, therefore $Y_1 = Y_2$ therefore $N'_1 \sim_{c,M} Y_1 \sim_{c,M} N'_2$ and every reduction sequence in this sequence starts with $M \to N$ therefore $N'_1 \sim_{c,N} N'_2$.

Combining this with the $\sim_{p,N}^*$s at the beginning and end then yields the desired result that $(N_1,\alpha_1) \sim_N^* (N_2, \alpha_2)$, therefore the restriction of $\sim_M^*$ to $\sim_N$'s domain ($Rch(N)$) is a subset of $\sim_N^*$ therefore the two versions of $\sim$ match on this domain, as desired.
\end{proof}


At each reduction step $M \to N$, the sample space must be restricted from $I^{L_s(M)}$ to $I^{L_s(N)}$. 
The injection $L_0(N) \to L_0(M)$ is trivial to define by appending $Sk(M) \to Sk(N)$ to each path, and using Lemma~\ref{lem:sim-M-N}, this induces a corresponding injection on the quotient, $L(N) \to L(M)$. 
The corresponding map $L_s(N) \to L_s(M)$ is then denoted $i(M \to N)$.

Unlike in the purely call-by-value case, the version of the reduction relation that takes into account samples is still a general relation rather than a function, so it is denoted ``$\Rightarrow$'' instead of ``$\red$'', and it relates $\biguplus_{M \in \Lambda_0} I^{L_s(M)}$ to itself.
\changed[lo]{We write an element of $\biguplus_{M \in \Lambda_0} I^{L_s(M)}$ as $(M', s)$ where the \emph{term} $M' \in \Lambda^0$ and $s \in  I^{L_s(M')}$.}
\begin{align*}
& (M,s) \Rightarrow (N,s \circ i(M \to N)) \text{ if either} \\
& \qquad \text{$M \to N$ and the redex is not $\tsample$, or} \\
& \qquad \text{$M \mid \alpha = \tsample$, $N = M[\underline{s(Sk(M),\alpha)}/\alpha]$ and $\lambda$ does not occur after $@_2$ or $\tY$ in $\alpha$}
\end{align*}

\paragraph{}
\begin{lemma} \label{churchRosser}
The relation $\Rightarrow$ is Church-Rosser.
\end{lemma}
\begin{proof}
Suppose that $(M,s) \Rightarrow^* (M_1,s_1)$ and also $(M,s) \Rightarrow^* (M_2,s_2)$, then it is required to prove that there is some $(M',s')$ such that both $(M_1,s_1) \Rightarrow^* (M',s')$ and $(M_2,s_2) \Rightarrow^* (M',s')$. First consider the special case where $(M,s) \Rightarrow (M_1,s_1)$ and $(M,s) \Rightarrow (M_2,s_2)$, with only a single step in each case. Let the positions of the redexes in $M \to M_1$ and $M \to M_2$ be $\alpha_1$ and $\alpha_2$ respectively.

First consider the case that $\alpha_1$ and $\alpha_2$ are disjoint. Let $M_1 = M[X_1/\alpha_1]$ and similarly for $X_2$, then let $M' = M[X_1/\alpha_1][X_2/\alpha_2]$. As the positions are disjoint, the substitutions commute and both $M_1$ and $M_2$ reduce (with $\to$) to $M'$. 
Let $s' = s \circ i(M \to M_1) \circ i(M_1 \to M')$. 
The injection $i(M \to M_1) \circ i(M_1 \to M')$ consists of prepending $M \to M_1 \to M'$ to each reduction sequence, but by case a of $\sim_c$, this is equivalent to prepending $M \to M_2 \to M'$, which is $i(M \to M_2) \circ i(M_2 \to M')$. 
In the case that the reduction $M \to M_2$ isn't a $\tsample$-reduction, this is enough to establish that $(M_1, s_1) \Rightarrow (M', s')$ (and similarly if the redex of $M \to M_1$ isn't $\tsample$, $(M_2,s_2) \Rightarrow (M',s')$).
If it is $\tsample$ though, in order for it to be the case that $(M_1, s_1) \Rightarrow (M', s')$, it is additionally necessary that $X_2$, the result of the reduction at $\alpha_2$, be $s_1(M_1, \alpha_2)$, which follows from the fact that $(M, \alpha_2) \sim (M_1, \alpha_2)$ by case 1 of $\sim_p$. 
The case that the reduction $M \to M_1$ is a $\tsample$-reduction is similar.

The case that $\alpha_1 = \alpha_2$ is trivial, because there is at most one possible $\Rightarrow$ reduction at any given position, therefore $(M_1, s_1) = (M_2, s_2)$ already.

The remaining case is that $\alpha_1 < \alpha_2$ or $\alpha_1 > \alpha_2$. 
Assume without loss of generality that $\alpha_1 < \alpha_2$. 
For each possible case of what type of redex $M | \alpha_1$ is, and $\alpha_2$'s position within it, there is a corresponding case of $\sim_c$, and similarly to the case where $\alpha_1$ and $\alpha_2$ are disjoint, the term $O_1 = O_2$ from the definition of $\sim_c$ is a suitable value of $M'$. 
The term $M \mid \alpha_1$ can't be $\tsample$, because it has strict subterms, but the case that $M \mid \alpha_2 = \tsample$ is still somewhat more complicated. 
$\alpha_2$ can't be within $\alpha_1 ; @_2$ or $\alpha_1 ; Y$ (cases e and f of $\sim_c$) because those are not valid positions to reduce a $\tsample$, 
and the case that $\alpha_2$ is within the branch of an if statement that is deleted by the reduction at $\alpha_1$ (case b) doesn't actually present a problem because there is no corresponding reduction to $M \to M_2$ in the other branch, 
which leaves cases c and d, that $\alpha_2$ is in the $\tif{\cdot}{\cdot}{\cdot}$ branch that isn't deleted, and that $\alpha_2 < \alpha_1 ; @_1 ; \lambda$.  
The values that the $\tsample$s take in these cases match because $(M,\alpha) \sim_p (M_1, \alpha;\beta)$ by cases 3 and 2 of $\sim_p$ respectively.

\paragraph{}
In the case that $(M,s) \Rightarrow^* (M_1,s_1)$ or $(M,s) \Rightarrow^* (M_2,s_2)$ by multiple steps, it is possible to repeatedly replace a pair of the form $A \Leftarrow B \Rightarrow C$ by $A \Rightarrow^* D \Leftarrow^* C$ by the construction above, but it is not immediate that this process terminates, because each $\Rightarrow$ or $\Leftarrow$ may be replaced by multiple, so the sequence of $\Rightarrow$s and $\Leftarrow$s from $(M_1,s_1)$ to $(M_2,s_2)$ may get longer at some steps. However, termination can be proved by noting that the structure of this process is identical to the process of swapping $\sim_\uparrow$ and $\sim_\downarrow$ steps in Lemma \ref{downAcrossUp}. To be more precise, in this case there is a sequence of $\Leftarrow$ and $\Rightarrow$ steps, each of which has an associated reduction position and initial and final terms, and if a $\Leftarrow$ immediately precedes a $\Rightarrow$, they may be swapped to produce some number of $\Rightarrow$s, followed by some number of $\Leftarrow$s. In the case of Lemma \ref{downAcrossUp}, there is a sequence of $\sim_\uparrow$ and $\sim_\downarrow$ steps, each of which has an associated reduction position and initial and final skeletons, and if a $\sim_\uparrow$ immediately precedes a $\sim_\downarrow$, they may be swapped to produce some number of $\sim_\downarrow$s followed by some number of $\sim_\uparrow$s. In both cases, the number and reduction positions of the resultant steps are determined by the case of $\sim_c$ that matches the way the initial reduction positions overlap, and the initial skeleton (or the skeleton of the initial term). The same argument that the process in Lemma \ref{downAcrossUp} terminated is therefore applicable here too. At every stage, the sequence of reductions that results from one of the initial $\Rightarrow$s is a parallel sequence, and swapping a parallel sequence of $\Rightarrow$s with a $\Leftarrow$ always terminates, therefore each $\Rightarrow$ in turn can be moved past all of the $\Leftarrow$s, and the process as a whole will terminate in a state where all of the $\Rightarrow$s precede all of the $\Leftarrow$s, i.e.~a pair of reduction sequences $(M_1,s_1) \Rightarrow^* (M',s') \Leftarrow^* (M_2,s_2)$.
\end{proof}

\begin{lemma} \label{simpDoesn'tMix}
If $A$ is some descendant of $M$ and $(A,\gamma) \sim^* (M,\delta)$, then $(A,\gamma) \sim_p^* (M,\delta)$, with the length of the reduction sequences decreasing by one each step from $A$ to $M$.
\end{lemma}
\begin{proof}
This is a simple induction on $\sim^*$. In the base case, $(A,\gamma) = (M,\delta)$ therefore $(A,\gamma) \sim_p^* (M,\delta)$ trivially. Otherwise, suppose that $(M,\delta) \sim_p^* (B,\epsilon) \sim (A,\gamma)$. Either $(B,\epsilon) \sim_p (A,\gamma)$ or $(B,\epsilon) \sim_c (A,\gamma)$. In the first case, either $B \to A$, in which case the result follows directly, or $A \to B$, in which case the fact that each (reduction sequence of) skeleton(s) has only one parent implies that a $(M,\delta) \sim_p^* (A,\gamma)$ directly as a subsequence of the path to $(B,\epsilon)$.

In the $\sim_c$ case, consider the definition of $\sim_c$. Either $O'_1 = A$ and $O'_2 = B$ or vice-versa. As $M \to^* N \to^* O_1 \to^* B$ and $(M,\delta) \sim_p^* (B,\epsilon)$, and $\sim_p$ only relates positions in a term and its parent, there are some positions $\zeta, \theta$ such that $(M,\delta) \sim_p^* (N,\zeta) \sim_p^* (O_1, \theta) \sim_p^* (B,\epsilon)$. It follows that $(O_2, \theta) \sim_p^* (A,\gamma)$ by following the same path, therefore it suffices to provide the only missing portion of the path from $M$ to $A$, i.e.~to prove that $(N,\zeta) \sim_p^* (O_2,\theta)$ given $(N,\zeta) \sim_p^* (O_1,\theta)$ (or vice-versa).

If $\zeta$ is disjoint from all the positions of reduction from $N$ to $O_1$ and $O_2$ (and consequently $\zeta = \theta$), this follows from case 1 of $\sim_p$. Otherwise, this can be proved by taking cases from the definition of $\sim_c$. This is rather long, but all of the cases are similar. The general idea is that the reductions from $N$ to $O_1$ correspond to the reductions from $N$ to $O_2$, so that if a position is related by $\sim_p$ across that reduction, it is related in the other branch for the same reason. Case d, where $B = O'_1$ rather than $O'_2$, is given here in more detail as an illustrative example:

Let $I$ be $N$ reduced at $\alpha$, and $J$ be $N$ reduced at $\alpha;@_1;\lambda;\beta$, so that $N \to I \to O_1$ and $N \to J \to O_2$. All of the reduction positions are $\geq \alpha$, and $\zeta$ is not disjoint from all of them, therefore $\zeta$ is not disjoint from $\alpha$. Let $\iota$ be the position such that $(N,\zeta) \sim_p (I,\iota) \sim_p (O_1,\theta)$. The fact that $(N,\zeta) \sim_p (I,\iota)$ implies that $\zeta > \alpha;@_1;\lambda$. Let $\zeta = \alpha;@_1;\lambda;\kappa$ and $\theta = \alpha;\kappa'$. If $\kappa$ is disjoint from $\beta$, then $(N,\zeta) \sim_p (J,\zeta) \sim_p (O_2,\alpha;\kappa') = (O_2;\theta)$ by cases 1 and 2 of $\sim_p$. In the other case, that $\kappa$ isn't disjoint from $\beta$, $\kappa > \beta$ because none of the positions $<= \alpha;@_1;\lambda;\beta$ in $N$ are related to any position in $I$ by $\sim_p$. As $(I,\alpha;\kappa) \sim_p (O_1,\alpha;\kappa')$ (with the redex at $\alpha;\beta$), for exactly the same reason $(N,\alpha;@_1;\lambda;\kappa) \sim_p (J,\alpha;@_1;\lambda;\kappa')$ (with the redex at $\alpha;@_1;\lambda;\beta$). Because $(N,\alpha;@_1;\lambda;\kappa) \sim_p (J,\alpha;@_1;\lambda;\kappa')$, $N|\alpha;@_1;\lambda;\kappa = J|\alpha;@_1;\lambda;\kappa'$, and $N|\alpha;@_1;\lambda;\kappa \neq $ the variable of $N|\alpha;@_1$, therefore $J|\alpha;@_1;\lambda;\kappa'$ is also not the variable therefore $(J,\alpha;@_1;\lambda;\kappa') \sim_p (O_2, \alpha;\kappa') = (O_2, \theta)$ by case 2 of $\sim_p$. Combining these results, $(N,\zeta) \sim_p^* (O_2;\theta)$ as desired.
\end{proof}

\begin{lemma} \label{lem:redexDestroyed}
If $M \to N$, with the redex at position $\alpha$, then no position in any term reachable from $N$ is related by $\sim^*$ to $(M,\alpha)$.
\end{lemma}
\begin{proof}
Suppose on the contrary that $(M,\alpha) \sim^* (A,\beta)$, where $N \to^* A$, then by Lemma \ref{simpDoesn'tMix}, $(M,\alpha) \sim_p^* (A,\beta)$. $M \neq A$ therefore $(M,\alpha) \sim_p$ some position in $N$, but in all the cases of the definition of $\sim_p$, no position in the child term is related to the position of the redex.
\end{proof}

\paragraph{}
The reduction relation $\Rightarrow$ is nondeterministic, so it admits multiple possible reduction strategies. 
A \emph{reduction strategy} starting from a closed term $M$ is a partial function $f$ from $Rch(M)$ to positions, such that for any reachable term $N$ where $f$ is defined, $f(N)$ is a position of a redex in $N$, and if $f(N)$ is not defined, $N$ is a value.
Using a reduction strategy $f$, a subset of $\Rightarrow$ that isn't nondeterministic, $\Rightarrow_f$, can be defined by $(N,s) \Rightarrow_f (N',s')$ just if $(N,s) \Rightarrow (N',s')$ and $N$ reduces to $N'$ with the redex at $f(N)$.

Essentially what Lemma~\ref{lem:redexDestroyed} demonstrates is that the samples taken during any reduction sequence are independent of each other. This is made more precise in the following lemmas.

\begin{lemma}
For any skeletons $M \to N$, with the redex at position $\alpha$, and measurable set of samples $S \subset I^{L_s(N)}$, $\mu(S) = \mu(\{s \in I^{L_s(M)} \mid s \circ i(M \to N) \in S\})$, and furthermore, if $M | \alpha = \tsample$, for any $S \subset I \times I^{L_s(N)}$, $\mu(S) = \mu(\{s \in I^{L_s(M)} \mid (s(M,\alpha), s \circ i(M \to N)) \in S\})$.
\end{lemma}
\begin{proof}
If the result holds for all sets $S$ of the form $\{s \in I^{L_s(N)} \mid \forall j : s(j) \in x_i\}$, where $(x_j)_{j \in J}$ is a family of measurable subsets of $I$ indexed by some finite set $J \subset L_s(N)$ of positions, it also holds for all other $S$ by taking limits and disjoint unions.

Take such a $(x_j)_{j \in J}$, where $x_j = x_k$ and define $K = i(M \to N)[J]$. Because $i(M \to N)$ is injective, it defines a bijection between $J$ and $K$. We can then calculate the measure $\mu(\{s \in I^{L_s(M)} \mid s \circ i(M \to N) \in S\}) = \mu(\{s \in I^{L_s(M)} \mid \forall k : s(k) \in x_{i(M \to N)^{-1}(k)}\}) = \prod_{k \in K} \mu_I(x_{i(M \to N)^{-1}(k)}) = \prod_{j \in J} \mu_I(x_j) = \mu(S)$.

In the case that $M|\alpha = \tsample$, we can similarly consider only those sets $S$ of the form $x_\tsample \times \{s \in I^{L_s(N)} \mid \forall j : s(j) \in x_i\}$. The position $\alpha$ in $M$ is not related to any position in $L_s(N)$, therefore $(M,\alpha) \not \in K$. Again, we can calculate the measure $\mu(\{s \in I^{L_s(M)} \mid (s(M,\alpha), s \circ i(M \to N)) \in S\}) = \mu(\{s \in I^{L_s(M)} \mid s(M,\alpha) \in x_\tsample, \forall k : s(k) \in x_{i(M \to N)^{-1}(k)}\}) = \mu_I(x_\tsample) \prod_{k \in K} \mu_I(x_{i(M \to N)^{-1}(k)}) = \mu_I(x_\tsample) \prod_{j \in J} \mu_I(x_j) = \mu(S)$.
\end{proof}

\begin{lemma} \label{lem:independentSamples}
For any initial term $M$, reduction strategy $f$ on $M$, natural number $n$, skeleton $N$ with $k$ holes, measurable set $T \subset \mathbb R ^ k$ and measurable set $S$ of samples in $I^{L_s(N)}$, $\mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in S : (M,s) \Rightarrow_f^n (N[r], s')\}) = \mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in I^{L_s(N)} : (M,s) \Rightarrow_f^n (N[r], s')\}) \mu(S)$
\end{lemma}
\begin{proof}
Suppose, to begin with, that $n = 0$. Either $\exists r \in T : N[r] = M$, in which case both sides of the equation are $\mu(S)$, or there is no such $r$, in which case both sides of the equation are 0.

For $n > 0$, suppose for induction that the lemma is true for $n - 1$, for all $N, T$ and $S$. If $\exists r \in T, s' \in S : (M,s) \Rightarrow_f^n (N[r], s')$, the $r$ and $s'$ are necessarily unique, therefore we may assume that $T$ is the product of $k$ measurable setsets of $\mathbb R$, $(T_j)_{0 \leq j < k}$, as all of the other cases follow by taking unions and limits of these rectangles. Let\begin{align*}
R_s & = \{\text{the skeleton of } O \mid O \in Rch(M), r \in \mathbb R^k, O | f(O) = \tsample, O \to N[r]\} \\
R_d & = \{\text{the skeleton of } O \mid O \in Rch(M), r \in \mathbb R^k, O | f(O) \neq \tsample, O \to N[r]\}.
\end{align*}
Together, $R_s$ and $R_d$ contain all of the skeletons of terms that can reduce to $N$, and they're disjoint. For each of these skeletons, there is a map $g_O$ from the reals in $O$ to the reals in $N$, i.e. $g_O(r) = r'$ if $O[r] \to N[r']$ for $O \in R_d$, and $g_O(s, r) = r'$ if $O[r] \to N[r']$ with $s$ as the value the sample takes in the reduction. All of these functions are measurable, as they are just rearrangements of vector components, or in the case that $O|f(O) = \underline x(\skeletonPlaceholder, \dots, \skeletonPlaceholder)$, a combination fo the measurable function $x$ with a rearrangement of vector components. In the case that $O \in R_s$, the function $g_O$ simply inserts the sample into the vector of reals at a certain index, call it $h_O$.
\begin{align*}
& \mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in S : (M,s) \Rightarrow_f^n (N[r], s')\}) \\
= & \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_s, r \in \mathbb R^{k-1}, s' \in I^{L_s(O)}, r' \in T, s'' \in S : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) + \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_d, r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}, r' \in T, s'' \in S : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \mathbb R^{k-1}, s' \in I^{L_s(O)}: g_O(r,s'(O,f(O))) \in T, s' \circ i(O[r] \to N[g_O(r,s'(O,f(O)))]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, s' \circ i(O[r] \to N[g_O(r)]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)}: s'(O,f(O)) \in T_{h(O)}, s' \circ i(O[r] \to N[g_O(r,s'(O,f(O)))]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, s' \circ i(O[r] \to N[g_O(r)]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)} : (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(\{s' \in I^{L_s(O)} \mid s'(O,f(O)) \in T_{h(O)}, s' \circ i(O[r] \to N[g_O(r,s'(O,f(O)))]) \in S\}) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu({s' \in I^{L_s(O)} \mid s' \circ i(O[r] \to N[g_O(r)]) \in S}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)} : (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(T_{h(O)}) \mu(S) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)} : s'(O,f(O)) \in T_{h(O)}, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \mathbb R^{k-1}, s' \in I^{L_s(O)} : g_O(r,s'(O,f(O))) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) \\
= & \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_s, r \in \mathbb R^{k-1}, s' \in I^{L_s(O)}, r' \in T, s'' \in I^{L_s(N)} : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) \mu(S) + \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_d, r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}, r' \in T, s'' \in I^{L_s(N)} : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) \mu(S) \\
= & \mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in I^{L_s(N)} : (M,s) \Rightarrow_f^n (N[r], s')\}) \mu(S)
\end{align*}
\end{proof}

The usual call-by-value semantics can be implemented as one of these reduction strategies, given by (with $V$ a value and $T$ a term that isn't a value and $M$ a general term)
\begin{align*}
\cbv(T M) & = @_1 ; \cbv(T) \\
\cbv(V T) & = @_2 ; \cbv(T) \\
\cbv(\underline f(V_1, \dots, V_{k-1}, T, M_{k+1}, \dots, M_n)) & = \underline f_k ; \cbv(T) \\
\cbv(\tY T) & = \tY ; \cbv(T) \\
\cbv(\tif{T < 0}{M_1}{M_2}) & = \textsf{if}_1 ; \cbv(T) \\
\cbv(\tscore(T)) & = \tscore ; \cbv(T) \\
\cbv(V) & \text{ is undefined} \\
\cbv(T) & = \cdot \text{ otherwise}
\end{align*}
(this last case covers redexes at the root position).

A closed term $M$ terminates with a given reduction strategy $f$ and samples $s$ if there is some natural number $n$ such that $(M,s) \Rightarrow_f^n (N,s')$ where $f$ gives no reduction at $N$. The term terminates almost surely with respect to $f$ if it terminates with $f$ for almost all $s$.

\begin{theorem} \label{thm:AstEquivalence}
A closed term $M$ is AST with respect to $\cbv$ iff it is AST.
\end{theorem}
\begin{proof}
\akr{I'm not sure whether this proof is sufficiently detailed. It would be pretty easy to fill it in more, just very verbose.}

For both $\red$ and $\Rightarrow_{\cbv}$, the probability measure on the sample space can be used to define a measure on the reduction sequences of each finite length. The distributions are equal for each length, by induction, as follows.

The base case, length 0, is trivial, because in both cases the distribution has probability 1 on the sequence containing only $M$.

For the inductive case, suppose that the distributions of reduction sequences of length $n$ are equal. \akr{TODO: mention the case that it has already terminated by step $n$, or is about to. The two versions of the semantics handle it a little differently.}

For any term $N$ which is not a value, there is a unique environment $E$ and redex $R$ such that $N = E[R]$. The position of the hole in $E$ is equal to $\cbv(N)$, so that $N|\cbv(N) = R$, as the cases in the definition of $\cbv$ match the cases in the definition of environments. If $R \neq \tsample$, let $R'$ be the result of reducing $R$ (which is equal in both versions of the semantics), then $E[R'] = N[R' / \cbv(N)]$ therefore $\red(N,s) = (E[R'],s)$ and $(N,s) \Rightarrow_{\cbv} (E[R'],s)$. The distribution of next terms, conditional on the previous term, in the case that that previous term reduces deterministically, is therefore equal for $\red$ and $\Rightarrow_{\cbv}$.

In the other case that $R = \tsample$, the first (term) part of $\red(N,s)$ is $E[\underline{\pi_h(s)}]$. If $k$ is the number of samples already taken in this reduction sequence, this is equivalent to $E[\underline{s_0(k)}]$, where $s_0$ is the initial sample. For any sufficiently small neighbourhood of $N$ (i.e.~containing only reduction sequences with the same skeletons), the probability of reaching this neighbourhood is independent of $s(k)$, because it depends only on the samples taken so far, and the samples are independent. The distribution of $s(k)$ conditional on reaching $N$ is therefore the uniform distribution on $I$. Similarly for the other semantics, if the distribution of remaining samples after $n$ steps is independent of the term by Lemma \ref{lem:independentSamples}, therefore the distribution of $s(Sk(N),\cbv(N))$ conditional on $N$ is the uniform distribution on $I$. In either case, the distribution of the next term conditional on the previous term, in the case that it reduces randomly, is equal to the image under $r \mapsto E[\underline r]$ of the uniform distribution on $I$.

In any case, the distribution of reduction sequences after $n+1$ steps, conditional on the reduction sequence after $n$ steps, as defined by $\Rightarrow_{\cbv}$ and by $\red$, is equal, any by the inductive hypothesis the distributions after $n$ steps are equal, therefore by integrating over the reduction sequence up to step $n$, the distributions on reduction sequences up to step $n+1$ are equal.

The distributions on reduction sequences of any finite length as defined by $\Rightarrow_{\cbv}$ and $\red$ are therefore equal, therefore so is the probability of having reached a value after $n$ steps, therefore so is its limit, the probability of termination, therefore the probability of termination is 1 iff the probability of termination with respect to $\cbv$ is 1.
\end{proof}

\begin{theorem} \label{thm:CbvIsTerminatingest}
If $M$ terminates with some reduction strategy $f$ and samples $s$, it terminates with $\cbv$ and $s$.
\end{theorem}
\begin{proof}
  Suppose $M$ terminates with the reduction strategy $f$ and samples $s$. Let $(M,s) = (M_0,s_0) \Rightarrow_f \dots \Rightarrow_f (M_n,s_n)$, where $M_n$ is a value. By Lemma \ref{canonicalCousins}, the reduction sequence $(Sk(M_i))_i$ is related by $\sim_c^*$ to some skeletal reduction sequence $X$ which is in call-by-value order. For each skeletal reduction sequence $X_j$ in the sequence $(Sk(M_i))_i \sim_c \dots \sim_c X$, it is possible to define a reduction sequence of terms such that $(M,s) = (N_{j,0},s_{j,0}) \Rightarrow \dots \Rightarrow (N_{j,n_j},s_{j,n_j})$ and $Sk(N_{j,k}) = X_{j,k}$, by at each step applying the reduction at the same place as in $X_j$, essentially just filling in the reals in the skeletal reduction sequence to make it a reduction sequence of terms (it will be shown shortly that the correct branch is taken in each $\textsf{if}$-reduction). Assume for induction on $j$ that $(N_{j,n_j},s_{j,n_j}) = (M_n,s_n)$. To prove that this holds true for $j+1$ as well, it suffices to show that the corresponding term and samples immediately after the reductions involved in this $\sim_c$ step match, as from that point on, the $\Rightarrow$-sequences will match, as they start from the same point and have all the same reduction positions. Letting the term and remaining samples immediately before the reductions involved in this $\sim_c$ step be $(N,t)$ (which is the same in both $N_j$ and $N_{j+1}$ for the same reason again, they start at the same point and the sequences of reduction positions up to this point are the same), and the terms and samples after these reductions be $(O_1,t_1)$ and $(O_2,t_2)$, so that these match the $N$, $O_1$ and $O_2$ in the definition of $\sim_c$, $t_1 = t_2$ because both of them are, by Lemma \ref{lem:sim-M-N}, equal to the restriction of $t$ to the subset of $Rch(N)$ that starts with the reduction sequences to $O_1$ and $O_2$ respectively, and the potential positions in those subsets of $Rch(N)$ are identified with each other by $\sim$ because of the same case of $\sim_c$ that means $N_j$ and $N_{j+1}$ are related. If none of the relevant reductions are $\tsample$-reductions, $O_1 = O_2$ trivially. If there are samples taken (which must be the arbitrary reduction at $\beta$), it is also required that the same samples are taken in both branches. Similarly to Lemma \ref{churchRosser}, the positions of these $\tsample$-reductions in each branch are related by $\sim_p^*$, therefore they correspond to the same sample in $t$, therefore $O_1 = O_2$. The fact that the terms are equal also implies that the same branch of an $\textsf{if}$ statements will be taken.

  \paragraph{}
  There is therefore a reduction sequence $(M,s) = (N_0,s_0') \Rightarrow \dots \Rightarrow (N_m, s_m')$ which ends in a value ($N_m = M_n$) and is in CBV order. It doesn't immediately follow that this is the $\Rightarrow_{\cbv}$ reduction sequence starting from $(M,s)$ because the fact that $(N_i)_i$ is in CBV order only means that the reductions that do take place in this reduction sequence are in the correct order, not that the reduction sequence finishes once it reaches a value, or that there are no other reductions that would come earlier in CBV order that don't occur at all in $(N_i)_i$. Suppose that at some point $j$ in the reduction sequence, the reduction that occurs is not the next reduction in CBV order, but something else, and that the reduction that should be next is at position $\alpha = \cbv(N_i)$, and the position of the actual reduction $N_j \to N_{j+1}$ is $\beta$. Suppose further that $N_j$ is not a value. Either $\alpha = \cdot$, the root position, or the reduction at the root position is dependent in some way on the reduction at $\alpha$ (e.g. $\alpha = \textsf{if}_1$, or $\alpha = \underline{+};@_1$). In any case, all the reductions in the rest of the reduction sequence are after $\alpha$ in CBV order, therefore none of them are $\leq \alpha$, and there continues to be an unreduced redex at $\alpha$, therefore the reduction at the root position never occurs, therefore $N_m$ is not a value, which is a contradiction, therefore at the point (if any) where $(N_i)_i$ departs from the $\Rightarrow_{\cbv}$ reduction sequence starting at $(M,s)$, it has already reached a value, therefore $(M,s) \Rightarrow_{\cbv} \dots$ does reach a value eventually, i.e.~$M$ terminates with the reduction strategy $\cbv$ and samples $s$.
\end{proof}

\begin{corollary}
If $M$ is AST with respect to any reduction strategy, it is AST.
\end{corollary}
\begin{proof}
Suppose $M$ is AST with respect to $f$. Let the set of samples with which it terminates with this reduction strategy be $X$. By Theorem~\ref{thm:CbvIsTerminatingest}, $M$ also terminates with $\cbv$ and every element of $X$, and $X$ has measure 1, by assumption, therefore $M$ is AST with respect to $\cbv$ therefore by Theorem~\ref{thm:AstEquivalence} it is AST.
\end{proof}

All of the theorems on the termination of rankable terms therefore extend to other reduction strategies too. The proofs of Theorems \ref{thm:rankable implies termination}, \ref{thm:partial implies rankable}, \ref{thm:antitone rankable implies termination} and \ref{thm:antitone partial implies rankable} are all sufficiently generic with respect to what the reduction relation actually is that they can be directly applied to other reduction strategies. They only require that the number of reductions that can occur without any of them being a $\tY$-reduction is bounded for any starting term (which is true, because Theorem \ref{thm:de groote} applies equally to any reduction strategy). For a reduction strategy $r$ and term $M$, just substitute $N[r(N)]$ being a $\tY$-redex for $N$ being of the form $E[\tY \lambda x. O]$, and $r(N)$ being undefined for $N$ being a value.

The domain of definition of the ranking functions also needs to be changed from the reachable terms $Rch(M)$ to the \emph{reachable terms with respect to the reduction strategy $r$},
\[
Rch_r(M) = \{N \mid \exists n, N_0, \dots, N_n: N_0 = M, N_n = N, N_i \to N_{i+1} \text{ at } r(N_i)\}.
\]

To be more explicit, the modified forms of the theorems are:
\begin{definition}\rm
A \emph{ranking function on $M$ with respect to a reduction strategy $r$} is a measurable function $f:\mathit{Rch}_r(M) \to \mathbb{R}$ such that $f(N) \geq 0$ for all $N$, and
\begin{itemize}
    \item $f(N) \geq 1+ f(N')$ if $N[r(N)]$ is a $\tY$-redex and $N$ reduces to $N'$ at $r(N)$
    \item $f(N) \geq \int_I f(N[\underline{x}/r(N)]) \, \Leb(\mathrm{d}x)$ if $N[r(N)] = \tsample$

    \item $f(N) \geq f(N')$ if $r(N)$ is any other redex, where $N \to N'$ at $r(N)$.
\end{itemize}
Any closed term for which a ranking function with respect to a reduction strategy $r$ exists is called \emph{rankable} with respect to $r$. 
\end{definition}

\begin{definition}
A \emph{partial ranking function on $M$ with respect to a reduction strategy $r$} is a partial function $f : Rch_r(M) \rightharpoonup \mathbb R$ such that
\begin{itemize}
    \item $f(N) \geq 0$ for all $N$ where $f$ is defined.
    \item $f$ is defined at $M$.
    \item For any $N$ in the domain of definition of $f$, evaluation of $N$ at the positions specified by $r$ will eventually reach some $O$ such that either $r(O)$ isn't defined or $f(O)$ is, and $f(N) \geq \mathbb E[f(O) + \text{ the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}
\end{definition}

\begin{definition}\rm
An \emph{antitone ranking function on $M$ with respect to a reduction strategy $r$} is a measurable function $f:\mathit{Rch}_r(M) \to \mathbb{R}$ such that $f(N) \geq 0$ for all $N$, and there exists an antitone function $\epsilon : \nnReal \to \pReal$ such that
\begin{itemize}
    \item $f(N) \geq \epsilon(f(N)) + f(N')$ if $N[r(N)]$ is a $\tY$-redex and $N$ reduces to $N'$ at $r(N)$
    \item $f(N) \geq \int_I f(N[\underline{x}/r(N)]) \, \Leb(\mathrm{d}x)$ if $N[r(N)] = \tsample$

    \item $f(N) \geq f(N')$ if $r(N)$ is any other redex, where $N \to N'$ at $r(N)$.
\end{itemize}
Any closed term for which an antitone ranking function with respect to a reduction strategy $r$ exists is called \emph{antitone rankable} with respect to $r$. 
\end{definition}

\begin{definition}
An \emph{antitone partial ranking function on $M$ with respect to a reduction strategy $r$} is a partial function $f : Rch_r(M) \rightharpoonup \mathbb R$ such that there exists some antitone function $\epsilon : \nnReal \to \pReal$ such that
\begin{itemize}
    \item $f(N) \geq 0$ for all $N$ where $f$ is defined.
    \item $f$ is defined at $M$.
    \item For any $N$ in the domain of definition of $f$, evaluation of $N$ at the positions specified by $r$ will eventually reach some $O$ such that either $r(O)$ isn't defined or $f(O)$ is, and $f(N) \geq \mathbb E[f(O) + \epsilon(f(O)) \times \text{the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}
\end{definition}

\begin{theorem}
If a closed SPCF term $M$ has a ranking function, partial ranking function, antitone ranking function or antitone partial ranking function with respect to a reduction strategy $r$, then $M$ is AST with respect to $r$, and AST.
\end{theorem}
\lo{Give an example or two of a reduction strategy (for $\Rightarrow$) that is not $\cbv$.}

\akr{NOTE. For the standard CBV semantics, this example is indeed not a problem. 
In the alternative semantics, defining 
\begin{align*}
A[x] &= \tif{\tif{x>0}{I}{I} \, \tsample - 0.5 > 0}{0}{\Omega}\\
B &= \tif{\tsample - 0.5 > 0}{0}{\Omega}
\end{align*}
The set of samples where it terminates is 
\[\begin{array}{l}
\bigcup_{r \in [0,1]} \{s \in I^{L_s(A[\tsample])} \mid s([A[\tsample]], \mathsf{if}_1;\underline{ -
}_1;@_1;\mathsf{if}_1) = r, \\
\qquad \qquad \hfill \qquad \qquad s([A[\tsample], A[r], \ldots, B, \mathsf{if}_1) > 0.5 \}.
\end{array}\] 
This is a rather unwieldy expression, but the crucial part is that $r$ occurs twice in the conditions on $s$: once as the value a sample must take, and once in the location of a sample. 
It is this set that's unmeasurable.}


\bibliographystyle{apalike}
\bibliography{references}

\lo{22 Jan: 
\paragraph{Further directions}

An obvious next step is to extend the result to the $\mathsf{score}$ construct.

The following are highly topical, and could form the basis of an interesting and novel DPhil thesis.
\begin{enumerate}
\item Devise methods for proving (positive) a.s.~termination. 

- For example, develop a type system satisfying the property: if a term is typable then it is (positively) a.s.~terminating.
See~\citep{DBLP:conf/ppdp/BreuvartL18,DBLP:conf/esop/LagoG17}.

- Another approach is to develop algorithms that synthesise ranking supermartingales, following, for example, \citep{DBLP:journals/pacmpl/AgrawalC018}.

\item Develop principles (e.g.~in the form of ``proof rules'') for reasoning about (positively) a.s.~termination, in the style of \citep{DBLP:journals/pacmpl/McIverMKK18}.


\item Design algorithms that synthesise probabilistic invariants (\emph{qua} martingales), \`a la \cite{SchreuderO19}; see also \citep{DBLP:journals/pacmpl/HarkKGK20}.

\end{enumerate}}

\iffalse
\begin{thebibliography}{9}
\bibitem{ppcf} Thomas Ehrhard, Michele Pagani, and Christine Tasson. Measurable cones and stable, measurable functions: a model for probabilistic higher-order programming. \emph{PACMPL}, 2(POPL):59:1–59:28, 2018. doi: 10.1145/3158147. URL \href{https://doi.org/10.1145/3158147}{https://doi.org/10.1145/3158147}.
\end{thebibliography}
\fi

\end{document}
