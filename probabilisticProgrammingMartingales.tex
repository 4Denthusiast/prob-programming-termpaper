\documentclass{article}

%% BEGIN {Luke's Macros}
%%
\newif\ifdraft
\drafttrue %% To hide all comments and highlighting, just comment this line out 

\input{Style/comment}
\usepackage{calculation}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning, graphs, quotes, graphdrawing, shapes.geometric}
\usegdlibrary{layered, circular}
\usepackage[square]{natbib}
\setcitestyle{aysep={}}

\definecolor{oxblue}{RGB}{0,33,71}
\definecolor{oxgold}{HTML}{a0630a}
\usepackage[final,
bookmarks,
bookmarksopen,
colorlinks,
final,
linkcolor=red,
citecolor=oxgold,
pdfstartview=FitH ]{hyperref}

\usepackage{amsmath} % cleveref must be loaded afer amsmath
\usepackage{cleveref}
\Crefname{theorem}{Theorem}{Theorems}
\Crefname{corollary}{Corollary}{Corollary}
\crefname{proposition}{Proposition}{Propositions}
\Crefname{claim}{Claim}{Claims}
\Crefname{definition}{Definition}{Definitions}
\Crefname{fact}{Fact}{Facts}
\Crefname{conj}{Conjecture}{Conjectures}
\Crefname{example}{Example}{Examples}
\Crefname{remark}{Remark}{Remarks}
\Crefname{convention}{Convention}{Conventions}
\Crefname{lemma}{Lemma}{Lemmas}
\Crefname{assumption}{Assumption}{Assumptions}
\Crefname{section}{Section}{Sections}
\Crefname{appendix}{Appendix}{Appendices}
\Crefname{figure}{Figure}{Figures}
%\Crefname{equation}{Eq.}{Equations}

% \addtotheorempostheadhook[theorem]{\crefalias{thmlisti}{theorem}}
% \addtotheorempostheadhook[lemma]{\crefalias{thmlisti}{lemma}}
% \addtotheorempostheadhook[proposition]{\crefalias{thmlisti}{proposition}}
% \addtotheorempostheadhook[corollary]{\crefalias{thmlisti}{corollary}}
% \addtotheorempostheadhook[claim]{\crefalias{thmlisti}{claim}}
% \addtotheorempostheadhook[fact]{\crefalias{thmlisti}{fact}}
% \addtotheorempostheadhook[example]{\crefalias{thmlisti}{example}}
% \addtotheorempostheadhook[definition]{\crefalias{thmlisti}{definition}}
% \addtotheorempostheadhook[remark]{\crefalias{thmlisti}{remark}}
% \addtotheorempostheadhook[assumption]{\crefalias{thmlisti}{assumption}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\nnReal}{\mathbb{R}_{{\geq}0}}
\newcommand{\pReal}{\mathbb{R}_{{>}0}}

\newcommand\expect[1]{\mathbb{E}[#1]}
\newcommand\set[1]{\{#1\}}
\newcommand\dif{\mathrm{d}}
\newcommand\calF{\mathcal{F}}
\newcommand\calI{\mathcal{I}}
\newcommand\Leb{\mathrm{Leb}}
\newcommand\ndraw[2]{\#\mathrm{draw}_{#1}(#2)}

\makeatletter
\newcommand{\dotDelta}{{\vphantom{\Delta}\mathpalette\d@tD@lta\relax}}
\newcommand{\d@tD@lta}[2]{%
  \ooalign{\hidewidth$\m@th#1\mkern-1mu\cdot$\hidewidth\cr$\m@th#1\Delta$\cr}%
}

%\usepackage[bbgreekl]{mathbbol} %for \bblambda
%%
%% END {Luke's Macros}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{enumitem}
\newcommand{\tY}{\mathsf{Y}}
%\newcommand{\tif}[3]{\textsf{if }#1\textsf{ then }#2\textsf{ else }#3}
\newcommand{\tif}[3]{\mathsf{if}(#1, #2, #3)} % space-saving concrete syntax
\newcommand{\tsample}{\mathsf{sample}}
\newcommand{\tscore}{\mathsf{score}}
\newcommand{\skeletonPlaceholder}{\mathsf{X}} % I'm not sure what the best notation here is.
\DeclareMathOperator{\red}{red}
\DeclareMathOperator{\nnext}{next}
\DeclareMathOperator{\cbv}{cbv}


\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\theoremstyle{lemma}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\title{Supermartingales and Termination Analysis of Statistical PCF}

\begin{document}

\maketitle

\lo{Tentative title}

\begin{abstract}
\lo{TODO}
\end{abstract}

\iffalse
\akr{Following your suggestion, I will be providing a criterion for termination of programs in PPCF \citep{DBLP:journals/pacmpl/EhrhardPT18} based on ranking supermartingales. 
As it's more convenient for this proof, a sampling-based semantics \lo{You need to provide reference(s) for sampling-based semantics.} will be used instead of the original distributional semantics. 
I assume some roughly applicable equivalence is proven somewhere, but it doesn't seem that hard anyway.}
\lo{Why not provide a proof as an appendix?}
\fi

\tableofcontents

Various theorems about probabilistic programs rely on the assumption that the program terminates almost surely. Proof rules based on relating the program state to supermartingales already exist for first-order imperative programs \citep{DBLP:journals/pacmpl/McIverMKK18}. This paper's contribution is to extend this method to a higher-order setting.

\section{Statistical PCF}

\subsection{Syntax of SPCF}

The language SPCF is a simply-typed lambda calculus with sampling of real numbers from $[0,1]$ and unbounded scoring, following \cite{MakOP20b}. Types and terms are defined as follows, where $r$ is a real number, $x$ is a variable, $f : \mathbb{R}^n \to \mathbb{R}$ is any measurable function, and $\Gamma$ is an environment:
\begin{align*}
  & \text{types } A, B ::= \textsf{R}  \mid  A \to B \\
  & \text{values } v ::= \lambda x.s  \mid  \underline{r} \\
  & \text{terms } s, t ::= v  \mid  x  \mid  t_1 \, t_2  \mid  \underline{f}(s_1,\dots ,s_n)  \mid  \tY s  \mid  \tif{s < 0}{t_1}{t_2}  \mid  \tsample  \mid  \tscore(s)
\end{align*}
\begin{align*}
  \frac{}{\Gamma ; x:A \vdash x:A} \qquad
  \frac{\Gamma ; x:A \vdash s : B}{\Gamma \vdash \lambda x.s : A \to B} \qquad
  \frac{}{\underline{r} : \textsf{R}} \qquad
  \frac{\Gamma \vdash s:A \to B \quad \Gamma \vdash t : A}{\Gamma \vdash s \, t : B} \\ \\
  \frac{\Gamma \vdash s_1:\textsf{R} \dots \Gamma \vdash s_n:\textsf{R}}{\Gamma \vdash \underline{f}(s_1,\dots,s_n) : \textsf{R}} \ f : \mathbb{R}^n \to \mathbb{R} \qquad
  \frac{\Gamma \vdash s : (A \to B) \to (A \to B)}{\Gamma \vdash \tY s : (A \to B)} \\ \\
  \frac{\Gamma \vdash c : \textsf{R} \quad \Gamma \vdash s_1 : A \quad \Gamma \vdash s_2 : A}{\Gamma \vdash \tif{c < 0}{s_1}{s_2} : A} \qquad
  \frac{}{\Gamma \vdash \tsample : \textsf{R}} \qquad
  \frac{\Gamma \vdash s : \textsf{R}}{\Gamma \vdash \tscore (s) : \textsf{R}}
\end{align*}

The set of all terms is denoted $\Lambda$, and the set of closed terms is denoted $\Lambda^0$.

\paragraph{}
To define the reduction relation, let \emph{evaluation contexts} be of the form:
\begin{align*}
  E ::= & \, [\,] \mid E \, t \mid v \, E \mid \underline{f}(r_1,\dots ,r_{k-1}, E, s_{k+1}, \dots, s_n) \\ & \mid \tY E \mid \tif{E<0}{s_1}{s_2} \mid \tscore (E)
\end{align*}
then a term reduces if it is formed by substituting a redex in a context i.e.
\begin{align*}
  E[(\lambda x.s) v] & \to E[s[v/x]] \\
  E[\underline f (\underline r_1, \dots , \underline r_n)] & \to E[\underline{f(r_1,\dots,r_n)}] \\
  E[\tY \lambda x. s] & \to E[\lambda z. s[(\tY \lambda x. s)/x] z] \text{ where $z$ is not free in $s$}\\
  E[\tif{\underline r < 0}{s_1}{s_2}] & \to E[s_1] \text{ where }r < 0 \\
  E[\tif{\underline r < 0}{s_1}{s_2}] & \to E[s_2] \text{ where }r \geq 0 \\
  E[\tsample] & \to E[\underline r] \text{ where } r \in [0,1] \\
  E[\tscore(\underline r)] & \to E[\underline r].
\end{align*}
\changed[lo]{We write $\to^\ast$ for the reflexive, transitive closure of $\to$.}
\lo{The $\tscore$ rule should have a side condition: if $r > 0$.}

Every closed well-typed term either is a value or reduces to another term.

\lo{NOTATION. A minor point: metavariables for terms are $s, t$, etc.~up to this point, but $M, N$, etc.~here after all.}

\subsection{Sampling semantics}
\label{sec:sampling semantics}
This version of the reduction relation allows $\tsample$ to reduce to any number in $[0,1]$. To more precisely specify the probabilities, an additional argument is needed to determine the outcome of random samples. Let $ I = [0,1] \subset \mathbb{R} $, and let $S = I^{\mathbb{N}}$, with the \changed[lo]{Borel $\sigma$-algebra $\calF$} and the probability measure, denoted $\mu$, given by the limit of $1 \gets I \gets I^2 \gets \cdots$, where the maps are the projections that ignore the last element. Equivalently, a basis of measurable sets is $\prod_{i=0}^\infty X_i$ where $X_i$ are all Borel and all but finitely many are $I$, and $\mu (\prod_{i=0}^\infty X_i) = \prod_{i=0}^\infty \Leb(X_i)$.
The maps $\pi_h:S \to I, \; \pi_t:S \to S$ popping the first element are then measurable.
\changed[lo]{Following \cite{DBLP:conf/esop/CulpepperC17}, we call the probability space $(S, \calF, \mu)$ the \emph{entropy space}.}

The $\sigma$-algebra and measure on $\Lambda$ are defined by considering it as a disjoint union of equivalence classes under replacing all the real constants by a placeholder, where the measure on each class is that of $\mathbb{R}^n$, where $n$ is the number of real constants.
\changed[lo]{Precisely, following \cite{DBLP:conf/icfp/BorgstromLGS16},
we view $\Lambda$ as $\bigcup_{m\in\omega} \big(\mathsf{Sk}_m \times \Real^m \big)$,
where $\mathsf{Sk}_m$ is the set of SPCF terms with exactly $m$ place-holders (a.k.a.~\emph{skeleton terms}) for numerals.
Thus identified, we give $\Lambda$ the countable disjoint union topology of the product topology of the discrete topology on $\mathsf{Sk}_m$ and the standard topology on $\Real^m$.
Note that the connected components of $\Lambda$ have the form $\{M\} \times \Real^m$, with $M$ ranging over $\mathsf{Sk}_m$, and $m$ over $\omega$. 
We fix the Borel algebra of this topology to be the $\sigma$-algebra on $\Lambda$.}

The one-step reduction is given by the function $\red : \Lambda^0 \times S \to \Lambda^0 \times S$ where
\begin{equation}
\red(M,s) = \left\{
    \begin{array}{ll}
        (E[N],s) & \text{if } M = E[R], R \to N \text{ and } R \neq \tsample \\
        (E[\underline{\pi_h(s)}],\pi_t(s)) & \text{if } M = E[\tsample] \\
        (M,s) & \text{if } M \text{ is a value}
    \end{array} \right .
\end{equation}
\lo{Both $\to$ and $\red$ are called reduction relation.}

The result after $n$ steps is then simply $\red^n(M,s) = \overbrace{\red(...\red(}^n M,s)...)$, and the limit $\red^\infty$ can then be defined as a partial function as $\lim_{n \to \infty} \red^n(M,s)$ whenever that sequence becomes constant by reaching a value. A term $M$ terminates for a sample sequence $s$ if the limit $\red^\infty(M,s)$ is defined.

The reduction function is measurable, and the set of values is measurable, therefore the set of $s$ such that $M$ terminates at $s$ within $n$ steps is measurable for any $n$, therefore $\{s \mid M \text{ terminates at } s \}$ is measurable. A term $M$ is said to terminate almost surely if $\mu(\{s \mid M \text{ terminates at } s\}) = 1$.

\lo{Alternatively, define the \emph{runtime of $M$} to be the random variable 
\[
T_M(s) := 
\begin{cases}
\min \set{n \mid \pi_0(\red^n(M, s)) \textrm{ is a value}} & \hbox{if $\red^\infty(M,s)$ is defined}\\
\infty & \hbox{otherwise}
\end{cases}
\]
Equivalently, we say that $M$ is \emph{almost-surely terminating} (AST) if $T_M < \infty$ a.s.; 
and $M$ is \emph{positively almost-surely terminating} (PAST) if $\expect{T_M} < \infty$.}

For example, the term 
\[
(\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)} ) \, \underline{0},
\] 
which generates a geometric distribution, terminates on the set $S \setminus [0.5,1]^\mathbb N$, which has measure 1, therefore it terminates almost surely, whereas 
\[
\tif{\tsample - 0.5 < 0}{\underline 0}{(\tY \lambda x. x) \underline 0},
\] 
which terminates on the set $\pi_h^{-1}[[0,0.5]]$, has probability 0.5 of failing to terminate.

\paragraph{}
Note that the requirement that $M$ be closed is not important for the proofs of the theorems, but is simply required to ensure that $M$ reaches a value, rather than a general normal form like $x \, y$.

\paragraph{}
This definition of almost sure termination is equivalent to that given in \citep{MakOP20b}, although the program semantics is stated in a slightly different way. In particular, the argument to $\tscore$ is not relevant to termination (except for the possibility that its argument's evaluation wouldn't terminate).

\lo{Your operational semantics does not maintain a record of the current weight of the reduction.
A.s.~termination does depend on $\tscore$: see \cite[\S 4.3]{DBLP:journals/corr/abs-2004-03924}\footnote{\url{https://arxiv.org/abs/2004.03924}}.
I think it important to take the behaviour of $\tscore$ into account;
you should do it as a future task.}

\section{Alternative Semantics}
\akr{This version of the semantics is much more convenient for allowing multiple different reduction orders. I don't currently have a proof of the equivalence of this with the more usual semantics. I expect that that would be somewhat complicated, but not terribly difficult. I don't know whether a semantics like this has already been defined elsewhere. I just thought I'd write it out so you can see it and in case I use it later. If I do end up actually using it I'll write it up nicer, but hopefully this is at least enough for you to understand what I mean.

Having thought a bit more about this, I realise it may not be quite right. I'll see if I can fix it later.}
\lo{I'll read this after you fix it ;-)}

When proving almost sure termination in this way \akr{I guess I should move this section somewhere later.}, it is necessary to consider which terms a given term may reduce to. Sometimes however, the reduction that the programmer has in mind may not be strictly the call-by-value order defined so far, or considering an alternative reduction order may be simpler or more intuitive.

Non-probabilistic lambda calculi generally have the Church-Rosser property, that if a term $A$ reduces to both $B_1$ and $B_2$, there is some $C$ 
%with reduction sequences $B_1 \to^* C$ and $B_2 \to^* C$, 
to which both $B_1$ and $B_2$ reduce,
so the reduction order mostly doesn't matter. 
In the probabilistic case, this may not be true, because $\beta$-reduction can duplicate $\tsample$s, so the outputs of the copies of the sample may be identical or independent, depending on whether the sample is taken before or after $\beta$-reduction. 
There are, however, some restricted variations on the reduction order that do not have this problem.

\paragraph{}
Even with this restriction, a sampling semantics in the style of the one already defined would not be entirely Church-Rosser, as, for example, $\red^3(\tsample - \tsample, (1,0,\dots))$ would be either $1$ or $-1$ depending on the order of evaluation of the $\tsample$s, as that determines which sample from the pre-selected sequence is used for each one. To fix this, rather than pre-selecting samples according to the order they'll be drawn in, select them according to the position in the term where they'll be used instead.

A \emph{position} 
%\akr{I'm not really sure how to phrase the citation, but this is adapted from \cite{Kennaway96infinitarylambda}.} 
%\lo{The idea is common enough. I think no citation necessary.}
is a finite sequence of steps into a term, defined inductively as
\begin{align*}
P ::= \cdot \mid \lambda ; P \mid @_1 ; P \mid @_2 ; P \mid \underline f_i ; P \mid \tY ; P \mid \textsf{if}_1 ; P \mid \textsf{if}_2 ; P \mid \textsf{if}_3 ; P \mid \tscore ; P.
\end{align*}
The \emph{subterm of $M$ at a position $P$}, denoted $M \mid P$, is defined as
\begin{align*}
M \mid \cdot & = M \\
\lambda x. M \mid \lambda ; P & = M \mid P \\
M_1 M_2 \mid @_i ; P & = M_i \mid P \quad \text{for } i = 1,2 \\
\underline f(M_1,\dots,M_n) \mid \underline f_i ; P & = M_i \mid P \quad \text{for }i \leq n \\
\tY M \mid \tY ; P & = M \mid P \\
\tif{M_1 < 0}{M_2}{M_3} \mid \textsf{if}_i ; P & = M_i \mid P \quad \text{for } i = 1,2,3 \\
\tscore(M) \mid \tscore ; P & = M \mid P
\end{align*}
so that every subterm is located at a unique position, but not every position corresponds to a subterm (e.g. $x \, y \mid \lambda ; \cdot$ is undefined). 
A position such that $M\mid P$ does exist is said to \emph{occur} in $M$. 
%\changed[lo]{
\emph{Substitution} of $N$ at position $P$ in $M$, written $M[N/P]$, is defined similarly.
For example, let 
\[
M = \lambda x \, y. y \, (\tif{x < 0}{y \, (\underline f (x))}{\underline 3})
\qquad
P =\lambda ; \lambda ; @_2 ; \textsf{if}_2 ; @_2
\]
then 
\(
M[\tsample / P] = \lambda x \, y. y \, (\tif{x < 0}{y \, \tsample}{\underline 3}).
\)
%}

Two subterms $N_1$ and $N_2$ of a term $M$, corresponding to positions $P_1$ and $P_2$, can overlap in a few different ways. 
If $P_1$ is an initial segment (i.e.~prefix) of $P_2$ (written as $P_1 \leq P_2$), then $N_2$ is also a subterm of $N_1$. If neither $P_1 \leq P_2$ nor $P_1 \geq P_2$, the positions are said to be disjoint. 
This is mostly relevant in that in any substitution at some position, the subterms at all other positions are unaffected. 
%\lo{The preceding sentence is unclear. Typo? Mostly $\mapsto$ most?}

With this notation, a more general reduction relation $\to$ can be defined. \akr{It would almost work to have one of the neater versions of $\tY$-reduction here.}
%\lo{On first reading, I struggled to parse the following clauses.}
\begin{definition}
\label{def:more general red}
The binary relation $\to$ is defined by the following rules, each is conditional on a redex occurring at position $P$ in the term $M$:
\begin{align*}
  \text{if } M \mid P = (\lambda x.s) v,\ & M \to M[s[v/x]/P] \\
  \text{if } M \mid P = \underline f (\underline r_1, \dots , \underline r_n),\ & M \to M[\underline{f(r_1,\dots,r_n)}/P] \\
  \text{if } M \mid P = \tY \lambda x. s,\ & M \to M[\lambda z. s[(\tY \lambda x. s)/x] z/P] \text{ where $z$ is not free in $s$}\\
  \text{if } M \mid P = \tif{\underline r < 0}{s_1}{s_2},\ & M \to M[s_1/P] \text{ where }r < 0 \\
  \text{if } M \mid P = \tif{\underline r < 0}{s_1}{s_2},\ & M \to M[s_2/P] \text{ where }r \geq 0 \\
  \text{if } M \mid P = \tsample & \text{ and $\lambda$ does not occur after $@_2$ or $\tY$ in $P$},\\ & M \to M[\underline r/P] \text{ where } r \in [0,1] \\
  \text{if } M \mid P = \tscore(\underline r),\ & M \to M[\underline r/P].
\end{align*}
In each of these cases, $M \mid P$ is the \emph{redex}, and reduction takes place at $P$.
\end{definition}

\paragraph{}
Labelling the pre-chosen samples by the positions in the term would also not work because in some cases, a $\tsample$ will be duplicated before being reduced, for example, in $(\lambda x. x \, {\underline 0} \, \mathbin{\underline{+}} \, {x \, \underline 0}) (\lambda y. \tsample)$, both of the $\tsample$ redexes that eventually occur originate at $@_2 ; \lambda$. 
It is therefore necessary to consider possible positions that may occur in other terms reachable from the original term. 
Even this is itself inadequate because some of the positions in different reachable terms need to be considered the same, and the number of reachable terms is in general uncountable, which leads to measure-theoretic issues.

Define a \emph{skeleton} to be a term but, instead of having real constants $\underline r$, it has a placeholder $\skeletonPlaceholder$, so that each term has a skeleton $Sk(M)$ and each skeleton can be converted to a term $S[r]$ given a vector of $n$ real numbers to substitute in, where $n$ is the number of occurrences of $\skeletonPlaceholder$ in $S$. 
\lo{Skeleton is defined in Sec.~\ref{sec:sampling semantics}.}
The positions in a skeleton and the reduction relation on skeletons can be extended from the definitions on terms in the obvious way, with $\tif{\skeletonPlaceholder < 0}{A}{B}$ reducing nondeterministically to both $A$ and $B$, \changed[lo]{and $M \to M[\skeletonPlaceholder / P]$, in case $M \mid P = \tsample$ and $\lambda$ does not occur after $@_2$ or $\tY$ in $P$.
(By abuse of notation, we use the same metavariables $M, N$, etc.~for skeletons as for terms.)}
For example, we have 
\(
(\lambda x . \tif{x < 0}{x}{\skeletonPlaceholder}) \, \tsample
\to
(\lambda x . \tif{x < 0}{x}{\skeletonPlaceholder}) \, \skeletonPlaceholder
\to
\tif{\skeletonPlaceholder < 0}{\skeletonPlaceholder}{\skeletonPlaceholder}
\to
\skeletonPlaceholder
\)

Given a closed term $M$, let $L_0(M)$ be the set of pairs, the first element of which is a $\to$-reduction sequence of skeletons starting at $Sk(M)$, and the second of which is a position in the final skeleton of the reduction sequence. 
\akr{I've yet to go through and replace references to terms with references to skeletons, but it should be reasonably obvious where the substitution should take place. 
This is potentially even more confusing though because I'm now implicitly identifying terms, skeletons, reduction sequences and skeletal reduction sequences.} 
Reduction sequences are used rather than reachable \changed[lo]{skeletons} because if the same \changed[lo]{skeleton} is reached twice, different samples may be needed, \lo{Give a simple example illustrating this.} but they will be discussed as though they were terms, by abuse of notation. 

\lo{Readers would benefit greatly from some examples of an $L_0(M)$-pair, and the relations $\sim_p$ and $\sim_c$, here, before the formal definitions. Explain the underlying intuition.} 

\begin{definition}
The relation $\sim$ is defined as the union of the symmetric relations $\sim_p$ \changed[lo]{($p$ for parent-child)} and $\sim_c$ \changed[lo]{($c$ for cousin)} where
\lo{As defined, $\sim_p$ is not a symmetric relation.}
\begin{enumerate}
    \item If $N$ reduces to $O$ with the redex at position $P$ \changed[lo]{(i.e.~$M \to^\ast N \to O = N[\dotDelta / P]$ with $O \mid P = \Delta$ for some redex contraction $\Delta \to \dotDelta$)}, and $Q$ is a position in $N$ disjoint from $P$, then $(N,Q) \sim_p (O,Q)$.
    
    \item If $N$ $\beta$-reduces to $O$ at position $P$, $Q$ is a position in $N \mid P;@_1;\lambda$ and $N \mid P;@_1;\lambda;Q$ is not the variable involved in the reduction, $(N,P;@_1;\lambda;Q) \sim_p (O, P;Q)$
    
    \item If $N$ $\textsf{if}$-reduces to $O$ at position $P$, with the first resp. second branch being taken, and $P;\textsf{if}_i;Q$ occurs in $N$ (where $i = 2$ resp. $3$), $(N,P;\textsf{if}_i;Q) \sim_p (O,P;Q)$
    
    \item If $N$, $O_1$ and $O_2$ match any of the following cases:
    \begin{enumerate}
        \item $N$ contains redexes at disjoint positions $P_1$ and $P_2$, $O_1$ is $N$ reduced first at $P_1$ then $P_2$ and $O_2$ is $N$ reduced first at $P_2$ then at $P_1$.
        
        \item $N \mid P = \tif{\underline r < 0}{N_1}{N_2}$, where $r < 0$ (or, respectively, $r \geq 0$), $(N_2 \text{ resp. } N_1) \mid Q$ is a redex, and $O_1$ is $N$ reduced at $P$ and $O_2$ is $N$ reduced first at $P;(\textsf{if}_3 \text{ resp. } \textsf{if}_2);Q$ then at $P$
        
        \item $N \mid P = \tif{\underline r < 0}{N_1}{N_2}$, where $r < 0$ (or, respectively, $r \geq 0$), $(N_1 \text{ resp. } N_2) \mid Q$ is a redex, and $O_1$ is $N$ reduced first at $P$ then at $P;Q$ and $O_2$ is $N$ reduced first at $P;(\textsf{if}_2 \text{ resp. } \textsf{if}_3);Q$ then at $P$
        
        \item $N \mid P = (\lambda x. A) B$, there is a redex in $A$ at position $Q$, $O_1$ is $N$ reduced first at $P$ then at $P;Q$, and $O_2$ is $N$ reduced first at $P;@_1;\lambda;Q$ then at $P$
        
        \item $N \mid P = (\lambda x. A) B$, $B \mid Q$ is a redex, $(R_i)_i$ is a list of all the positions in $A$ where $A \mid R = x$, ordered from left to right, $O_1$ is $N$ reduced first at $P ; @_2 ; Q$ then at $P$, and $O_2$ is $N$ reduced first at $P$ then at $P;R_i;Q$ for each $i$ in order.
        
        \item $N \mid P = \tY (\lambda x. A)$, $A$ reduced at $Q$ is $A'$, $(R_i)_i$ is a list of all the positions where $A' \mid R  = x$, ordered from left to right, $O_1$ is $N$ reduced first at $P;\tY;\lambda;Q$ then at $P$, and $O_2$ is $N$ reduced first at $P$ then at $P;\lambda;@_1;Q$ then at $P;\lambda;@_1;R_i;Y;\lambda;Q$ for each $i$ in order.
    \end{enumerate}
    (in which case $O_1$ and $O_2$ are equal as terms, but with different reduction sequences), $O_1'$ and $O_2'$ are the results of applying some reduction sequence to each of $O_1$ and $O_2$ (the same reductions in each case, which is always possible because they're equal terms), and $S$ is a position in $O_1'$ (or equivalently $O_2'$), then $(O_1',S) \sim_c (O_2',S)$.
\end{enumerate}
See \Cref{fig:adk-diagram} for a graphical depiction of $\sim_p$- and $\sim_c$-rules.
\end{definition}

\begin{figure}
\include{sim-diagram}
\caption{\label{fig:adk-diagram} Some examples of the $\sim_p$- and $\sim_c$-rules}
\end{figure}

\lo{TODO: Typeset Fig.~\ref{fig:adk-diagram} in vector graphics using PGF/TikZ.}

The reflexive transitive closure $\sim^*$ of this relation is used to define the set of \emph{potential positions} $L(M) = L_0(M) / \sim^*$, and each equivalence class can be considered as the same position as it may occur across multiple reachable \changed[lo]{skeletons}. 
If $(N,P) \sim^* (O,Q)$, then $N \mid P$ and $O \mid Q$ both have the same shape (i.e.~they're either \changed[lo]{both placeholders}, both variables, both applications, both $\tsample$s etc.), therefore it's well-defined to talk of the set of potential positions where there is a $\tsample$, $L_s(M)$. 
\changed[lo]{Formally $L_s(M) := \{[(X, P)]_{\sim^\ast_M} : X \mid P = \tsample\}$.}
The new sample space is then defined as $I^{L_s(M)}$, with the Borel $\sigma$-algebra and product measure.
\changed[lo]{Since $I^{L_s(M)}$ is a countable product, the measure space is well-defined \citep[Cor.~2.7.3]{AshDD00}.}

\iffalse
\changed[lo]{Since $L_s(M)$ is a countable set, there is a unique probability measure $\mu$ on the infinite product $\prod_{i \in L_s(M)} \Sigma_I$ of the Borel $\sigma$-algebras $\Sigma_I$ satisfying
\[
\mu\set{\omega \in \Omega \mid \omega_1 \in A_1, \cdots, \omega_n \in A_n} = \prod_{i=1}^n \Leb(A_i)
\]
where $\Leb$ is the Lebesgue measure on the measurable space $(I, \Sigma_I)$.}
\fi

\paragraph{}
\akr{For now, here's a summary of the lemmas and theorems I intend to prove about this alternative semantics:
\begin{itemize}
    \item If $M \to N$, the relation $\sim_N^*$ defined on $Rch(N)$ is equal to the restriction of $\sim_M^*$ to $Rch(N)$ %\lo{Define $\sim_N^*$.}
    \item Given the definition below of $\Rightarrow$, which is still applicable to this version of $\sim$, and which is the equivalent of $\red$ in this version of the semantics, $\Rightarrow$ is Church-Rosser.
    \item If $M \to N$ at $P$, no position in any term in $Rch(N)$ is related to $(M,P)$ by $\sim_M^*$, and therefore each sample is used at most once in any reduction sequence.
    \item The subprobability distribution of values obtained by reducing a term in the usual call-by-value semantics is equal to that obtained by any reduction strategy on $\Rightarrow$ (i.e.~a sub-relation of $\Rightarrow$ that is a function) (with some allowance made for the fact that some values are not normal forms of $\Rightarrow$, so the reduction strategy might overshoot the value that $\red^\infty$ reaches. I'm not quite sure what form this will take.)
    \item Similarly, the probability of termination is equal in $\red^\infty$ and any reduction strategy on $\Rightarrow$, so almost sure termination can be equivalently defined based on $\Rightarrow$.
    \item For any closed term $N$, if there is any reduction strategy for $N$ and ranking function corresponding to that reduction strategy, $N$ is AST.
    \item Corresponding sparse and antitone-strict ranking function theorems.
\end{itemize}}

\paragraph{}
Before defining the new version of the reduction relation $\red$, a few lemmas about the properties of $\sim$ are necessary for it to be well-defined.

\begin{lemma} \label{downAcrossUp}
If $(N_1,P_1) \sim^* (N_2,P_2)$, there is some descendants $N'_1, N'_2$ of $N_1$ resp.~$N_2$, and a position $P'$ in both of them such that $(N_1,P_1) \sim_p^* (N'_1, P') \sim_c^* (N'_2, P') \sim_p^* (N_2, P_2)$.
\end{lemma}
\begin{proof}
The $\sim$ relation can be split into $\sim_\downarrow \cup \sim_\uparrow$, where $(A,P_A) \sim_\downarrow (B,P_B)$ if either $A \to B$ and $(A,P_A) \sim_p (B,P_B)$ or $(A,P_A) \sim_c (B,P_B)$, and similarly $(A,P_A) \sim_\uparrow (B,P_B)$ if either $B \to A$ and $(A,P_A) \sim_p (B,P_B)$ or $(A,P_A) \sim_c (B,P_B)$. The sequence of $\sim$ relations linking $N_1$ to $N_2$ can then be expressed as an alternating sequence of $\sim_\downarrow^*$ and $\sim_\uparrow^*$s, starting with $\sim_\downarrow^*$ and ending with $\sim_\uparrow^*$, and the proof proceeds by induction on the number of these alternations.

In the base case, that $(N_1,P_1) \sim_\downarrow^* X \sim_\uparrow^* (N_2,P_2)$, \dots

\akr{Actually proving that this process terminates will be a pain although I'm confident it will work, but the basic idea is that the sequence of $\sim$ steps can be altered by repeatedly replacing the subsequence $\sim_c, \sim_{p,\downarrow}$ with $\sim_{p,\downarrow}, \sim_c$, the subsequence $\sim_{p,\uparrow}, \sim_c$ with $\sim_c, \sim_{p,\uparrow}$, and the subsequence $\sim_{p,\uparrow}, \sim_{p,\downarrow}$ with $\sim_{p,\downarrow}^*, \sim_c, \sim_{p,\uparrow}^*$, until everything is in the correct order.}
\end{proof}


If $(X,P) \sim_c (Y,P)$ then the final skeletons in $X$ and $Y$ are equal, and $(X,Q) \sim_c (Y,Q)$, therefore $\sim_c$ can be considered to apply to reduction sequences even without positions, $X \sim_c Y$ iff $(X,\cdot) \sim_c (Y,\cdot)$.

\begin{lemma} \label{canonicalCousins}
For any reduction sequence $X$, there is a unique reduction sequence $X_{cbv}$ that is related to $X$ by $\sim_c$ (for each position in the final term in $X$) and is in call-by-value order, and if $X \sim_c^* Y$, then $X_{cbv} = Y_{cbv}$.
\end{lemma}

\lo{As defined, $\sim_c$ is not symmetric.}

\begin{proof}
Note that although the reductions that occur in $X_{cbv}$ are in call-by-value order, it may not be the actual call-by-value reduction sequence because some redexes may remain un-reduced even when other reductions that should happen afterwards occur. It is only required that the reductions that do occur occur in the correct order.
\lo{This paragraph (especially the last sentence) is ambiguous.
What does it mean to say that that a reductions that occur in a reduction sequences are \emph{in CBV order}?}
\iffalse
\lo{So this is permitted?
\[
(II)(I (I \bar I)) \to
(II)(I\bar I) \to
(II)\bar I 
\]
but not this: 
\(
(II)(I (I \bar I)) \to
(II)(I \bar I) \to
(II)\bar I 
\)?}
\fi

Proof sketch:
Starting at $X$, take the last pair of adjacent reductions which aren't in CBV order, and switch them (with $\sim_c$). Repeat this process until CBV order is reached.

If $Y \sim_c X \sim_c^* X_{cbv}$, and the reductions involved in $Y \sim_c X$ are not the last pair of reductions in $Y$ that aren't in CBV order, the order of the $\sim_c$s can be rearranged so that the sequence $Y \sim_c^* X_{cbv}$ is the canonical sequence given above, therefore $Y_{cbv} = X_{cbv}$.

If Y is in CBV order and $Y \sim_c^* X$, then $Y_{cbv} = X_{cbv}$ but also $Y_{cbv} = Y$ therefore $X_{cbv}$ is unique.
\end{proof}

\begin{lemma}
\label{lem:sim-M-N}
The relation $\sim$ is defined on $L_0(M)$ with reference to a particular starting term $M$, so different versions, $\sim_M$ and $\sim_N$, can be defined starting at different terms. If $M \to N$, then $\sim^*_N$ is equal to the restriction of $\sim^*_M$ to $L_0(N)$.
\end{lemma} 

\begin{proof}
$\sim_N^*$ is trivially a subset of $\sim_M^*$ because $\sim_N$ is a subset of $\sim_M$.

In the other direction, suppose $(N_1, P_1) \sim_M^* (N_2, P_2)$ where both $N_1$ and $N_2$ are descendants of $N$. By Lemma~\ref{downAcrossUp}, take $(N_1, P_1) \sim_{p,M}^* (N'_1, P') \sim_{c,M}^* (N'_2, P') \sim_{p,M}^* (N_2, P_2)$. 
The $\sim_p^*$ steps remain within $Rch(N)$, and $\sim_p$ does not depend on the history of the reduction sequences, therefore $(N_1, P_1) \sim_{p,N}^* (N'_1, P') \sim_{c,M}^* (N'_2, P') \sim_{p,N}^* (N_2, P_2)$.

Let $X$ be the call-by-value reduction sequence related to both $N'_1$ and $N'_2$ by $\sim_{c,M}$ given by Lemma~\ref{canonicalCousins}. 
As $M \to N$ is the first reduction in the sequence $N'_1$, it is the last to be affected by the $\sim_{c,M}$ sequence $N'_1 \sim_{c,M}^* X$ given by Lemma \ref{canonicalCousins} therefore it can be split into $N'_1 \sim_{c,M}^* Y_1 \sim_{c,M}^* X$ where $Y_1$ is in CBV order except possibly for its first reduction, which is still $M -> N$. Let the position of the reduction $M \to N$ be $Q$. 
\akr{TODO: fill in the details that are missing here. It shouldn't be too hard, it's just a bit sketchy at present.} 
The rearrangement of the reduction sequence $Y_1 \sim_{c,M} X$ consists of moving the reduction at $Q$ down past the other reductions in $Y_1$, possibly duplicating or deleting it in the process, but not affecting the positions or the correct order for any of the other reductions. 
The reductions derived from $M \to N$ can be identified as follows:

%\begin{definition}
A position in some (reduction sequence of) term(s) is \emph{derived from} another if it is related by the reflexive transitive closure of $\rightsquigarrow$, where $(V,R) \rightsquigarrow (W,T)$ just if $V \to W$ and one of the following cases holds:
\begin{itemize}
    \item $(V,R) \sim_p (W,T)$
    \item $R = T$, $V \to W$ at $A$, and $A > R$
    \item $V \mid A = (\lambda x.U) Z, U | B = x, R = A;@_2;C$ and $T = A;B;C$
    \item $V \mid A = \tY (\lambda x.U), R = A;\tY;\lambda;B$ and $T = A;\lambda;@_1;B$
    \item $V \mid A = \tY (\lambda x.U), U | B = x, R = A;C$ and $T = A;\lambda;@_1;B;C$
\end{itemize}
%\end{definition}

Crucially, the reductions in $X$ can be partitioned into 2 sets: those at positions derived from $(M,Q)$, and those at positions equal to the reductions in $Y_1$ (in the same order as they occur in $Y_1$. Using the same construction for $N'_2$ shows that the positions of the reductions in $Y_2$ are also the positions of the reductions in $X$ other than those derived from $(M,Q)$, therefore $Y_1 = Y_2$ therefore $N'_1 \sim_{c,M} Y_1 \sim_{c,M} N'_2$ and every reduction sequence in this sequence starts with $M \to N$ therefore $N'_1 \sim_{c,N} N'_2$.

Combining this with the $\sim_{p,N}^*$s at the beginning and end then yields the desired result that $(N_1,P_1) \sim_N^* (N_2, P_2)$, therefore the restriction of $\sim_M^*$ to $\sim_N$'s domain ($Rch(N)$) is a subset of $\sim_N^*$ therefore the two versions of $\sim$ match on this domain, as desired.
\end{proof}


At each reduction step $M \to N$, the sample space must be restricted from $I^{L_s(M)}$ to $I^{L_s(N)}$. 
The injection $L_0(N) \to L_0(M)$ is trivial to define by appending $Sk(M) \to Sk(N)$ to each path, and using Lemma~\ref{lem:sim-M-N}, this induces a corresponding injection on the quotient, $L(N) \to L(M)$. 
The corresponding map $L_s(N) \to L_s(M)$ is then denoted $i(M \to N)$.

Unlike in the purely call-by-value case, the version of the reduction relation that takes into account samples is still a general relation rather than a function, so it is denoted ``$\Rightarrow$'' instead of ``$\red$'', and it relates $\biguplus_{M \in \Lambda_0} I^{L_s(M)}$ to itself.
\changed[lo]{We write an element of $\biguplus_{M \in \Lambda_0} I^{L_s(M)}$ as $(M', s)$ where the term $M' \in \Lambda^0$ identifies the component of the disjoint set, and $s \in  I^{L_s(M')}$.}
\begin{align*}
& (M,s) \Rightarrow (N,s \circ i(M \to N)) \text{ if either} \\
& \qquad \text{$M \to N$ and the redex is not $\tsample$, or} \\
& \qquad \text{$M \mid P = \tsample$, $N = M[\underline{s(Sk(M),P)}/P]$ and $\lambda$ does not occur after $@_2$ or $\tY$ in $P$}
\end{align*}

\paragraph{}
\begin{lemma}
The relation $\Rightarrow$ is Church-Rosser.
\end{lemma}
\begin{proof}
Suppose that $(M,s) \Rightarrow (M_1,s_1)$ and also $(M,s) \Rightarrow (M_2,s_2)$, then it is required to prove that there is some $(M',s')$ such that both $(M_1,s_1) \Rightarrow^* (M',s')$ and $(M_2,s_2) \Rightarrow^* (M',s')$. Let the positions of the redexes in $M \to M_1$ and $M \to M_2$ be $P_1$ and $P_2$ respectively.

First consider the case that $P_1$ and $P_2$ are disjoint. Let $M_1 = M[X_1/P_1]$ and similarly for $X_2$, then let $M' = M[X_1/P_1][X_2/P_2]$. As the positions are disjoint, the substitutions commute and both $M_1$ and $M_2$ reduce (with $\to$) to $M'$. 
Let $s' = s \circ i(M \to M_1) \circ i(M_1 \to M')$. 
The injection $i(M \to M_1) \circ i(M_1 \to M')$ consists of prepending $M \to M_1 \to M'$ to each reduction sequence, but by case 4.a of $\sim$, this is equivalent to prepending $M \to M_2 \to M'$, which is $i(M \to M_2) \circ i(M_2 \to M')$. 
In the case that the reduction $M \to M_2$ isn't a $\tsample$-reduction, this is enough to establish that $(M_1, s_1) \Rightarrow (M', s')$, and vice-versa. 
If it is $\tsample$ though, in order for it to be the case that $(M_1, s_1) \Rightarrow (M', s')$, it is additionally necessary that $X_2$, the result of the reduction at $P_2$, be $s_1(M_1, P_2)$, which follows from the fact that $(M, P_2) \sim (M_1, P_2)$ by case 1. 
The case that the reduction $M \to M_1$ is a $\tsample$-reduction is similar.

The case that $P_1 = P_2$ is trivial, because there is at most one possible $\Rightarrow$ reduction at any given position, therefore $(M_1, s_1) = (M_2, s_2)$ already.

The remaining case is that $P_1 < P_2$ or $P_1 > P_2$. 
Assume without loss of generality that $P_1 < P_2$. 
For each possible case of what type of redex $M | P_1$ is, and $P_2$'s position within it, there is a corresponding case of $\sim_c$, and similarly to the case where $P_1$ and $P_2$ are disjoint, the term $O_1 = O_2$ from the definition of $\sim_c$ is a suitable value of $M'$. 
The term $M \mid P_1$ can't be $\tsample$, because it has strict subterms, but the case that $M \mid P_2 = \tsample$ is still somewhat more complicated. 
$P_2$ can't be within $P_1 ; @_2$ or $P_1 ; Y$ (cases e and f of $\sim_c$) because those are not valid positions to reduce a $\tsample$, 
and the case that $P_2$ is within the branch of an if statement that is deleted by the reduction at $P_1$ (case b) doesn't actually present a problem because there is no corresponding reduction to $M \to M_2$ in the other branch, 
which leaves cases c and d, that $P_2$ is in the $\tif{\cdot}{\cdot}{\cdot}$ branch that isn't deleted, and that $P_2 < P_1 ; @_1 ; \lambda$.  
The values that the $\tsample$s take in these cases match because $(M,P) \sim_p (M_1, P;Q)$ by cases 3 and 2 of $\sim_p$ respectively.
\end{proof}

\begin{lemma} \label{simpDoesn'tMix}
If $A$ is some descendant of $M$ and $(A,D) \sim^* (M,E)$, then $(A,D) \sim_p^* (M,E)$, with the length of the reduction sequences decreasing by one each step from $A$ to $M$.
\end{lemma}
\begin{proof}
This is a simple induction on $\sim^*$. In the base case, $(A,D) = (M,E)$ therefore $(A,D) \sim_p^* (M,E)$ trivially. Otherwise, suppose that $(M,E) \sim_p^* (B,F) \sim (A,D)$. Either $(B,F) \sim_p (A,D)$ or $(B,F) \sim_c (A,D)$. In the first case, either $B \to A$, in which case the result follows directly, or $A \to B$, in which case the fact that each (reduction sequence of) term(s) has only one parent implies that a $(M,E) \sim_p^* (A,D)$ directly as a subsequence of the path to $(B,F)$.

In the $\sim_c$ case, consider the definition of $\sim_c$. Either $O'_1 = A$ and $O'_2 = B$ or vice-versa. As $M \to^* N \to^* O_1 \to^* B$ and $(M,E) \sim_p^* (B,F)$, and $\sim_p$ only relates positions in a term and its parent, there are some positions $G, H$ such that $(M,E) \sim_p^* (N,G) \sim_p^* (O_1, H) \sim_p^* (B,F)$. It follows that $(O_2, H) \sim_p^* (A,D)$ by following the same path, therefore it suffices to provide the only missing portion of the path from $M$ to $A$, i.e.~to prove that $(N,G) \sim_p^* (O_2,H)$ given $(N,G) \sim_p^* (O_1,H)$ (or vice-versa).

If $G$ is disjoint from all the positions of reduction from $N$ to $O_1$ and $O_2$ (and consequently $G = H$), this follows from case 1 of $\sim_p$. Otherwise, this can be proved by taking cases from the definition of $\sim_c$. This is rather long, but all of the cases are similar. The general idea is that the reductions from $N$ to $O_1$ correspond to the reductions from $N$ to $O_2$, so that if a position is related by $\sim_p$ across that reduction, it is related in the other branch for the same reason. Case d, where $B = O'_1$ rather than $O'_2$, is given here in more detail as an illustrative example:

Let $I$ be $N$ reduced at $P$, and $J$ be $N$ reduced at $P;@_1;\lambda;Q$, so that $N \to I \to O_1$ and $N \to J \to O_2$. All of the reduction positions are $\geq P$, and $G$ is not disjoint from all of them, therefore $G$ is not disjoint from $P$. Let $K$ be the position such that $(N,G) \sim_p (I,K) \sim_p (O_1,H)$. The fact that $(N,G) \sim_p (I,K)$ implies that $G > P;@_1;\lambda$. Let $G = P;@_1;\lambda;L$ and $H = P;L'$. If $L$ is disjoint from $Q$, then $(N,G) \sim_p (J,G) \sim_p (O_2,P;L') = (O_2;H)$ by cases 1 and 2 of $\sim_p$. In the other case, that $L$ isn't disjoint from $Q$, $L > Q$ because none of the positions $<= P;@_1;\lambda;Q$ in $N$ are related to any position in $I$ by $\sim_p$. As $(I,P;L) \sim_p (O_1,P;L')$ (with the redex at $P;Q$), for exactly the same reason $(N,P;@_1;\lambda;L) \sim_p (J,P;@_1;\lambda;L')$ (with the redex at $P;@_1;\lambda;Q$). Because $(N,P;@_1;\lambda;L) \sim_p (J,P;@_1;\lambda;L')$, $N|P;@_1;\lambda;L = J|P;@_1;\lambda;L'$, and $N|P;@_1;\lambda;L \neq $ the variable of $N|P;@_1$, therefore $J|P;@_1;\lambda;L'$ is also not the variable therefore $(J,P;@_1;\lambda;L') \sim_p (O_2, P;L') = (O_2, H)$ by case 2 of $\sim_p$. Combining these results, $(N,G) \sim_p (O_2;H)$ as desired.
\end{proof}

\begin{lemma} \label{lem:redexDestroyed}
If $M \to N$, with the redex at position $P$, then no position in any term reachable from $N$ is related by $\sim^*$ to $(M,P)$.
\end{lemma}
\begin{proof}
Suppose on the contrary that $(M,P) \sim^* (A,D)$, where $N \to^* A$, then by Lemma \ref{simpDoesn'tMix}, $(M,P) \sim_p^* (A,D)$. $M \neq A$ therefore $(M,P) \sim_p$ some position in $N$, but in all the cases of the definition of $\sim_p$, no position in the child term is related to the position of the redex.
\end{proof}

\paragraph{}
The reduction relation $\Rightarrow$ is nondeterministic, so it admits multiple possible reduction strategies. 
A \emph{reduction strategy} starting from a closed term $M$ is a partial function $f$ from $Rch(M)$ to positions, such that for any reachable term $N$ where $f$ is defined, $f(N)$ is a position of a redex in $N$, and if $f(N)$ is not defined, $N$ is a value.
Using a reduction strategy $f$, a subset of $\Rightarrow$ that isn't nondeterministic, $\Rightarrow_f$, can be defined by $(N,s) \Rightarrow_f (N',s')$ just if $(N,s) \Rightarrow (N',s')$ and $N$ reduces to $N'$ with the redex at $f(N)$.

Essentially what Lemma~\ref{lem:redexDestroyed} demonstrates is that the samples taken during any reduction sequence are independent of each other. This is made more precise in the following lemmas.

\begin{lemma}
For any skeletons $M \to N$, with the redex at position $P$, and measurable set of samples $S \subset I^{L_s(N)}$, $\mu(S) = \mu(\{s \in I^{L_s(M)} \mid s \circ i(M \to N) \in S\})$, and furthermore, if $M | P = \tsample$, for any $S \subset I \times I^{L_s(N)}$, $\mu(S) = \mu(\{s \in I^{L_s(M)} \mid (s(M,P), s \circ i(M \to N)) \in S\})$.
\end{lemma}
\begin{proof}
If the result holds for all sets $S$ of the form $\{s \in I^{L_s(N)} \mid \forall j : s(j) \in x_i\}$, where $(x_j)_{j \in J}$ is a family of measurable subsets of $I$ indexed by some finite set $J \subset L_s(N)$ of positions, it also holds for all other $S$ by taking limits and disjoint unions.

Take such a $(x_j)_{j \in J}$, where $x_j = x_k$ and define $K = i(M \to N)[J]$. Because $i(M \to N)$ is injective, it defines a bijection between $J$ and $K$. We can then calculate the measure $\mu(\{s \in I^{L_s(M)} \mid s \circ i(M \to N) \in S\}) = \mu(\{s \in I^{L_s(M)} \mid \forall k : s(k) \in x_{i(M \to N)^{-1}(k)}\}) = \prod_{k \in K} \mu_I(x_{i(M \to N)^{-1}(k)}) = \prod_{j \in J} \mu_I(x_j) = \mu(S)$.

In the case that $M|P = \tsample$, we can similarly consider only those sets $S$ of the form $x_\tsample \times \{s \in I^{L_s(N)} \mid \forall j : s(j) \in x_i\}$. The position $P$ in $M$ is not related to any position in $L_s(N)$, therefore $(M,P) \not \in K$. Again, we can calculate the measure $\mu(\{s \in I^{L_s(M)} \mid (s(M,P), s \circ i(M \to N)) \in S\}) = \mu(\{s \in I^{L_s(M)} \mid s(M,P) \in x_\tsample, \forall k : s(k) \in x_{i(M \to N)^{-1}(k)}\}) = \mu_I(x_\tsample) \prod_{k \in K} \mu_I(x_{i(M \to N)^{-1}(k)}) = \mu_I(x_\tsample) \prod_{j \in J} \mu_I(x_j) = \mu(S)$.
\end{proof}

\begin{lemma} \label{lem:independentSamples}
For any initial term $M$, reduction strategy $f$ on $M$, natural number $n$, skeleton $N$ with $k$ holes, measurable set $T \subset \mathbb R ^ k$ and measurable set $S$ of samples in $I^{L_s(N)}$, $\mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in S : (M,s) \Rightarrow_f^n (N[r], s')\}) = \mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in I^{L_s(N)} : (M,s) \Rightarrow_f^n (N[r], s')\}) \mu(S)$
\end{lemma}
\begin{proof}
Suppose, to begin with, that $n = 0$. Either $\exists r \in T : N[r] = M$, in which case both sides of the equation are $\mu(S)$, or there is no such $r$, in which case both sides of the equation are 0.

For $n > 0$, suppose for induction that the lemma is true for $n - 1$, for all $N, T$ and $S$. If $\exists r \in T, s' \in S : (M,s) \Rightarrow_f^n (N[r], s')$, the $r$ and $s'$ are necessarily unique, therefore we may assume that $T$ is the product of $k$ measurable setsets of $\mathbb R$, $(T_j)_{0 \leq j < k}$, as all of the other cases follow by taking unions and limits of these rectangles. Let\begin{align*}
R_s & = \{\text{the skeleton of } O \mid O \in Rch(M), r \in \mathbb R^k, O | f(O) = \tsample, O \to N[r]\} \\
R_d & = \{\text{the skeleton of } O \mid O \in Rch(M), r \in \mathbb R^k, O | f(O) \neq \tsample, O \to N[r]\}.
\end{align*}
Together, $R_s$ and $R_d$ contain all of the skeletons of terms that can reduce to $N$, and they're disjoint. For each of these skeletons, there is a map $g_O$ from the reals in $O$ to the reals in $N$, i.e. $g_O(r) = r'$ if $O[r] \to N[r']$ for $O \in R_d$, and $g_O(s, r) = r'$ if $O[r] \to N[r']$ with $s$ as the value the sample takes in the reduction. All of these functions are measurable, as they are just rearrangements of vector components, or in the case that $O|f(O) = \underline x(\skeletonPlaceholder, \dots, \skeletonPlaceholder)$, a combination fo the measurable function $x$ with a rearrangement of vector components. In the case that $O \in R_s$, the function $g_O$ simply inserts the sample into the vector of reals at a certain index, call it $h_O$.
\begin{align*}
& \mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in S : (M,s) \Rightarrow_f^n (N[r], s')\}) \\
= & \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_s, r \in \mathbb R^{k-1}, s' \in I^{L_s(O)}, r' \in T, s'' \in S : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) + \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_d, r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}, r' \in T, s'' \in S : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \mathbb R^{k-1}, s' \in I^{L_s(O)}: g_O(r,s'(O,f(O))) \in T, s' \circ i(O[r] \to N[g_O(r,s'(O,f(O)))]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, s' \circ i(O[r] \to N[g_O(r)]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)}: s'(O,f(O)) \in T_{h(O)}, s' \circ i(O[r] \to N[g_O(r,s'(O,f(O)))]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, s' \circ i(O[r] \to N[g_O(r)]) \in S, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)} : (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(\{s' \in I^{L_s(O)} \mid s'(O,f(O)) \in T_{h(O)}, s' \circ i(O[r] \to N[g_O(r,s'(O,f(O)))]) \in S\}) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu({s' \in I^{L_s(O)} \mid s' \circ i(O[r] \to N[g_O(r)]) \in S}) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)} : (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(T_{h(O)}) \mu(S) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \prod_{j \neq h(O)} T_j, s' \in I^{L_s(O)} : s'(O,f(O)) \in T_{h(O)}, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) \\
= & \sum_{O \in R_s} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \mathbb R^{k-1}, s' \in I^{L_s(O)} : g_O(r,s'(O,f(O))) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) + \sum_{O \in R_d} \mu(\{s \in I^{L_s(M)} \mid \exists r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}: g_O(r) \in T, (M,s) \Rightarrow_f^{n-1} (O[r], s')\}) \mu(S) \\
= & \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_s, r \in \mathbb R^{k-1}, s' \in I^{L_s(O)}, r' \in T, s'' \in I^{L_s(N)} : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) \mu(S) + \mu(\{s \in I^{L_s(M)} \mid \exists O \in R_d, r \in \bigcup_l \mathbb R^l, s' \in I^{L_s(O)}, r' \in T, s'' \in I^{L_s(N)} : (M,s) \Rightarrow_f^{n-1} (O[r], s') \Rightarrow_f (N[r'], s'')\}) \mu(S) \\
= & \mu(\{s \in I^{L_s(M)} \mid \exists r \in T, s' \in I^{L_s(N)} : (M,s) \Rightarrow_f^n (N[r], s')\}) \mu(S)
\end{align*}
\end{proof}

The usual call-by-value semantics can be implemented as one of these reduction strategies, given by (with $V$ a value and $T$ a term that isn't a value and $M$ a general term)
\begin{align*}
\cbv(T M) & = @_1 ; \cbv(T) \\
\cbv(V T) & = @_2 ; \cbv(T) \\
\cbv(\underline f(V_1, \dots, V_{k-1}, T, M_{k+1}, \dots, M_n)) & = \underline f_k ; \cbv(T) \\
\cbv(\tY T) & = \tY ; \cbv(T) \\
\cbv(\tif{T < 0}{M_1}{M_2}) & = \textsf{if}_1 ; \cbv(T) \\
\cbv(\tscore(T)) & = \tscore ; \cbv(T) \\
\cbv(V) & \text{ is undefined} \\
\cbv(T) & = \cdot \text{ otherwise}
\end{align*}
(this last case covers redexes at the root position).

A closed term $M$ terminates with a given reduction strategy $f$ and samples $s$ if there is some natural number $n$ such that $(M,s) \Rightarrow_f^n (N,s')$ where $f$ gives no reduction at $N$. The term terminates almost surely with respect to $f$ if it terminates with $f$ for almost all $s$.

\begin{theorem} \label{thm:AstEquivalence}
A closed term $M$ is AST with respect to $\cbv$ iff it is AST.
\end{theorem}
\begin{proof}
\akr{I'm not sure whether this proof is sufficiently detailed. It would be pretty easy to fill it in more, just very verbose.}

For both $\red$ and $\Rightarrow_{\cbv}$, the probability measure on the sample space can be used to define a measure on the reduction sequences of each finite length. The distributions are equal for each length, by induction, as follows.

The base case, length 0, is trivial, because in both cases the distribution has probability 1 on the sequence containing only $M$.

For the inductive case, suppose that the distributions of reduction sequences of length $n$ are equal. \akr{TODO: mention the case that it has already terminated by step $n$, or is about to. The two versions of the semantics handle it a little differently.}

For any term $N$ which is not a value, there is a unique environment $E$ and redex $R$ such that $N = E[R]$. The position of the hole in $E$ is equal to $\cbv(N)$, so that $N|\cbv(N) = R$, as the cases in the definition of $\cbv$ match the cases in the definition of environments. If $R \neq \tsample$, let $R'$ be the result of reducing $R$ (which is equal in both versions of the semantics), then $E[R'] = N[R' / \cbv(N)]$ therefore $\red(N,s) = (E[R'],s)$ and $(N,s) \Rightarrow_{\cbv} (E[R'],s)$. The distribution of next terms, conditional on the previous term, in the case that that previous term reduces deterministically, is therefore equal for $\red$ and $\Rightarrow_{\cbv}$.

In the other case that $R = \tsample$, the first (term) part of $\red(N,s)$ is $E[\underline{\pi_h(s)}]$. If $k$ is the number of samples already taken in this reduction sequence, this is equivalent to $E[\underline{s_0(k)}]$, where $s_0$ is the initial sample. For any sufficiently small neighbourhood of $N$ (i.e.~containing only reduction sequences with the same skeletons), the probability of reaching this neighbourhood is independent of $s(k)$, because it depends only on the samples taken so far, and the samples are independent. The distribution of $s(k)$ conditional on reaching $N$ is therefore the uniform distribution on $I$. Similarly for the other semantics, if the distribution of remaining samples after $n$ steps is independent of the term by Lemma \ref{lem:independentSamples}, therefore the distribution of $s(Sk(N),\cbv(N))$ conditional on $N$ is the uniform distribution on $I$. In either case, the distribution of the next term conditional on the previous term, in the case that it reduces randomly, is equal to the image under $r \mapsto E[\underline r]$ of the uniform distribution on $I$.

In any case, the distribution of reduction sequences after $n+1$ steps, conditional on the reduction sequence after $n$ steps, as defined by $\Rightarrow_{\cbv}$ and by $\red$, is equal, any by the inductive hypothesis the distributions after $n$ steps are equal, therefore by integrating over the reduction sequence up to step $n$, the distributions on reduction sequences up to step $n+1$ are equal.

The distributions on reduction sequences of any finite length as defined by $\Rightarrow_{\cbv}$ and $\red$ are therefore equal, therefore so is the probability of having reached a value after $n$ steps, therefore so is its limit, the probability of termination, therefore the probability of termination is 1 iff the probability of termination with respect to $\cbv$ is 1.
\end{proof}

\begin{theorem} \label{thm:CbvIsTerminatingest}
If $M$ terminates with some reduction strategy $f$ and samples $s$, it terminates with $\cbv$ and $s$.
\end{theorem}
\begin{proof}
\end{proof}

\begin{corollary}
If $M$ is AST with respect to any reduction strategy, it is AST.
\end{corollary}
\begin{proof}
Suppose $M$ is AST with respect to $f$. Let the set of samples with which it terminates with this reduction strategy be $X$. By Theorem~\ref{thm:CbvIsTerminatingest}, $M$ also terminates with $\cbv$ and every element of $X$, and $X$ has measure 1, by assumption, therefore $M$ is AST with respect to $\cbv$ therefore by Theorem~\ref{thm:AstEquivalence} it is AST.
\end{proof}

\lo{Give an example or two of a reduction strategy (for $\Rightarrow$) that is not $\cbv$.}

\akr{NOTE. For the standard CBV semantics, this example is indeed not a problem. 
In the alternative semantics, defining 
\begin{align*}
A[x] &= \tif{\tif{x>0}{I}{I} \, \tsample - 0.5 > 0}{0}{\Omega}\\
B &= \tif{\tsample - 0.5 > 0}{0}{\Omega}
\end{align*}
The set of samples where it terminates is 
\[\begin{array}{l}
\bigcup_{r \in [0,1]} \{s \in I^{L_s(A[\tsample])} \mid s([A[\tsample]], \mathsf{if}_1;\underline{ -
}_1;@_1;\mathsf{if}_1) = r, \\
\qquad \qquad \hfill \qquad \qquad s([A[\tsample], A[r], \ldots, B, \mathsf{if}_1) > 0.5 \}.
\end{array}\] 
This is a rather unwieldy expression, but the crucial part is that $r$ occurs twice in the conditions on $s$: once as the value a sample must take, and once in the location of a sample. 
It is this set that's unmeasurable.}

\section{Ranking functions}
Given a probabilistic program (i.e.~a closed term $M$), in order to construct a supermartingale to prove its a.s.~termination, a function to assign values to each reachable program state is necessary. 

Let %$Rch(M) := \{x \in \Lambda \mid \exists (y_i) : M \to y_0 \to \dots \to y_n \to x\}$, 
\changed[lo]{$Rch(M) := \{N \in \Lambda \mid M \to^\ast N\}$},
with the $\sigma$-algebra induced as a subset of $\Lambda$. 

\begin{definition}\rm
A \emph{ranking function on $M$} is a measurable function $f:\mathit{Rch}(M) \to \mathbb{R}$ such that $f(N) \geq 0$ for all $N$, and
\begin{enumerate}
    \item $f(E[\tY \lambda x. N]) \geq 1+ f(E[\lambda z. N[(\tY \lambda x. N)/x] z]) \text{ where $z$ is not free in $N$}$
    \item $f(E[\tsample]) \geq \int_I f(E[\underline{x}]) \, \Leb(\mathrm{d}x)$
    \akr{I'm not sure whether $Rch(M)$ is a measurable subset of $\Lambda$, so it might not have a natural measure, but I'm pretty sure this doesn't actually matter, and the $\sigma$-algebra at least is well-defined. $Rch(M)$ could fail to be measurable if there are measurable functions $\mathbb R^n \to \mathbb R$ with non-measurable ranges. I don't know whether these exist. They do for the Lebesgue $\sigma$-algebra, but I'm using the Borel ones.}
    \lo{It is worth inserting the preceding (including the example of a measurable function with non-measurable range) as a remark (or as a ``commenting.sty'' note).}
    \akr{I'd like to include such a remark, but the only reference I've been able to find for the fact that these functions exist is a Stack Exchange question\footnote{\url{https://math.stackexchange.com/questions/1717282/construction-of-a-borel-measurable-function-mapping-borel-set-to-non-borel-set?rq=1}} which implies that the construction is difficult but doesn't actually say what it is (so I probably won't be able to find it on my own).}

    \item $f(E[R]) \geq f(E[R'])$ for any other redex $R$, where \changed[lo]{$R \to R'$}. \lo{Metavariables: $M$ and $N$ for terms and programs; $R$ / $R'$ for redex / contractum.}
\end{enumerate}
We say that the ranking function $f$ is \emph{strict} if there exists $\epsilon > 0$ such that $f(N) = 0$ iff $N$ is a value, and for all $E$ and $R \to R'$, $f(E[R']) \leq f(E[R]) - \epsilon$.

Any closed term for which a ranking (respectively, strict ranking) function exists is called \emph{rankable} (respectively, \emph{strictly rankable}).
\end{definition}

\section{Supermartingales}

\lo{A brief explanatory note on supermartingales by way of motivation is needed here.}

\lo{Supermartingales is a fitting title of this section.
However the details of the supermartingle construction are suppressed in the proof.
I propose that you reformulate the theorem as follows.}

\begin{theorem} 
\label{thm:rankable gives supermartingale}
\changed[lo]{If a closed SPCF term is rankable by $f$, then $(f(M_n))_{n \in \omega}$ is a supermartingale adapted to the filtration $(\mathcal{F}_n)_{n \in \omega}$ where $\mathcal{F}_n = \sigma(M_1, \cdots, M_n)$.}
\end{theorem}

%\begin{proof}

\lo{We also need to show $f(M_n)$ is integrable.}

Given a closed term $M$ and a ranking function $f$ for it, define random variables on the probability space $S$ (where $s$ is a random variable) by
\begin{align*}
(M_n,s_n) & = \red^n(M,s) \\
y_0 & = 0 \\
y_{n+1} & = \min \{ k \mid k>y_n, M_k \text{ a value or of the form } E[\tY N] \}\\
M'_n & = M_{y_n} \\
X_n & = f(M'_n)
\end{align*}
and define a filtration $\mathcal{F}_n = \sigma(M_k, k \leq n)$ (i.e. all the samples used up to step $n$).

\akr{This is quite a pedantic point but technically the proof wouldn't be correct without it.} In the following proof, conditional expectations (conditioning on $\mathcal F_n$) will be used. Conditional expectations are in general only defined up to a null set, but in this case there is a natural measure on each element of $\mathcal F_n$ (as they're always isomorphic to $S$), and this is the version of the conditional expectation that will be used. \lo{I find this a little cryptic. 
Perhaps I am missing something, but the definition of conditional expectation $\expect{f(M_{n+1}) \mid \mathcal{F}_n}$ is standard.
The stochastic process $(M_n)_{n \in \omega}$ is trivially adapted to the natural filtration $(\mathcal{F}_n)_{n \in \omega}$.}

The expectation of $f(M_{n+1})$ given $\mathcal{F}_n$ is trivially less than or equal to $f(M_n)$ in the cases that $M_n \neq E[\tsample]$, and in the case of $\tsample$,
\lo{This case analysis is problematic. The r.v.~$M_n$ is a measurable function of type $S \to \Lambda^0$ defined by $M_n(s) :=  N$ where $\red^n(M, s) = (N, s_n)$. 
In general, it does not make sense to say ``$M_n = E[\tsample]$'' without referencing a trace (i.e.~an element of of $S$).
Take $M = \tif{\tsample - 0.5 < 0}{\tsample}{1}$.
Then $M_3([0.1, \cdots]) = \tsample$ but $M_3([0.6, \cdots]) = 1$.
The case analysis should take place ``inside the conditional expectation''.}
\begin{align*}
& \mathbb{E}[f(M_{n+1}) \mid \mathcal{F}_n] \\
= & \mathbb{E}[f(M_{n+1}) \mid M_n = E[\tsample],\, \mathcal{F}_n] \\
= & \mathbb{E}[f(E[\pi_h(s_n)]) \mid \mathcal{F}_n] \\
= & \int_0^1 f(E[\underline x]) \, \mathrm{d} x \qquad & \text{as }s_n\text{ is independent of } \mathcal{F}_n \\
\leq & f(E[\tsample]) \qquad & \text{by assumption on } f \\
= & f(M_n),
\end{align*}
\lo{Problem: Some of the above terms (conditional expectation and $f(M_n)$) are random variables (i.e.~functions) but the integral is a real number.}
therefore the values of the ranking function $f(M_n)$ are a supermartingale with respect to $\mathcal{F}_n$.

Given $M'_n$, the reduction relation $\to$, \emph{excluding} $\tY$-reduction steps, is strongly normalising because of the type system, therefore there is some finite bound on the number of reduction steps that can take place from $M'_n$ without a $\tY$-reduction step, therefore $y_{n+1}$ is (conditional on $\mathcal{F}_{y_n+1}$) a bounded stopping time, therefore $\mathbb{E}[f(M_{y_{n+1}}) \mid \mathcal{F}_{y_n+1}] \leq f(M_{y_n+1})$. 
For $n>0$, if $M_{y_n}$ isn't already a value, then $M_{y_n} = E[\tY \lambda x.N]$ for some $E, N$, therefore $M_{y_n+1} = E[\lambda z. N[(\tY \lambda x. N)/x] z]$ and $f(M_{y_n+1}) \leq f(M_{y_n}) - 1$, therefore if $M_{y_n}$ isn't a value, $\mathbb{E}[X_{n+1} \mid \mathcal{F}_{y_n+1}] \leq X_n - 1$.

In the other case that $X_n$ is a value, $X_{n+1} = X_n$, and $X_n = \mathbb E[X_n \mid \mathcal F_{y_n+1}]$ therefore in either case $\mathbb E[X_{n+1} \mid \mathcal F_{y_n+1}] - E[X_n \mid \mathcal F_{y_n+1}] \leq \mathbb -P[M_{y_n} \text{ not a value} \mid \mathcal F_{y_n+1}]$ (as the probability is 0 or 1). Marginalising over $\mathcal F_{y_n+1}$, this implies that $\mathbb{E}[X_{n+1}] - \mathbb{E}[X_{n}] \leq -\mathbb P[M_{y_n} \text{ not a value}]$ therefore as $\mathbb{E}[X_n]$ is bounded below, $\mathbb P[M_{y_n} \text{ not a value}]$ must tend to 0 as $n \to \infty$. $M_{y_n} \text{a value} \Rightarrow M_{y_{n+1}} \text{a value}$ therefore 
\begin{align*}
\mathbb P[M_{y_n} \text{ not a value for infinitely many values of $n$}]  
& = \mathbb P[\forall n: M_{y_n} \text{ not a value}] \\
& \leq \inf_n \mathbb P[M_{y_n} \text{ not a value}] \\
& = 0
\end{align*} 
therefore $M$ terminates almost surely.
%\end{proof}

\input{ranking-supermartingales}

\section{Constructing Ranking Functions}
Although rankability implies almost-sure termination, the converse does not hold in general. For example,
\begin{equation}
\tif{{-}\tsample < 0}{\underline{0}}{(\tY \lambda x. x) \; \underline 0}
\label{ex:0 probability reachable}
\end{equation}
terminates in 3 steps with probability 1, but isn't rankable because $(\tY \lambda x. x) \underline 0$ is reachable, although that has probability 0. 
Not only is this counterexample AST, it's positively almost surely terminating i.e.~the expected time to termination is finite.

A ranking function can be constructed under the stronger assumptions that, for every $N$ reachable from $M$, the expected number of $\tY$-reduction steps from $N$ to a value is finite. In particular, the expected number of $\tY$-reduction steps from each reachable term is a ranking function. Note that a finite number of expected $\tY$-reduction steps does not necessarily imply a finite number of expected total reduction steps.

\begin{example}
\label{ex:tY finite does not imply t finite}
The term
\[
M = \big(\tY \lambda f n. \tif{\tsample - 0.5 < 0}{n \, \underline 0}{f (\lambda x. n (n x))}\big) (\lambda x. x+1)
\] 
terminates with only 2 $\tY$-reductions on average i.e.~$\expect{T_M^\tY} = 2$, but applies the increment function $2^n$ times with probability $2^{-1-n}$ for $n \geq 0$, which diverges i.e.~$\expect{T_M} = \infty$.
\end{example}

\begin{theorem} \label{thm:minimal}
Given a closed term $M$, the function $f:Rch(M) \to \mathbb R$ given by $f(N) = \mathbb E [\text{the number of }\tY\text{-reduction steps from }N\text{ to a value}]$, if it exists, is the least of all possible ranking functions of $M$.
\end{theorem}
\begin{proof}
Let $f$ be the candidate least ranking function defined above, and suppose $g$ is another ranking function such that $f(N) > g(N)$ for some $N \in Rch(M)$. The restrictions of $f$ and $g$ to $Rch(N)$ have the same properties assumed of $f$ and $g$, so assume w.l.o.g.~that $N=M$. The difference $g - f$ is then a supermartingale (with the same setup as in the proof of \Cref{thm:rankable gives supermartingale}
%Theorem \ref{rankable implies ast}) 
therefore $\forall n.\ \mathbb E[g(M_n)] \leq \mathbb E[f(M_n)] + g(M)-f(M)$.

$\mathbb E[f(M_n)] = \sum_{k=n}^\infty \mathbb P[M_n = E[\tY N] \text{ for some }E, N] \to 0 \text{ as } n \to \infty$ therefore as $g(M) - f(M) < 0$, eventually $\mathbb E[g(M_n)] < 0$, which is impossible, therefore $g \geq f$ as required.

In order for $f$ to be the least ranking function of $M$, it also has to actually be a ranking function itself. Each of the conditions on a ranking function is easily verified from the definition of $f$.
\end{proof}

\subsection{Partial ranking functions}
Even in the case of reasonable simple terms, explicitly constructing a ranking function would be a lot of work, and \Cref{thm:minimal} makes even stronger assumptions than almost sure termination, so it isn't useful for proving it. Take, for example, the term
\begin{align*}
&(\tY \lambda f, n: \\
&\quad \tif{\tsample - 0.5 < 0}{n}{f (n+1)} \\
&) \underline{0}
\end{align*}
which generates a geometric distribution.
    Despite its simplicity, its $Rch$ contains all the terms
\begin{enumerate}
    \item $(\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) \underline{0}$
    \item $(\lambda z.(\lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) (\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) z) \underline{i}$
    \item $(\lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) (\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) \underline{i}$
    \item $(\lambda n: \tif{\tsample - 0.5 < 0}{n}{(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{n}{f (m+1)}) (m+1)}) \underline{i}$
    \item $\tif{\tsample - 0.5 < 0}{\underline{i}}{(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{m}{f (m+1)}) (\underline{i}+1)}$
    \item $\tif{\underline r - 0.5 < 0}{\underline{i}}{(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{m}{f (m+1)}) (\underline{i}+1)}$
    \item $\tif{\underline{r - 0.5} < 0}{\underline{i}}{(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{m}{f (m+1)}) (\underline{i}+1)}$
    \item $\underline{i}$
    \item $(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{m}{f (m+1)}) (\underline{i}+1)$
    \item $(\lambda z.(\lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) (\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) z) (\underline{i} + 1)$.
\end{enumerate}
\akr{TODO: make this into a diagram with arrows.}

\lo{Here is a more readable version.}

Writing $\Theta = \lambda f, n : \tif{\tsample - 0.5 < 0}{n}{f (n+1)}$, we have
\begin{enumerate}
    \item $(\tY \, \Theta) \; \underline{0}$

    \item $\big(\lambda z.\Theta \; (\tY \, \Theta) \; z \big) \; \underline{i}$
    
    \item $\Theta \; (\tY \, \Theta) \; \underline{i}$
    
    \item \changed[akr]{$(\lambda n: \tif{\tsample - 0.5 < 0}{n}{(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{n}{f (m+1)}) (m+1)}) \; \underline{i}$}

    \lo{I think the preceding should be: $(\lambda n: \tif{\tsample - 0.5 < 0}{n}{(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{m}{f (m+1)}) (n+1)}) \; \underline{i}$, and so, more succinctly:
    \[
    \big(
    \lambda n: 
    \tif{\tsample - 0.5 < 0}{n}{(\tY \, \Theta) \; (n+1)} 
    \big) 
    \; \underline{i}
    \]}

    \item $\tif{\tsample - 0.5 < 0}{\underline{i}}{
    (\tY \, \Theta) \, (\underline{i}+1)}$
    
    \item $\tif{\underline r - 0.5 < 0}{\underline{i}}{(\tY \, \Theta) (\underline{i}+1)}$
    
    \item $\tif{\underline{r - 0.5} < 0}{\underline{i}}{(\tY \, \Theta) (\underline{i}+1)}$
    
    \item $\underline{i}$
    
    \item $(\tY \, \Theta) \; (\underline{i}+1)$
    
    \item $\big(\lambda z. \Theta \; (\tY \, \Theta) \;  z\big) \; (\underline{i} + 1)$

    \item $\Big(\lambda z. \big(
    \lambda n: 
    \tif{\tsample - 0.5 < 0}{n}{(\tY \, \Theta) \; (n+1)} 
    \big) \, z \Big)
    \; \underline{i}$
\end{enumerate}

\lo{So we have:
\[
\begin{array}{ll}
1 \to 11 \to 4 \to 5 \to 6 \to 7 \to 8\\
7 \to 9 \to 11 \to \cdots
\end{array}
\]
You have 2, 3 and 10 because your $\tY$-redex rule is
\[
\tY \lambda x. s \to \lambda z. (\lambda x.s)\, (\tY \lambda x. s) \, z
\]
which is not the official definition.}

Even in this simple case, defining a ranking function explicitly is awkward because of the number of cases, although in most cases, because the value need only be greater than or equal to that of the next term in sequence, it suffices to take the ranking function as taking only 3 distinct values.
(We will explain why later.)

The definition of rankability is also inconvenient for syntactic sugar. It could be useful, for example, to define $M \oplus_p N = \tif{\tsample - p < 0} M N$, where $M \oplus_p N$ reduces to $M$ or $N$, depending on the first value of $s$, with probability $p$ resp.~$(1-p)$. Technically though, it reduces first to $\tif{\underline r - p < 0} M N$ for $r \in I$, so those terms all need values of the ranking function too.

\paragraph{}
In both of these cases, there are only some values of the ranking function that are semantically important. Define a \emph{partial ranking function} on a closed term $M$ to be a partial function $f : Rch(M) \rightharpoonup \mathbb R$ such that
\begin{itemize}
    \item $f(N) \geq 0$ for all $N$ where $f$ is defined.
    \item $f$ is defined at $M$.
    \item For any $N$ in the domain of definition of $f$, evaluation of $N$ will eventually \akr{It would be nicer for this to be only almost certain, to make this more neatly in-between rankability and ast, but that doesn't actually work because of terms reachable with 0 probability. Of course, astness doesn't imply rankability, so any definition of partial ranking function that includes all Y-past terms wouldn't always extend to a total ranking function.} reach some $O$ which is either a value or in the domain of definition of $f$, and $f(N) \geq \mathbb E[f(O) + \text{ the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}
A partial ranking function that is total is just a ranking function. Providing a partial ranking function is essentially part way between providing a ranking function and directly proving almost sure termination.

\begin{theorem} \label{partial implies rankable}
Every partial ranking function is a restriction of a ranking function.
\end{theorem}
\begin{proof}
Take a closed term $M$ and partial ranking function $f$ on $M$. Define $f_1 : Rch(M) \rightharpoonup \mathbb R$ by
\begin{align*}
    f_1(N) & = f(N) \text{ whenever $f(N)$ is defined,} \\
    f_1(V) & = 0 \text{ for values $V$ not in the domain of $f$.}
\end{align*}

Define $(\nnext(N,s),\_) = \red^n(N,s)$ for the least $n \geq 0$ such that it's in the domain of $f_1$, and $g(N,s) = \left | \{m < n \mid \red^m(N,s) \text{ is of the form } (E[\tY N'],s') \} \right |$. 
The function $\nnext$ is well-defined (i.e.~$n$ is finite) for all $N \in Rch(M)$ by induction on the path from $M$ to $N$, by the third condition on partial ranking functions. Define $f_2(N) = \int_S f_1(\nnext(N,s)) + g(N,s) \, \mu(\mathrm d s)$. The (total) function $f_2$ agrees with $f$ on $f$'s domain, and it is a ranking function on $M$ (in fact, the least ranking function of which $f$ is a restriction).
\lo{TODO. The preceding sentence deserves a proof.}
\end{proof}


As a corollary, any term which admits a partial ranking function terminates almost surely.

\subsection{Examples}
Let $M \oplus_p N = \tif{\tsample - p < 0} M N$, for $p \in (0,1]$, then there are the pseudo-reduction relations
\begin{align*}
E[M \oplus_p N] \to^3 & E[M] & \\
E[M \oplus_p N] \to^3 & E[N] & \\
\red^3(E[M \oplus_p N], s) = & \left\{
    \begin{array}{ll}
        (E[M],\pi_t(s)) & \text{if } \pi_h(s) < p \\
        (E[N],\pi_t(s)) & \text{if } \pi_h(s) \geq p. \\
    \end{array} \right .
\end{align*}
A partial ranking function could be defined with respect to this shortcut reduction simply by replacing $\to$ and $\red$ in the definition of a partial ranking function by a version that goes straight from $N \oplus_p O$ to $N$ or $O$. Such a pseudo-partial ranking function would then be a partial function from a subset of $Rch(M)$, so it could also be considered as a partial function from all of $Rch(M)$, and it would in fact also be an actual partial ranking function. It is therefore possible to prove rankability directly using the shortcutted reductions.

A similar procedure would work for other forms of syntactic sugar. If a closed term $N$ eventually reduces to one of a set of other terms $\{N_i \mid i \in I\}$ with certain probabilities, a partial ranking function defined with respect to a reduction sequence that skips straight from $N$ to $N_i$ is also a valid partial ranking function for the original reduction function, and therefore its existence implies almost sure termination. There is a caveat, however, that $\tY$-reduction steps skipped over in the shortcut still need to be counted for the expected number of $\tY$-reduction steps.

\paragraph{}
With this abbreviation, the geometric distribution example from earlier can be written as $(\tY f, n. n \oplus_{0.5} f(n+1)) \underline 0$. It is then easy to see that the following is a partial ranking function:
\begin{align*}
(\tY f, n. n \oplus_{0.5} f(n+1)) N \mapsto 2 \\
\underline i \oplus_{0.5} (\tY f, n. n \oplus_{0.5} f(n+1)) (\underline i + 1) \mapsto 1 \\
\underline i \mapsto 0
\end{align*}

\lo{Call the above partial function $h$.
I assume that $h: (\tY f, n. n \oplus_{0.5} f(n+1)) N \mapsto 2$ for all $N$ (such that $(\tY f, n. n \oplus_{0.5} f(n+1)) N$ is reachable from $M$), 
and the second and third clauses hold for all $i \geq 0$.

Maybe I have misread / misunderstood the preceding, but I don't see why $h$ is a restriction of a ranking function. 
Suppose $h$ is a restriction of $H$.
Consider the reduction sequence (writing $\Xi = (\tY f, n. n \oplus_{0.5} f(n+1))$):
\[
\Xi \, 0 
\xrightarrow{\tY} 
0 \oplus_{0.5} \Xi \, 1 
\to 
\Xi \, 1
\xrightarrow{\tY} 
1 \oplus_{0.5} \Xi \, 2
\to
\Xi \, 2
\xrightarrow{\tY} 
2 \oplus_{0.5} \Xi \, 3
\]
Since $H(\Xi \, 0) = 2$, we must have 
$H(\Xi \, 1) \leq 1$ and $H(\Xi \, 2) \leq 0$, contradicting the restriction assumption.
}
\akr{
In this case, $H(0 \oplus_{0.5} \Xi 1) \leq 1$, but that term reduces to either $0$ or $\Xi 1$ therefore the actual condition is that $0.5 H(0) + 0.5 H(\Xi 1) \leq 1$, which is possible.
}
\lo{
  OK. I see it now.
}

\section{Antitone ranking functions}

\begin{example}
\label{ex:ac-ranking}
\begin{enumerate}
\item 1D unbiased random walk
\[
M_1 = 
\big(\tY \, f \, n \, . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{1/2} f \, (n + 1)}\big) \, 10
\]

\item 1D biased (away from 0) random walk
\[
M_2 = 
\big(\tY \, f \, n \, . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{1/3} f \, (n + 1)}\big) \, 10
\]

\item 1D biased (towards 0) random walk
\[
M_3 = 
\big(\tY \, f \, n \, . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{2/3} f \, (n + 2)}\big) \, 10
\]


%While {x > 0} do {x := x-1 \oplus_2/3 x:= x+1}

%M2 = While {x > 0} do {x := x-1 \oplus_2/3 x:= x+2}
\end{enumerate}
\end{example}


\paragraph{Problem.} Find a generalised notion of ranking function 
%--- call it \emph{antitone-convex rankable} --- 
so that $M_1$ becomes rankable, and then prove it sound i.e.~rankable in this generalised sense implies AST.

\paragraph{Idea.} 

The definition of antitone-convex ranking function $f$ (say) for $M \in \Lambda^0$ is the same as that of ranking function except that in the case of $\tY$-redex, 
we require the existence of an antitone (meaning: $r < r'$ implies $\epsilon(r) \geq \epsilon(r')$) and convex function $\epsilon : \pReal \to \pReal$ such that the ranking function $f :\mathit{Rch}(M) \to \nnReal$ satisfies
\[
f(E[R]) \leq f(E[R]) - \epsilon(f(E[R])) 
\]
where $R \to R$ is the $\tY$-redex rule.

\lo{An aside: This CBV $\tY$-rule seems cleaner:
\[
\big(\tY f^{A \to B} \, x^A \, . \, \theta^B \big) \, v \to \theta[(\tY f \, x \, . \, \theta) / f, v / x].
\]
We assume $\tY f^{A \to B} \, x^A \, . \, \theta^B$ is a value.}

\begin{definition}
We say that a supermartingale $(Y_n)_{n \in \omega}$ and a stopping time $T$ \akr{Separating the stopping time from $Y_n = 0$ allows for a neater definition of the ranking function of a term} \lo{Nice!} adapted to filtration $(\calF_n)_{n \in \omega}$ are \emph{antitone strict} if $Y_n \geq 0$, and there exists an antitone measurable function $\epsilon : \nnReal \to \pReal$ satisfying
\[
\expect{Y_{n+1} \mid \calF_n} \leq Y_n - \epsilon (Y_n) \cdot 1_{\set{T > n}}
\]
\end{definition}

\begin{theorem}
\label{thm:a-c strict}
Let $(Y_n)_{n \in \omega}$ be an antitone strict supermartingale via function $\epsilon : \nnReal \to \pReal$, and stopping time $T$. 
Then $T < \infty$ almost surely.
\akr{It's not actually true that $\expect{T} \leq \dfrac{\expect{Y_0}}{\epsilon_0}$ where $\epsilon_0 := \epsilon(\expect{Y_0})$. This is fortunate because it means that a rankability theorem based on antitone strict supermartingales actually has a chance of being stronger than the existing one (which was pretty much complete for $\tY$-PAST terms already). In fact, for any almost surely finite stopping time, there is a corresponding antitone strict supermartingale. This seems promising for the method being conditionally complete (for terms where all reachable terms are AST).}
\lo{See \Cref{sec:Relativised completeness}.}
\end{theorem}

\begin{proof}
First, as $(Y_n)$ is a supermartingale, $\expect{Y_n} \leq \expect{Y_0}$, therefore

\begin{calculation}
\expect{Y_n \mid T > n}
  \step[=]{}
\dfrac{\mathbb P[T > n] \expect{Y_n \mid T > n} + \mathbb P[T \leq n] \expect{Y_n \mid T \leq n} - \mathbb P[T \leq n] \expect{Y_n \mid T \leq n}}{\mathbb P[T > n]}
  \step[=]{}
\dfrac{\expect{Y_n} - \mathbb P[T \leq n] \expect{Y_n \mid T \leq n}}{\mathbb P[T > n]}
  \step[\leq]{$Y_n \geq 0$ always}
\dfrac{\expect{Y_n}}{\mathbb P[T > n]}
  \step[\leq]{$(Y_n)_n$ is a supermartingale}
\dfrac{\expect{Y_0}}{\mathbb P[T > n]}
\end{calculation}

\emph{Claim}: for all $0 < x \leq 1$, let $B_x = \left \lceil \dfrac{\expect{Y_0}+1}{x \; \epsilon(\expect{Y_0} \, x^{-1})} \right \rceil$, then
\(
\mathbb P[T > B_x] \leq x.
\)
As the convex hull of $\epsilon$ (the greatest convex function less than or equal to it) satisfies all the conditions assumed of $\epsilon$, in addition to being convex, assume wlog that $\epsilon$ is convex.
Suppose for a contradiction that $\mathbb P[T > B_x] > x$, then for every $n \leq B_x$
\lo{Change of justification of the second equality below: event, not discrete r.v. 
Recall: expectation conditioning on r.v.~(resp.~event) is a r.v.~(resp.~number).}
\begin{calculation}
\expect{Y_n-Y_{n+1}}
  \step[=]{definition and linearity of conditional expectation}
\expect{Y_n - \expect{Y_{n+1} \mid \calF_n}}
  \step[\geq]{antitone strict assumption}
\expect{\epsilon(Y_n) \cdot 1_{\set{T > n}}}
%  \step[=]{definition of conditional expectation for a discrete variable}
\step[=]{definition of expectation conditioning on an \emph{event}}
\mathbb P[T > n]\ \expect{\epsilon(Y_n) \mid T > n}
  \step[\geq]{Jensen's inequality}
\mathbb P[T > n]\ \epsilon(\expect{Y_n \mid T > n})
  \step[\geq]{proved earlier}
\mathbb P[T > n]\ \epsilon\Big(\dfrac{\expect{Y_0}}{\mathbb P[T > n]}\Big)
  \step[>]{assumption, $\mathbb P[T > n] \geq \mathbb P[T > B_x] > x$}
x \; \epsilon\big(\expect{Y_0} \, x^{-1}\big).
\end{calculation}
Therefore, by a telescoping sum
\[
\expect{Y_{B_x}} = \expect{Y_{B_x}-Y_0+Y_0} \leq \expect{Y_0} - B_x \; x \; \epsilon(\expect{Y_0} \, x^{-1}) \leq -1 < 0 
\]
%\lo{$\expect{Y_0 - B_x \; x \; \epsilon(\expect{Y_0} \, x^{-1})}$}
which is a contradiction, therefore the claim must be true, therefore $P[T > n]$ tends to 0 as $n$ tends to infinity, therefore $T < \infty$ almost surely.
\end{proof}

\paragraph{Results to prove:}
\begin{enumerate}
\item (AST Soundness) If a closed SPCF term $M$ is antitone rankable, then $T^{\tY}_M < \infty$ a.s.~(equivalently $M$ is AST). \lo{This should be fine.}

\item A partial ranking function theorem for antitone-convex ranking functions. \lo{TODO}
\end{enumerate}

\paragraph{\Cref{ex:ac-ranking} revisited}

Programs $M_1$ and $M_3$ are AST.
To construct antitone ranking functions, we appeal \lo{(hopefully)} to a partial ranking function theorem for antitone ranking functions.
\iffalse
\lo{NOTE. Suppose a function $f : E \to \Real$ from a convex subset $E \subseteq \Real^n$ has continuous second-order partial derivatives. Then it is convex iff its Hessian is positive semidefinite.}
\fi
\iffalse
For $M_1$, consider the function $g_1(n) := \frac{2^n - 1}{2^{n-1}}$.
\fi

For $M_1$, define the function $g_1 : \nnReal \to [0, 2)$ by $g_1(x) = 2 - \frac{1}{2^{x-1}}$.
Note that $(g_1 (n))_{n \in \omega}$ satisfies the recurrence relation
\[
\left\{
\begin{array}{rll}
z_0 &=& 0\\
n \geq 1, \quad z_n &>& \dfrac{1}{2} \, \big( z_{n-1} + z_{n+1}\big).
\end{array}
\right.
\]
Set the antitone function $\epsilon_1 (y) :=  \frac{2-y}{4}$.
Then
\begin{equation}
\epsilon_1(g_1(x)) = g_1(x) - \dfrac{1}{2} \, \big(g_1(x-1) + g_1(x+1)\big) = \dfrac{1}{2^{x+1}}
\label{eqn:epsilon g M1}
\end{equation}
(Since $g_1$ is bijective, with inverse $g_1^{-1}(y) = 1 - \log (2-y)$, we can obtain $\epsilon_1(y)$ from \Cref{eqn:epsilon g M1}.) 
%We have $1/2 \cdot \big(g_1(x-1) + g_1(x+1)\big) = 2 - \frac{5}{2^{x+1}}$.

Using shorthand 
$\Theta_1 = \tY \, f \, n \, . \, 
\tif{n = 0}{0}{f \, (n - 1) \oplus_{1/2} f \, (n + 1)}$, 
we can define a partial antitone ranking function $f_1 : \mathit{Rch}(M_1) \to \nnReal$ with antitone function $\epsilon_1$ defined as follows:
\begin{align*}
{\Theta_1} \, n 
&\mapsto 
g_1(n)
\\
\tif{n = 0}{0}{\Theta_1 \, (n - 1) \oplus_{1/2} \Theta_1 \, (n + 1)}
&\mapsto
\dfrac{1}{2} \, \big(g_1(n-1) + g_1(n+1)\big)
\\
0 &\mapsto 0
\end{align*}

For $M_3$, we use the function $g_3(x) := 3 - \frac{1}{3^{x-1}}$ and the antitone function $\epsilon_3(y) := \frac{84-28y}{27}$. 
Then
\[
\epsilon_3(g_3(x)) = g_3(x) - \dfrac{1}{3}\big(2 \cdot g_3(x-1) + \cdot g_3(x+2) \big) = 
\frac{28}{3^{x+2}}.
\]
Following $M_1$ above, we can define a partial antitone ranking function $f_3 : \mathit{Rch}(M_3) \to \nnReal$ with $\epsilon_3$ as the antitone function.

\begin{example}[Continuous random walk]\label{ex:raven complex}
In SPCF we can construct a function whose argument changes by a random amount at each recursive call:
\[
\underbrace{\big
(\tY \, f \, x . \tif{x \leq 0}{0}{f(x - \tsample))} \big)}_{\Theta} \, 10
\]
We can construct a partial antitone ranking function $f$ as follows:
\begin{align*}
\Theta \, l 
&\mapsto 
l
\\
\tif{l \leq 0}{0}{\Theta \, (l - \tsample)}
&\mapsto
l - 1/2
\\
0 &\mapsto 0
\end{align*}
with the constant antitone function $\epsilon(y) = 1/2$.

For a more complex example, consider the following ``continuous random walk''
\[
\underbrace{\big
(\tY \, f \, x . \tif{x \leq 0}{0}{f(x - \tsample) \oplus_{1/2} f(x + \tsample)} \big)}_{\Xi} \, 10
\]
Let $g(x) := \sqrt{2} - \frac{1}{2^{x-\frac{1}{2}}}$ and let $\epsilon$ be the function specified by
\[
\epsilon(g(x)) = g(x) - \dfrac{1}{2}\Big(g(x - \dfrac{1}{2}) + \cdot g(x + \dfrac{1}{2}) \Big) = 
\dfrac{3 - 2\sqrt{2}}{2^{x+1}}.
\]
We define a partial antitone ranking function as follows:
\begin{align*}
\Xi \, l 
&\mapsto 
g(l)
\\
\tif{l \leq 0}{0}{\Xi \, (l - \tsample) \oplus_{1/2} \Xi \, (l + \tsample)}
&\mapsto
\frac{1}{2}\big(g(l - \frac{1}{2}) + \cdot g(l + \frac{1}{2}) \big)
\\
r \leq 0, \quad r &\mapsto 0
\end{align*}
\end{example}

\begin{example}[Fair-in-the-limit random walk]
\label{ex:Fair-in-the-limit random walk}\citep[\S 5.3]{DBLP:journals/pacmpl/McIverMKK18}
\[
\underbrace{\big
(\tY \, f \, x . 
\tif{x \leq 0}{0}{f(x - 1) \oplus_{\frac{x}{2x+1}} f(x + 1)} \big)}_{\Xi} 
\, 10
\]
To construct an antitone ranking function, we solve the recurrence relation:
\[
\left\{
\begin{array}{rll}
z_0 &=& 0\\
n \geq 1, \quad z_n &>& \dfrac{n}{2n + 1} \, z_{n-1} + \dfrac{n+1}{2n + 1} \, z_{n+1}.
\end{array}
\right.
\]
It turns out the solution to Example $M_1$, $z_n = \frac{2^n - 1}{2^{n-1}}$, is nearly a solution (for $n = 1$, we have equality instead). 

Let $g(x) = \frac{2^n - 1}{2^{n-1}}$.
Then 
\begin{equation}
\epsilon(g(x)) = 
g(x) - \dfrac{x}{2x + 1} \, g(x-1) - \dfrac{x+1}{2x + 1} \, g(x+1) 
= \dfrac{x-1}{2^{x} \, (2x + 1)}
\label{eqn:in the limit}
\end{equation}

\lo{ongoing}

\end{example}

\begin{example}[Escaping spline]
\label{ex:escaping spline}\citep[\S 5.4]{DBLP:journals/pacmpl/McIverMKK18}
\[
\underbrace{\big
(\tY \, f \, x . 
\tif{x \leq 0}{0}{0 \oplus_{\frac{1}{x+1}} f(x + 1)} \big)}_{\Xi} 
\, 10
\]
\end{example}

\input{y-stopping-time}

\input{relativised-completeness}

\bibliographystyle{apalike}
\bibliography{references}

\lo{22 Jan: 
\paragraph{Further directions}

An obvious next step is to extend the result to the $\mathsf{score}$ construct.

The following are highly topical, and could form the basis of an interesting and novel DPhil thesis.
\begin{enumerate}
\item Devise methods for proving (positive) a.s.~termination. 

- For example, develop a type system satisfying the property: if a term is typable then it is (positively) a.s.~terminating.
See~\citep{DBLP:conf/ppdp/BreuvartL18,DBLP:conf/esop/LagoG17}.

- Another approach is to develop algorithms that synthesise ranking supermartingales, following, for example, \citep{DBLP:journals/pacmpl/AgrawalC018}.

\item Develop principles (e.g.~in the form of ``proof rules'') for reasoning about (positively) a.s.~termination, in the style of \citep{DBLP:journals/pacmpl/McIverMKK18}.


\item Design algorithms that synthesise probabilistic invariants (\emph{qua} martingales), \`a la \cite{SchreuderO19}; see also \citep{DBLP:journals/pacmpl/HarkKGK20}.

\end{enumerate}}

\iffalse
\begin{thebibliography}{9}
\bibitem{ppcf} Thomas Ehrhard, Michele Pagani, and Christine Tasson. Measurable cones and stable, measurable functions: a model for probabilistic higher-order programming. \emph{PACMPL}, 2(POPL):59:159:28, 2018. doi: 10.1145/3158147. URL \href{https://doi.org/10.1145/3158147}{https://doi.org/10.1145/3158147}.
\end{thebibliography}
\fi

\end{document}
