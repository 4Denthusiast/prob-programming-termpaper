\documentclass{article}

%% BEGIN {Luke's Macros}
%%
\newif\ifdraft
\drafttrue %% To hide all comments and highlighting, just comment this line out 

\input{Style/comment}

\usepackage[square]{natbib}
\setcitestyle{aysep={}}

\definecolor{oxblue}{RGB}{0,33,71}
\definecolor{oxgold}{HTML}{a0630a}
\usepackage[final,
bookmarks,
bookmarksopen,
colorlinks,
final,
linkcolor=red,
citecolor=oxgold,
pdfstartview=FitH ]{hyperref}
%%
%% END {Luke's Macros}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{verbatim}
\newcommand{\tY}{\textsf{Y}}
\newcommand{\tif}[3]{\textsf{if }#1\textsf{ then }#2\textsf{ else }#3}
\newcommand{\tsample}{\textsf{sample}}
\newcommand{\tscore}{\textsf{score}}
\DeclareMathOperator{\red}{red}
\DeclareMathOperator{\next}{next}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}


\akr{Following your suggestion, I will be providing a criterion for termination of programs in PPCF \citep{DBLP:journals/pacmpl/EhrhardPT18} based on ranking supermartingales. 
As it's more convenient for this proof, a sampling-based semantics \lo{You need to provide reference(s) for sampling-based semantics.} will be used instead of the original distributional semantics. 
I assume some roughly applicable equivalence is proven somewhere, but it doesn't seem that hard anyway.}
\lo{Why not provide a proof as an appendix?}

Various theorems about probabilistic programs rely on the assumption that the program terminates almost surely. Proof rules based on relating the program state to supermartingales already exist for first-order imperative programs \citep{DBLP:journals/pacmpl/McIverMKK18}. This paper's contribution is to extend this method to a higher-order setting.

\section{Language definition}
The language SPCF is a simply-typed lambda calculus with sampling of real numbers from $[0,1]$ and unbounded scoring, following \cite{MakOP20b}. Types and terms are defined as follows, where $r$ is a real, $x$ is a variable, $f : \mathbb{R}^n \to \mathbb{R}$ is any measurable function and $\Gamma$ is an environment:
\begin{align*}
  & \text{types } A, B ::= \textsf{R}  \mid  A \to B \\
  & \text{values } v ::= \lambda x.s  \mid  \underline{r} \\
  & \text{terms } s, t ::= v  \mid  x  \mid  t_1 t_2  \mid  \underline{f}(s_1,\dots ,s_n)  \mid  \tY s  \mid  \tif{s < 0}{t_1}{t_2}  \mid  \tsample  \mid  \tscore(s)
\end{align*}
\begin{align*}
  \frac{}{\Gamma ; x:A \vdash x:A} \qquad
  \frac{\Gamma ; x:A \vdash s : B}{\Gamma \vdash \lambda x.s : A \to B} \qquad
  \frac{}{\underline{r} : \textsf{R}} \qquad
  \frac{\Gamma \vdash s:A \to B \quad \Gamma \vdash t : A}{\Gamma \vdash s t : B} \\ \\
  \frac{\Gamma \vdash s_1:\textsf{R} \dots \Gamma \vdash s_n:\textsf{R}}{\Gamma \vdash \underline{f}(s_1,\dots,s_n) : \textsf{R}} \ f : \mathbb{R}^n \to \mathbb{R} \qquad
  \frac{\Gamma \vdash s : (A \to B) \to (A \to B)}{\Gamma \vdash \tY s : (A \to B)} \\ \\
  \frac{\Gamma \vdash c : \textsf{R} \quad \Gamma \vdash s_1 : A \quad \Gamma \vdash s_2 : A}{\Gamma \vdash \tif{c < 0}{s_1}{s_2} : A} \qquad
  \frac{}{\Gamma \vdash \tsample : \textsf{R}} \qquad
  \frac{\Gamma \vdash s : \textsf{R}}{\Gamma \vdash \tscore (s) : \textsf{R}}
\end{align*}

The set of all terms is denoted $\Lambda$, and the set of closed terms is denoted $\Lambda^0$.

\paragraph{}
To define the reduction relation, let evaluation contexts be of the form:
\begin{align*}
  E ::= & \, \cdot \mid E t \mid v E \mid \underline{f}(r_1,\dots ,r_{k-1}, E, s_{k+1}, \dots, s_n) \\ & \mid \tY E \mid \tif{E<0}{s_1}{s_2} \mid \tscore (E)
\end{align*}
then a term reduces if it is formed by substituting a redex in a context i.e.
\begin{align*}
  E[(\lambda x.s) v] & \to E[s[v/x]] \\
  E[\underline f (\underline r_1, \dots , \underline r_n)] & \to E[\underline{f(r_1,\dots,r_n)}] \\
  E[\tY \lambda x. s] & \to E[\lambda z. s[(\tY \lambda x. s)/x] z] \text{ where $z$ is not free in $s$}\\
  E[\tif{\underline r < 0}{s_1}{s_2}] & \to E[s_1] \text{ where }r < 0 \\
  E[\tif{\underline r < 0}{s_1}{s_2}] & \to E[s_2] \text{ where }r \geq 0 \\
  E[\tsample] & \to E[\underline r] \text{ where } r \in [0,1] \\
  E[\tscore(\underline r)] & \to E[\underline r].
\end{align*}

Every closed well-typed term either is a value or reduces to another term.


\subsection{Sampling semantics}
This version of the reduction relation allows $\tsample$ to reduce to any number in $[0,1]$. To more precisely specify the probabilities, an additional argument is needed to determine the outcome of random samples. Let $ I = [0,1] \subset \mathbb{R} $, and let $S = I^{\mathbb{N}}$, with the Borel $\sigma$-algebra and the probability measure, denoted $\mu$, given by the limit of $1 \gets I \gets I^2 \gets \cdots$, where the maps are the projections that ignore the last element. Equivalently, a basis of measurable sets is $\prod_{i=0}^\infty X_i$ where $X_i$ are all Borel and all but finitely many are $I$, and $\mu (\prod_{i=0}^\infty X_i) = \prod_{i=0}^\infty \mu_{leb}(X_i)$.
The maps $\pi_h:S \to I, \; \pi_t:S \to S$ popping the first element are then measurable.

The $\sigma$-algebra and measure on $\Lambda$ are defined by considering it as a disjoint union of equivalence classes under replacing all the real constants by a placeholder, where the measure on each class is that of $\mathbb{R}^n$, where $n$ is the number of real constants.

The one-step reduction is given by the function $\red : \Lambda^0 \times S \to \Lambda^0 \times S$ where
\begin{equation}
\red(M,s) = \left\{
    \begin{array}{ll}
        (E[N],s) & \text{if } M = E[R], R \to N \text{ and } R \neq \tsample \\
        (E[\underline{\pi_h(s)}],\pi_t(s)) & \text{if } M = E[\tsample] \\
        (M,s) & \text{if } M \text{ is a value}
    \end{array} \right .
\end{equation}

The result after $n$ steps is then simply $\red^n(M,s) = \overbrace{\red(...\red(}^n M,s)...)$, and the limit $\red^\infty$ can then be defined as a partial function as $\lim_{n \to \infty} \red^n(M,s)$ whenever that sequence becomes constant by reaching a value. A term $M$ terminates for a sample sequence $s$ if the limit $\red^\infty(M,s)$ is defined.

The reduction function is measurable, and the set of values is measurable, therefore the set of $s$ such that $M$ terminates at $s$ within $n$ steps is measurable for any $n$, therefore $\{s \mid M \text{ terminates at } s \}$ is measurable. A term $M$ is said to terminate almost surely if $\mu(\{s \mid M \text{ terminates at } s\}) = 1$.

For example, the term $(\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)} ) \, \underline{0}$, which generates a geometric distribution, terminates on the set $S \setminus [0.5,1]^\mathbb N$, which has measure 1, therefore it terminates almost surely, whereas $\tif{\tsample - 0.5 < 0}{\underline 0}{(\tY \lambda x. x) \underline 0}$, which terminates on the set $\pi_h^{-1}[[0,0.5]]$, has probability 0.5 of failing to terminate.

\paragraph{}
Note that the requirement that $M$ be closed is not important for the proofs of the theorems, but is simply required to ensure that $M$ reaches a value, rather than a general normal form like $xy$.

\paragraph{}
This definition of almost sure termination is equivalent to that given in \citep{MakOP20b}, although the program semantics is stated in a slightly different way. In particular, the argument to $\tscore$ is not relevant to termination (except for the possibility that its argument's evaluation wouldn't terminate).

\lo{Your operational semantics does not maintain a record of the current weight of the reduction.
A.s.~termination does depend on $\tscore$: see \cite[\S 4.3]{DBLP:journals/corr/abs-2004-03924}\footnote{\url{https://arxiv.org/abs/2004.03924}}.
I think it important to take the behaviour of $\tscore$ into account;
you should do it as a future task.}

\subsection{Alternative Semantics}
\akr{This version of the semantics is much more convenient for allowing multiple different reduction orders. I don't currently have a proof of the equivalence of this with the more usual semantics. I expect that that would be somewhat complicated, but not terribly difficult. I don't know whether a semantics like this has already been defined elsewhere. I just thought I'd write it out so you can see it and in case I use it later. If I do end up actually using it I'll write it up nicer, but hopefully this is at least enough for you to understand what I mean.

Having thought a bit more about this, I realise it may not be quite right. I'll see if I can fix it later.}
\lo{I'll read this after you fix it ;-)}

When proving almost sure termination in this way /akr{I guess I should move this section somewhere later.}, it is necessary to consider which terms a given term may reduce to. Sometimes however, the reduction that the programmer has in mind may not be strictly the call-by-value order defined so far, or considering an alternative reduction order may be simpler or more intuitive.

Non-probabilistis lambda calculi generally have the Church-Rosser property, that if a term $A$ reduces to both $B_1$ and $B_2$, there is some $C$ with reduction sequences $B_1 \to^* C$ and $B_2 \to^* C$, so the reduction order mostly doesn't matter. In the probabilistic case, this may not be true, because $\beta$-reduction can duplicate $\tsample$s, so the outputs of the copies of the sample may be identical or independent, depending on whether the sample is taken before or after $\beta$-reduction. There are, however, some restricted variations on the reduction order that do not have this problem.

\paragraph{}
Even with this restriction, a sampling semantics in the style of the one already defined would not be entirely Church-Rosser, as, for example, $red^3(\tsample - \tsample, (1,0,\dots))$ would be either $1$ or $-1$ depending on the order of evaluation of the $\tsample$s, as that determines which sample from the pre-selected sequence is used for each one. To fix this, rather than pre-selecting samples according to the order they'll be drawn in, select them according to the position in the term where they'll be used instead.

A position \akr{I'm not really sure how to phrase the citation, but this is adapded from \cite{Kennaway96infinitarylambda}.} is a finite sequence of steps into a term, defined inductively as
\begin{align*}
P ::= \cdot \mid \lambda ; P \mid @_1 ; P \mid @_2 ; P \mid \underline f_i ; P \mid \tY ; P \mid \textsf{if}_1 ; P \mid \textsf{if}_2 ; P \mid \textsf{if}_3 ; P \mid \tscore ; P.
\end{align*}
The subterm at a position is defined as
\begin{align*}
M \mid \cdot & = M \\
\lambda x. M \mid \lambda ; P & = M \mid P \\
M_1 M_2 \mid @_i ; P & = M_i \mid P \quad \text{for } i = 1,2 \\
\underline f(M_1,\dots,M_n) \mid \underline f_i ; P & = M_i \mid P \quad \text{for }i \leq n \\
\tY M \mid \tY ; P & = M \mid P \\
\tif{M_1 < 0}{M_2}{M_3} \mid \textsf{if}_i ; P & = M_i \mid P \quad \text{for } i = 1,2,3 \\
\tscore(M) \mid \tscore ; P & = M \mid P
\end{align*}
so that every subterm is located at a unique position, but not every position corresponds to a subterm (e.g. $xy \mid \lambda ; \cdot$ is undefined). A position such that $M\mid P$ does exist is said to occur in $M$. Substitution at a given position, $M[N/P]$, is defined similarly.

Two subterms $N_1$ and $N_2$ of a term $M$, corresponding to positions $P_1$ and $P_2$, can overlap in a few different ways. If $P_1$ is an initial segment of $P_2$, then $N_2$ is also a subterm of $N_1$, which is written as $P_1 \leq P_2$. If neither $P_1 \leq P_2$ nor $P_1 \geq P_2$, the positions are said to be disjoint. This is mostly relevant in that in any substitution at some position, the subterms at all other positions are unaffected.

With this notation, a more general reduction relation can be defined. \akr{It would almost work to have one of the neater versions of $\tY$-reduction here.}
\begin{align*}
  \text{if } M \mid P = (\lambda x.s) v,\ & M \to M[s[v/x]/P] \\
  \text{if } M \mid P = \underline f (\underline r_1, \dots , \underline r_n),\ & M \to M[\underline{f(r_1,\dots,r_n)}/P] \\
  \text{if } M \mid P = \tY \lambda x. s,\ & M \to M[\lambda z. s[(\tY \lambda x. s)/x] z/P] \text{ where $z$ is not free in $s$}\\
  \text{if } M \mid P = \tif{\underline r < 0}{s_1}{s_2},\ & M \to M[s_1/P] \text{ where }r < 0 \\
  \text{if } M \mid P = \tif{\underline r < 0}{s_1}{s_2},\ & M \to M[s_2/P] \text{ where }r \geq 0 \\
  \text{if } M \mid P = \tsample & \text{ and $\lambda$ does not occur after $@_2$ or $\tY$ in $P$},\\ & M \to M[\underline r/P] \text{ where } r \in [0,1] \\
  \text{if } M \mid P = \tscore(\underline r),\ & M \to M[\underline r/P].
\end{align*}

\paragraph{}
Labelling the pre-chosen samples by the positions in the term would also not work because in some cases, a $\tsample$ will be duplicated before being reduced, for example, in $(\lambda x. \underline x 0 \underline + x \underline 0) (\lambda y. \tsample)$, both of the $\tsample$ redexes that eventually occur originate at $@_2 ; \lambda$. It is therefore necessary to consider possible positions that may occur in other terms reachable from the original term. This is itself inadequate because some of the positions in different reachable terms need to be considered the same.

Given a closed term $M$, let $L_0(M)$ be the set of pairs, the first element of which is a reduction sequence starting at $M$, and the second of which is a position in the final term of the reduction sequence. Reduction sequences are used rather than reachable terms because if the same term is reached twice, different samples may be needed, but they will be discussed as though they were terms, by abuse of notation. The relation $\sim$ is defined as the union of the symmetric relations $\sim_p$ and $\sim_c$ where
\begin{enumerate}
    \item If $N$ reduces to $O$ with the redex at position $P$, and $Q$ is a position in $N$ where $Q \not \geq P$, then $(N,Q) \sim_p (O,Q)$.
    \item If $N$ $\beta$-reduces to $O$ at position $P$, $Q$ is a position in $N | P;@_1;\lambda$ and $N \mid P;@_1;\lambda;Q$ is not the variable involved in the reduction, $(N,P;@_1;\lambda;Q) \sim_p (O, P;Q)$
    \item If $N$ $\textsf{if}$-reduces to $O$ at position $P$, with the first resp. second branch being taken, and $P;\textsf{if}_i;Q$ occurs in $N$ (where $i = 2$ resp. $3$), $(N,P;\textsf{if}_i;Q) \sim_p (O,P;Q)$
    \item If $N \mid P = (\lambda x. A) B$, $A \mid Q = x$, $B \mid R = C$ is a redex reducing to $C'$ and $S$ exists in $C'$, then letting $O_1$ be $N$ reduced first at $P$ then at $P;Q;R$ and letting $O_2$ be $N$ reduced first at $P;@_2;R$ then at $P$, $(O_1,P;Q;R;S) \sim_c (O_2,P;Q;R;S)$
    \item If $N \mid P = (\tY \lambda x. A)$, $A \mid Q = C$ is a redex reducing to $C'$, $R$ exists in $C'$ and $C' \mid R \neq x$, then letting $O_1$ be $N$ reduced first at $P$ then $P;\lambda;@_1;Q$ and $O_2$ reduced at $P;\tY;lambda;Q$ then $P$, $(O_1, P;\lambda;@_1;Q;R) \sim_c (O_2, P;\lambda;@_1;Q;R)$
    \item If $N \mid P = (\tY \lambda x. A)$, $A \mid Q = C$ is a redex reducing to $C'$, $R$ exists in $C'$ and $A \mid S = x$, then letting $O_1$ be $N$ reduced first at $P$ then $P;\lambda;@_1;S;\tY;\lambda;Q$ and $O_2$ reduced at $P;\tY;\lambda;Q$ then $P$, $(O_1, P;\lambda;@_1;S;\tY;\lambda;Q;R) \sim_c (O_2, P;\lambda;@_1;S;\tY;\lambda;Q;R)$
\end{enumerate}

The reflexive transitive closure $\sim^*$ of this relation is used to define the set of potential positions $L(M) = L_0(M) / \sim^*$, and each equivalence class can be considered as the same position as it may occur across multiple reachable terms. If $(N,P) \sim^* (O,Q)$, then $N \mid P$ and $O \mid Q$ both have the same shape (i.e.~they're either both variables, both applications, both $\tsample$s ect.), therefore it's well-defined to talk of the set of potential positions where there is a $\tsample$, $L_s(M)$. The new sample space is then defined as $I^{L_s(M)}$, with the Borel $\sigma$-algebra and product measure.

\paragraph{}
Before defining the new version of the reduction relation $\red$, a few lemmas about the properties of $\sim$ are necessary for it to be well-defined.
\begin{lemma}
If $(N,P) \sim^* (N,P')$, then $P = P'$.
\end{lemma}
\begin{proof}
Suppose contrariwise that $(N,P) \sim^* (N,P')$ where $P \neq P'$. Pick a path $(N,P) \sim (N_1,P_1) \sim \dots \sim (N_n,P_n) \sim (N,P')$ and assume without loss of generality that the sequence is the shortest for any such counterexample, then $\forall i:N_i \neq N$.

The set of paths through $Rch(M)$ (which we're implicitly identifying with $Rch(M)$ itself) has a tree structure which restricts the possibilities for $N_i$. The only terms that $~$ relates directly are parents and children, for $\sim_p$, and first cousins for $\sim_c$. For any term $O$ that's the result of a reduction $O' \to O$ with the redex at position $P$, the positions in $O$ can be classified as old positions (those that correspond uniquely to structure carried over from $O'$, and which are related to positions in $O'$ by $\sim_p$), and new positions (those that either may have been duplicated in the reduction from $O'$, or aren't directly carried over). New positions are never related to positions in $O'$ by $\sim_p$, and old positions are never related to anything by $\sim_c$. Furthermore, if $(O_1,Q_1) \sim (O_2,Q_2), (O_2,Q_2')$, then $Q_2 = Q_2'$, by an exhaustive check of the cases of the positions relative to the positions of the redexes, therefore taking this together with the minimality of the counterexample, none of the $N_i$ are equal to each-other.

Now simply take cases for how the sequence $N_i$ moves around the tree structure of $Rch(M)$. If it starts by going to a cousin of $N$, it is then on a new position within whatever term it reached therefore it can only proceed to the child of the new node or another cousin (as new positions are not related to anywhere in the parent term), at which point it would have the same problem, so it must eventually descend the tree. Having done so, it is unable to turn back (as all the $N_i$ are disjoint) or take any sideways steps (as it must not be on an old position), therefore it can never return to $N$, which is a contradiction, therefore $N_1$ is not a cousin of $N$. Symmetrically, neither is $N_n$. Similarly, if either $N_1$ or $N_n$ is a child of $N$, the sequence is stuck descending the tree and can't return to $N$. Both $N_1$ and $N_n$ are therefore the parent of $N$, therefore $n = 1$, but in that case $P = P'$, which is a contradiction, therefore the assumed couterexample could not exist.
\end{proof}

At each reduction step $M \to N$, the sample space must be restricted to $I^{L_s(N)}$, which requires a map $L_s(N) \to L_s(M)$. The injection $L_0(N) \to L_0(M)$ is trivial to define by appending $M \to N$ to each path, but in order for this to extend to the quotient, the following lemma is needed.
\begin{lemma}
If $M \to N$, the relation $\sim_N^*$ defined on $L_0(N)$ is equal to the restriction of $\sim_M^*$ from $L_0(M)$ to $L_0(N)$.
\end{lemma}
\begin{proof}
If $(O_1,P_1) \sim_N (O_2,P_2)$, they're related by $\sim_M$ for exactly the same reason. Conversely, consider $(O_1,P_1) \sim_M (O_2,P_2)$ where both $O_1$ and $O_2$ are reachable from $N$ (i.e.~ considering them as paths, they start with the reduction $M \to N$). If they're related by $\sim_{p,M}$, they're related by $\sim_N$ for the same reason. In the case of $\sim_c$, the same proof can only be used for $\sim_N$ if the initial term in these cases ($N'$ here, because of the naming overlap) is reachable from $N$. Given that $O_1$ and $O_2$ are actually reduction sequences, $N'$ is their longest common prefix and they're both reachable from $N$, $N'$ must also be reachable from $N$ therefore $(O_1,P_1) \sim_N (O_2,P_2)$.

The only case left to check is that $(O_1,P_1) \sim_M (O_1',P_1') \sim_M^* (O_2',P_2') \sim_M (O_2,P_2)$, where $O_1$ and $O_2$ are reachable from $N$ but $O_1'$ and $O_2'$. If the first relation in this sequence is $\sim_{p,M}$, then $O_1 = N$ and $O_1' = M$. Similarly for the last relation, 
\end{proof}
Unlike in the purely call-by-value case, the version of the reduction relation that takes into account samples is still a general relation rather than a function in this case, and the entropy value does not change, so it is denoted ``$\to_s$'' instead of ``$\red$'', and it relates $Rch(M)$ to itself.
\begin{align*}
& N \to_s O \text{ if $N \to O$ and the redex is not $\tsample$.} \\
& N \to_s N[\underline{s(N,P)}/P] \text{ if $N \mid P = \tsample$ and $\lambda$ does not occur after $@_2$ of $\tY$ in $P$}
\end{align*}

\akr{Obsolete but keeping for now just in case:
Given a closed term $M$, let $L_v(M), L_Y(M)$ and $L_s(M)$ be the sets of positions such that $M \mid P$ is a variable, $\tY N$ for some N, and $\tsample$, respectively. Although $L_s(M)$ is all the positions where sampling could occur immediately in M, in order to define values for all the samples that could possibly be taken in the course of the reduction of $M$, it is necessary to consider possible rearrangements that may occur later. The set of potential positions $F_x(M)$ (where $x$ stands for $v, Y$ or $s$) is defined inductively by
\begin{align*}
\frac{a \in L_x(M)}{a \in F_x(M)} \qquad
\frac{a \in F_v(M), b \in F_x(M)}{(ab) \in F_x(M)} \qquad
\frac{a \in F_Y(M)}{(Y_\eta a) \in F_v(M)} \qquad \\
\frac{a \in F_Y(M), b \in F_x(M)}{(Y_oab) \in F_x(M)} \qquad
\frac{}{((ab)(ac)) = (a(bc))} \qquad
\frac{}{((Y_oab)(Y_oac)) = (Y_oa(bc))} \qquad
\end{align*}

Given a term $M$, let $L(M)$ be the set of occurrences in $M$ of variables, $\tsample$ and $\tY$, and let $F(M)$ be the free magma on $L(M)$. \akr{Only the elements with no \tsample{}s except in the rightmost position are used. I'm not sure whether making this explicit in the definition of $F(M)$ makes things more or less clear.} Let $I = [0,1]$ and $S = I^{F(M)}$, with the Borel $\sigma$-algebra and the product measure.

For any reduction \akr{including reductions not included in the definition above, i.e.~out of order reductions} $M \to N$, define a corresponding map $F(M \to N) : F(N) \to F(M)$ induced (as a magma morphism) by the obvious injection along with the replacements
\begin{align*}
F(E[(\lambda x. F[x_i])G[y]] \to E[F(G[y]/x)[G[y_i]]]) y_i & = (x_i \cdot y)\\
F(E[\tY_i F[y]] \to E[F[y] (\tY F[y_0])]) (y_0) & = \tY_i \cdot y
\end{align*}
where $y$ can be any of the things in $L(N)$.

The one-step reduction is given by the function $\red : \Lambda^0 \times S \to \Lambda^0 \times S$ where
\begin{equation}
\red(M,s) = \left\{
    \begin{array}{ll}
        (E[N],s \circ F(E[R] \to E[N])) & \text{if } M = E[R], R \to N \text{ and } R \neq \tsample \\
        (E[\underline{s(\tsample_i)}],s\mid_{F(E[\underline{s(\tsample_i)}])}) & \text{if } M = E[\tsample_i] \\
        (M,s) & \text{if } M \text{ is a value}
    \end{array} \right .
\end{equation}

Essentially the entropy value here contains a pre-selected random value for each possible \tsample-reduction, in a way independent of reduction order, taking into account the possible duplication of \tsample{}s in $\beta$- and \tY-reduction steps.}

\section{Ranking functions}
Given a probabilistic program (i.e.~a closed term $M$), in order to construct a supermartingale to prove its a.s.~termination, a function to assign values to each reachable program state is necessary. Let $Rch(M) = \{x \in \Lambda \mid \exists (y_i) : M \to y_0 \to \dots \to y_n \to x\}$, with the $\sigma$-algebra induced as a subset of $\Lambda$, then define a \emph{ranking function on $M$} to be a measurable function $f:\mathit{Rch}(M) \to \mathbb{R}$ such that
\begin{itemize}
    \item $f(N) \geq 0$ for all $N$
    \item $f(E[\tY \lambda x. N] \geq 1+ f(E[\lambda z. N[(\tY \lambda x. N)/x] z]) \text{ where $z$ is not free in $N$}$
    \item $f(E[\tsample]) \geq \int_I f(E[\underline{x}]) \, \mu_{leb}(\mathrm{d}x)$
    \akr{I'm not sure whether $Rch(M)$ is a measurable subset of $\Lambda$, so it might not have a natural measure, but I'm pretty sure this doesn't actually matter, and the $\sigma$-algebra at least is well-defined. $Rch(M)$ could fail to be measurable if there are measurable functions $\mathbb R^n \to \mathbb R$ with non-measurable ranges. I don't know whether these exist. They do for the Lebesgue $\sigma$-algebra, but I'm using the Borel ones.}
    \lo{It is worth inserting the preceding (including the example of a measurable function with non-measurable range) as a remark (or as a ``commenting.sty'' note).}
    \akr{I'd like to include such a remark, but the only reference I've been able to find for the fact that these functions exist is a Stack Exchange question\footnote{\url{https://math.stackexchange.com/questions/1717282/construction-of-a-borel-measurable-function-mapping-borel-set-to-non-borel-set?rq=1}} which implies that the construction is difficult but doesn't actually say what it is (so I probably won't be able to find it on my own).}

    \item $f(E[R]) \geq f(E[N])$ for any other redex $R$, where $R \to N$.
\end{itemize}

Any closed term for which a ranking function exists is called ``rankable''.

\section{Supermartingales}

\begin{theorem} \label{rankable implies ast}
  If a closed SPCF term is rankable, it terminates almost surely.
\end{theorem}
\begin{proof}
Given a closed term $M$ and a ranking function $f$ for it, define random variables on the probability space $S$ (where $s$ is a random variable) by
\begin{align*}
(M_n,s_n) & = \red^n(M,s) \\
y_0 & = 0 \\
y_{n+1} & = \min \{ k \mid k>y_n, M_k \text{ a value or of the form } E[\tY N] \}\\
M'_n & = M_{y_n} \\
X_n & = f(M'_n)
\end{align*}
and define a filtration $\mathcal{F}_n = \sigma(M_k, k \leq n)$ (i.e. all the samples used up to step $n$).

\akr{This is quite a pedantic point but technically the proof wouldn't be correct without it.} In the following proof, conditional expectations (conditioning on $\mathcal F_n$) will be used. Conditional expectations are in general only defined up to a null set, but in this case there is a natural measure on each element of $\mathcal F_n$ (as they're always isomorphic to $S$), and this is the version of the conditional expectation that will be used.

The expectation of $f(M_{n+1})$ given $\mathcal{F}_n$ is trivially less than or equal to $f(M_n)$ in the cases that $M_n \neq E[\tsample]$, and in the case of $\tsample$,
\begin{align*}
& \mathbb{E}[f(M_{n+1}) \mid \mathcal{F}_n] \\
= & \mathbb{E}[f(M_{n+1}) \mid M_n = E[\tsample],\, \mathcal{F}_n] \\
= & \mathbb{E}[f(E[\pi_h(s_n)]) \mid \mathcal{F}_n] \\
= & \int_0^1 f(E[\underline x]) \, \mathrm{d} x \qquad & \text{as }s_n\text{ is independent of } \mathcal{F}_n \\
\leq & f(E[\tsample]) \qquad & \text{by assumption on } f \\
= & f(M_n),
\end{align*}
therefore the values of the ranking function $f(M_n)$ are a supermartingale with respect to $\mathcal{F}_n$.

Given $M'_n$, the reduction relation $\to$, \emph{excluding} $\tY$-reduction steps, is strongly normalising because of the type system, therefore there is some finite bound on the number of reduction steps that can take place from $M'_n$ without a $\tY$-reduction step, therefore $y_{n+1}$ is (conditional on $\mathcal{F}_{y_n+1}$) a bounded stopping time, therefore $\mathbb{E}[f(M_{y_{n+1}}) \mid \mathcal{F}_{y_n+1}] \leq f(M_{y_n+1})$. 
For $n>0$, if $M_{y_n}$ isn't already a value, then $M_{y_n} = E[\tY \lambda x.N]$ for some $E, N$, therefore $M_{y_n+1} = E[\lambda z. N[(\tY \lambda x. N)/x] z]$ and $f(M_{y_n+1}) \leq f(M_{y_n}) - 1$, therefore if $M_{y_n}$ isn't a value, $\mathbb{E}[X_{n+1} \mid \mathcal{F}_{y_n+1}] \leq X_n - 1$.

In the other case that $X_n$ is a value, $X_{n+1} = X_n$, and $X_n = \mathbb E[X_n \mid \mathcal F_{y_n+1}]$ therefore in either case $\mathbb E[X_{n+1} \mid \mathcal F_{y_n+1}] - E[X_n \mid \mathcal F_{y_n+1}] \leq \mathbb -P[M_{y_n} \text{ not a value} \mid \mathcal F_{y_n+1}]$ (as the probability is 0 or 1). Marginalising over $\mathcal F_{y_n+1}$, this implies that $\mathbb{E}[X_{n+1}] - \mathbb{E}[X_{n}] \leq -\mathbb P[M_{y_n} \text{ not a value}]$ therefore as $\mathbb{E}[X_n]$ is bounded below, $\mathbb P[M_{y_n} \text{ not a value}]$ must tend to 0 as $n \to \infty$. $M_{y_n} \text{a value} \Rightarrow M_{y_{n+1}} \text{a value}$ therefore 
\begin{align*}
\mathbb P[M_{y_n} \text{ not a value for infinitely many values of $n$}]  
& = \mathbb P[\forall n: M_{y_n} \text{ not a value}] \\
& \leq \inf_n \mathbb P[M_{y_n} \text{ not a value}] \\
& = 0
\end{align*} 
therefore $M$ terminates almost surely.
\end{proof}

\section{Constructing Ranking Functions}
Although rankability implies almost sure termination, the converse does not hold in general. For example,
\begin{equation}
\tif{-\tsample < 0}{\underline{0}}{(\tY \lambda x. x)\underline 0}
\end{equation}
terminates in 3 steps with probability 1, but isn't rankable because $(\tY \lambda x. x) \underline 0$ is reachable, although that has probability 0. Not only is this counterexample a.s.t, it's positively almost surely terminating i.e.~the expected time to termination is finite.

A ranking function can be constructed under the stronger assumptions that, for every $N$ reachable from $M$, the expected number of $\tY$-reduction steps from $N$ to a value is finite. In particular, the expected number of $\tY$-reduction steps from each reachable term is a ranking function. Note that a finite number of expected $\tY$-reduction steps does not necessarily imply a finite number of expected total reduction steps, for example $(\tY \lambda f n. \tif{\tsample - 0.5 < 0}{n \underline 0}{f (\lambda x. n (n x))}) (\lambda x. x+1)$ terminates with only 2 $\tY$-reductions on average, but applies the increment function $2^n$ times with probability $2^{-1-n}$ for $n \geq 0$, which diverges.

\begin{theorem} \label{minimal}
Given a closed term $M$, the function $f:Rch(M) \to \mathbb R$ given by $f(N) = \mathbb E [\text{the number of }\tY\text{-reduction steps from }N\text{ to a value}]$, if it exists, is the least of all possible ranking functions of $M$.
\end{theorem}
\begin{proof}
Let $f$ be the candidate least ranking function defined above, and suppose $g$ is another ranking function such that $f(N) > g(N)$ for some $N \in Rch(M)$. The restrictions of $f$ and $g$ to $Rch(N)$ have the same properties assumed of $f$ and $g$, so assume w.l.o.g.~that $N=M$. The difference $g - f$ is then a supermartingale (with the same setup as in the proof of Theorem \ref{rankable implies ast}) therefore $\forall n.\ \mathbb E[g(M_n)] \leq \mathbb E[f(M_n)] + g(M)-f(M)$.

$\mathbb E[f(M_n)] = \sum_{k=n}^\infty \mathbb P[M_n = E[\tY N] \text{ for some }E, N] \to 0 \text{ as } n \to \infty$ therefore as $g(M) - f(M) < 0$, eventually $\mathbb E[g(M_n)] < 0$, which is impossible, therefore $g \geq f$ as required.

In order for $f$ to be the least ranking function of $M$, it also has to actually be a ranking function itself. Each of the conditions on a ranking function is easily verified from the definition of $f$.
\end{proof}

\paragraph{}
Even in the case of reasonable simple terms, explicitly constructing a ranking function would be a lot of work, and Theorem \ref{minimal} makes even stronger assumptions than almost sure termination, so it isn't useful for proving it. Take, for example, the term
\begin{align*}
&(\tY \lambda f, n: \\
&\quad \tif{\tsample - 0.5 < 0}{n}{f (n+1)} \\
&) \underline{0}
\end{align*}
which generates a geometric distribution.
    Despite its simplicity, its $Rch$ contains all the terms
\begin{itemize}
    \item $(\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) \underline{0}$
    \item $(\lambda z.(\lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) (\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) z) \underline{i}$
    \item $(\lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) (\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) \underline{i}$
    \item $(\lambda n: \tif{\tsample - 0.5 < 0}{n}{(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{n}{f (m+1)}) (m+1)}) \underline{i}$
    \item $\tif{\tsample - 0.5 < 0}{\underline{i}}{(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{m}{f (m+1)}) (\underline{i}+1)}$
    \item $\tif{\underline r - 0.5 < 0}{\underline{i}}{(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{m}{f (m+1)}) (\underline{i}+1)}$
    \item $\tif{\underline{r - 0.5} < 0}{\underline{i}}{(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{m}{f (m+1)}) (\underline{i}+1)}$
    \item $\underline{i}$
    \item $(\tY \lambda f, m: \tif{\tsample - 0.5 < 0}{m}{f (m+1)}) (\underline{i}+1)$
    \item $(\lambda z.(\lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) (\tY \lambda f, n: \tif{\tsample - 0.5 < 0}{n}{f (n+1)}) z) (\underline{i} + 1)$.
\end{itemize}
\akr{TODO: make this into a diagram with arrows.}
Even in this simple case, defining a ranking function explicitly is awkward because of the number of cases, although in most cases, because the value need only be greater than or equal to that of the next term in sequence, it suffices to take the ranking function as taking only 3 distinct values.

The definition of rankability is also inconvenient for syntactic sugar. It could be useful, for example, to define $M \oplus_p N = \tif{\tsample - p < 0} M N$, where $M \oplus_p N$ reduces to $M$ or $N$, depending on the first value of $s$, with probability $p$ resp.~$(1-p)$. Technically though, it reduces first to $\tif{\underline r - p < 0} M N$ for $r \in I$, so those terms all need values of the ranking function too.

\paragraph{}
In both of these cases, there are only some values of the rankng function that are semantically important. Define a \emph{partial ranking function} on a closed term $M$ to be a partial function $f : Rch(M) \nrightarrow \mathbb R$ such that
\begin{itemize}
    \item $f(N) \geq 0$ for all $N$ where $f$ is defined.
    \item $f$ is defined at $M$.
    \item For any $N$ in the domain of definition of $f$, evaluation of $N$ will eventually \akr{It would be nicer for this to be only almost certain, to make this more neatly in-between rankability and ast, but that doesn't actually work because of terms reachable with 0 probability. Of course, astness doesn't imply rankability, so any definition of partial ranking function that includes all Y-past terms wouldn't always extend to a total ranking function.} reach some $O$ which is either a value or in the domain of definition of $f$, and $f(N) \geq \mathbb E[f(O) + \text{ the number of $\tY$-reduction steps from $N$ to $O$}]$ (where $f(O)$ is taken to be 0 if $O$ is a value outside of the domain of $f$).
\end{itemize}
A partial ranking function that is total is just a ranking function. Providing a partial ranking function is essentially part way between providing a ranking function and directly proving almost sure termination.

\begin{theorem} \label{partial implies rankable}
Every partial ranking function is a restriction of a ranking function.
\end{theorem}
\begin{proof}
Take a closed term $M$ and partial ranking function $f$ on $M$. Define $f_1 : Rch(M) \nrightarrow \mathbb R$ by
\begin{align*}
    f_1(N) & = f(N) \text{ whenever $f(N)$ is defined,} \\
    f_1(V) & = 0 \text{ for values $V$ not in the domain of $f$.}
\end{align*}

Define $(\next(N,s),\_) = \red^n(N,s)$ for the least $n \geq 0$ such that it's in the domain of $f_1$, and $g(N,s) = \left | \{m < n \mid \red^M(N,s) \text{ is of the form } (E[\tY N'],s') \} \right |$. $\next$ is well-defined (i.e.~$n$ is finite) for all $N \in Rch(M)$ by induction on the path from $M$ to $N$, by the third condition on partial ranking functions. Define $f_2(N) = \int_S f_1(\next(N,s)) + g(N,s) \, \mu(\mathrm d s)$. The (total) function $f_2$ agrees with $f$ on $f$'s domain, and it is a ranking function on $M$ (in fact, the least ranking function of which $f$ is a restriction).
\end{proof}

As a corollary, any term which admits a partial ranking function terminates almost surely.

\subsection{Examples}
Let $M \oplus_p N = \tif{\tsample - p < 0} M N$, for $p \in (0,1]$, then there are the pseudo-reduction relations
\begin{align*}
E[M \oplus_p N] \to^3 & E[M] & \\
E[M \oplus_p N] \to^3 & E[N] & \\
\red^3(E[M \oplus_p N], s) = & \left\{
    \begin{array}{ll}
        (E[M],\pi_t(s)) & \text{if } \pi_h(s) < p \\
        (E[N],\pi_t(s)) & \text{if } \pi_h(s) \geq p. \\
    \end{array} \right .
\end{align*}
A partial ranking function could be defined with respect to this shortcut reduction simply by replacing $\to$ and $\red$ in the definition of a partial ranking function by a version that goes straight from $N \oplus_p O$ to $N$ or $O$. Such a pseudo-partial ranking function would then be a partial function from a subset of $Rch(M)$, so it could also be considered as a partial function from all of $Rch(M)$, and it would in fact also be an actual partial ranking function. It is therefore possible to prove rankability directly using the shortcutted reductions.

A similar procedure would work for other forms of syntactic sugar. If a closed term $N$ eventually reduces to one of a set of other terms $\{N_i \mid i \in I\}$ with certain probabilities, a partial ranking function defined with respect to a reduction sequence that skips straight from $N$ to $N_i$ is also a valid partial ranking function for the original reduction function, and therefore its existence implies almost sure termination. There is a caveat, however, that $\tY$-reduction steps skipped over in the shortcut still need to be counted for the expected number of $\tY$-reduction steps.

\paragraph{}
With this abbreviation, the geometric distribution example from earlier can be written as $(\tY f, n. n \oplus_{0.5} f(n+1)) \underline 0$. It is then easy to see that the following is a partial ranking function:
\begin{align*}
(\tY f, n. n \oplus_{0.5} f(n+1)) N \mapsto 2 \\
\underline i \oplus_{0.5} (\tY f, n. n \oplus_{0.5} f(n+1)) (\underline i + 1) \mapsto 1 \\
\underline i \mapsto 0
\end{align*}

\lo{22 Jan: 
\paragraph{Further directions}

An obvious next step is to extend the result to the $\mathsf{score}$ construct.

The following are highly topical, and could form the basis of an interesting and novel DPhil thesis.
\begin{enumerate}
\item Devise methods for proving (positive) a.s.~termination. 

- For example, develop a type system satisfying the property: if a term is typable then it is (positively) a.s.~terminating.
See~\citep{DBLP:conf/ppdp/BreuvartL18,DBLP:conf/esop/LagoG17}.

- Another approach is to develop algorithms that synthesise ranking supermartingales, following, for example, \citep{DBLP:journals/pacmpl/AgrawalC018}.

\item Develop principles (e.g.~in the form of ``proof rules'') for reasoning about (positively) a.s.~termination, in the style of \citep{DBLP:journals/pacmpl/McIverMKK18}.


\item Design algorithms that synthesise probabilistic invariants (\emph{qua} martingales), \`a la \cite{SchreuderO19}; see also \citep{DBLP:journals/pacmpl/HarkKGK20}.

\end{enumerate}}

\bibliographystyle{apalike}
\bibliography{references}

\iffalse
\begin{thebibliography}{9}
\bibitem{ppcf} Thomas Ehrhard, Michele Pagani, and Christine Tasson. Measurable cones and stable, measurable functions: a model for probabilistic higher-order programming. \emph{PACMPL}, 2(POPL):59:1–59:28, 2018. doi: 10.1145/3158147. URL \href{https://doi.org/10.1145/3158147}{https://doi.org/10.1145/3158147}.
\end{thebibliography}
\fi

\end{document}
